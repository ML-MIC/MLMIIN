{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa411ef7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "---\n",
    "title: \"Chapter 2, Part 1 : The Classification Problem. EDA and Preprocessing\"\n",
    "subtitle: \"Machine Learning\"\n",
    "date: \"January 2026\"\n",
    "date-format: \"MMMM YYYY\"\n",
    "author: \n",
    "  - F.San Segundo & N.Rodríguez\n",
    "execute:\n",
    "  echo: true\n",
    "code-overflow: wrap\n",
    "format: \n",
    "  html: \n",
    "    toc: true\n",
    "    code-tools: true\n",
    "    code-fold: show\n",
    "    code-summary: \"Hide the code\"\n",
    "    embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b6d1d8",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "\n",
    "## COPY THIS NOTEBOOK FIRST\n",
    "\n",
    "<h2 style=\"color:blue; font-weight:bold;\">Checklist</h2>\n",
    "\n",
    "+ Have you started Docker Desktop?\n",
    "+ Have you launched Docker from the `MLMIIN` repository folder?\n",
    "+ Have you connected VS Code to the running container?\n",
    "\n",
    "If you have missed any of these steps you may need to restart VS Code after completing them.  \n",
    "Also if Python seems unresponsive at first, try restarting the kernel.\n",
    "\n",
    "<h2 style=\"color:red; font-weight:bold;\">IMPORTANT</h2>\n",
    "\n",
    "+ Remember to **make a copy of this notebook** (in the same folder) before starting to work on it.\n",
    "+ If you make changes to the original notebook, save it with another name, and use Git to undo the changes in the original notebook (ask for help with this if you need it).\n",
    "\n",
    ":::  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fedae1d",
   "metadata": {},
   "source": [
    "We begin by using cd to make sure we are in the right folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5abf000",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd 2_1_Classif_EDA_Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c04b5",
   "metadata": {},
   "source": [
    "# The classification problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd8a842",
   "metadata": {},
   "source": [
    "\n",
    "::: {.callout-note icon=false}\n",
    "\n",
    "## Example: Heart Disease Prediction\n",
    "\n",
    "![](./fig/02-01_heartDisease.png){width=60% fig-align=\"center\" fig-alt=\"Classification problem example: heart disease\"}\n",
    "\n",
    "::: \n",
    "\n",
    "[Source: CDC Heart Disease Facts](https://www.cdc.gov/heartdisease/facts.htm)\n",
    "\n",
    "+ The classification problem begins with a question, where **the expected answer belongs to a finite set of classes**. In the heart disease example above the answer to the question \"Are you at risk?\" is expected to be \"Yes\" or \"No\". Note that a different kind of answer (e.g. a numeric risk score) defines a different kind of problem (regression).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f6379",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=false}\n",
    "\n",
    "# Definition of the problem\n",
    "\n",
    "\n",
    "In statistics, classification is the problem of identifying which of a set of categories (sub-populations) an observation (or observations) belongs to.\n",
    "\n",
    "+  **Binary classification:** observations are grouped in *two categories*.  \n",
    "   ![](./fig/02-01_BinaryClassification.png){width=20% fig-align=\"center\" fig-alt=\"Binary Classification\"}\n",
    "   \n",
    "+  **Multiclass problem:** the number of possible categories exceeds two.  \n",
    "   ![](./fig/02-02_MulticlassClassification.png){width=20% fig-align=\"center\" fig-alt=\"Multiclass Classification\"}\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3479bf",
   "metadata": {},
   "source": [
    "Examples are assigning a given email to the [\"spam\" or \"non-spam\" class](https://en.wikipedia.org/wiki/Spam_filtering), and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d0d6c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "\n",
    "## A Five Steps Draft Plan to Solve the Classification Problem \n",
    "\n",
    "1) **Collect data**\n",
    "2) **Preprocess / Explore Data (EDA)**\n",
    "3) Choose a model\n",
    "4) Fit the parameters of the model\n",
    "5) Assess the model quality\n",
    "\n",
    ":::\n",
    "\n",
    "+ In this session we will see a very simple first example of executing this plan, to get a first impression without getting tangled in the details. But our focus, as indicated, will be in the first two steps pf the process. Then, in later sessions we will look into those details as we learn more about Machine Learning (ML) models and the whole ML process. \n",
    "\n",
    "+ As we gain experience we will see that this plan is in fact a draft, and we will begin to analyze more elaborate strategies in ML.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41618d",
   "metadata": {},
   "source": [
    "# Step1: collect data.\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "\n",
    "#### $\\,$\n",
    "\n",
    "+ **Data is the fuel of ML.** We need to gather as much data as possible with labeled cases (the outputs)\n",
    "+ Decide which variables should be measured. **Expert knowledge** is fundamental to identify potentially relevant variables. Missing a key relevant variable can throw away the entire project.\n",
    "+ **Feature Selection** is the process of selecting a subset of relevant features (inputs, variables, predictors) for use in model construction.\n",
    "+ It is very important to gather **data for all output categories!**\n",
    "\n",
    ":::\n",
    "\n",
    "![](./fig/02-02_heartDisease_featureSelection.png){width=50% fig-align=\"center\" fig-alt=\"Heart Disease Feature Selection\"}\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dbcc3b",
   "metadata": {},
   "source": [
    "\n",
    "::: {.callout-note icon=false}\n",
    "\n",
    "#### $\\quad$\n",
    "\n",
    "+ Learn about **Raw vs Processed** data. See [Jeff Leek's website](http://jtleek.com/modules/03_GettingData/01_02_rawAndProcessedData/#6) or [Wikipedia](https://en.wikipedia.org/wiki/Raw_data).\n",
    "+ **Garbage in, garbage out (GIGO) principle:** The output of our ML algorithm will be as good (or as bad) as the quality of the input data it receives.\n",
    "+ Getting good data takes a lot of time and resources. [Forbes 2019: Data is the New Oil](https://www.forbes.com/sites/forbestechcouncil/2019/11/15/data-is-the-new-oil-and-thats-a-good-thing/)\n",
    "\n",
    ":::\n",
    "\n",
    "![](./fig/02-08_dataNewOil.png){width=30% fig-align=\"center\" fig-alt=\"Wired. Data is the New Oil\" width=30%}  \n",
    "  \n",
    "[Source: Wired Journal (2014)](https://www.wired.com/insights/2014/07/data-new-oil-digital-economy/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db4210",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=false}\n",
    "\n",
    "### Data Structures for Machine Learning: \n",
    "\n",
    "Datasets for Machine Learning classification problems are usually prepared in the shape of a data table containing:\n",
    "\n",
    "+ A set of n *attributes (features or variables)* corresponding to the *columns* of the table.\n",
    "+ A set of N labeled *samples (observations, data points)* (elements for which you want to make predictions), Each sample corresponds to a *row* of the table.\n",
    "\n",
    ":::\n",
    "\n",
    "A data table for the heart disease problem could be something like:\n",
    "\n",
    "![](./fig/02-03_heartDisease_dataTable.png){width=75% fig-align=\"center\" fig-alt=\"data table example fpr heart disease classification\" width=90%}\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4730e",
   "metadata": {},
   "source": [
    "# Probabilistic (Soft) Approach to Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ebcbd",
   "metadata": {},
   "source": [
    "+ We now deal with binary classification and will return to the multiclass problem later. For binary classification many of the models we will study use a **soft partition** or **probabilistic** approach. \n",
    "\n",
    "+ Let $\\Omega$ be our inputs space, so that each data point (sample) $(\\mathbf x, y)$ is given by a point $\\mathbf x$ in $\\Omega$ and an output value $y\\in\\{0, 1\\}$.   \n",
    "**Note:** In each particular problem the output $y$ can only take two different values: (yes, no), (true, false), (dog, cat) and so on. But sometimes it is useful to identify these values with (1, 0). \n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "\n",
    "## Binary Classification Model, Probabilistic Approach\n",
    "\n",
    "+ In this probabilistic approach a (binary) classification model is a function (rule, algorithm)  \n",
    "  $$f(\\mathbf x) = P(y = 1 | \\mathbf x)$$\n",
    "  that returns the *conditional probability* that the output $y$ equals 1, given an input value $\\mathbf x \\in \\Omega$. \n",
    "\n",
    ":::\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9737bf6",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=false}\n",
    "\n",
    "### Class vs Probability Prediction \n",
    "\n",
    "+ You can think of the function $f$ as an answer to the question:    \n",
    "  \n",
    "  **Given that we know that the input is $\\mathbf x$, how certain are we that the output value $y$ is 1; that is, it belongs to the first class?**\n",
    "\n",
    ":::\n",
    "\n",
    "+ The answer comes in the form of a probability. In the plot below we have two continuous input variables $x_1, x_2$. The *true class* of the binary output $y$ is indicated by the color  of the sample points (blue corresponds to class yes, red to class no). The function $f$ defines *levels of certainty* of the value of $y$, represented in this plot by level curves of $f$.\n",
    "\n",
    "![](./fig/02-06_softPartition.png){width=60% fig-align=\"center\" fig-alt=\"Soft Partition\"}\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95c0061",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=false}\n",
    "\n",
    "### From probablity (soft) prediction to class (hard) prediction.\n",
    "\n",
    "+ Once we have a probability prediction model $f$ we can **set a threshold** to obtain class predictions. For example if we choose a threshold at $70%$ certainty (i.e. 0.7 in terms of probability) then we assign classes using this rule:\n",
    "$$\n",
    "\\text{class}(y|\\mathbf x) = \n",
    "\\begin{cases}\n",
    "1 & \\text{ if }f(\\mathbf x) = P(y = 1 | \\mathbf x) \\geq 0.7\\\\[3mm]\n",
    "0 & \\text{ if }f(\\mathbf x) = P(y = 1 | \\mathbf x) < 0.7\n",
    "\\end{cases}\n",
    "$$\n",
    ":::\n",
    "\n",
    "+ In the previous example, the plot of the class assignment now provides a **hard boundary**, a single curve between classes:  \n",
    "![](./fig/02-07_hardPartition.png){width=60% fig-align=\"center\" fig-alt=\"Hard Partition\"}  \n",
    "  The legend of the plot points at the associated **misclassification problem.** Some points with true class equal `no` get a prediction of class `yes` (false positives). We wiill return to this shortly, when discussing how to assess the quality of a classification model. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88da35f",
   "metadata": {},
   "source": [
    "## Scores and Calibration\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "\n",
    "### Probablity prediction vs scores.\n",
    "\n",
    "+ As we will see, the output of many Machine Learning models for classification problems is not a probability but a **score**. That is, a numerical value (usually in the $[0, 1]$ interval). The score is somehow connected to the degree of certainty that the model has in its predictions. If the score corresponds (as usual) to the positive class (usually represented as 1 or YES or True) then a **higher score means that the model is more certain when predicting positive**. The scores should therefore be **ordered**.  \n",
    "\n",
    ":::\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "\n",
    "###  Calibration.\n",
    "\n",
    "+ But to be interpreted as probabilities the scores should be **calibrated**. A perfectly calibrated scoring means that if we take 100 predictions of the model, all of them with score 0.7, then seventy of them shoul be real positives and 30 should be real negatives. Of course real models are often not perfectly calibrated. We will return to this issue of calibration later.\n",
    "\n",
    ":::\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a62f3",
   "metadata": {},
   "source": [
    "# 1. Collect and Input Data\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "\n",
    "## Tabular Data\n",
    "\n",
    "* Our data sources in this course will follow a tabular structure in which (tidy data):\n",
    "    * each **column** of the table contains values of a **variable** in the dataset.\n",
    "    * each **row** of the table contains an **observation** (case, individual).\n",
    "\n",
    "* Quite often data is not organized in a tabular way. Sometimes, it can be rearranged into that shape. This **data wrangling** operations (reshaping data, merges, etc.) are time consuming and require experience and skill. Besides there are many sources of data that do not fit into this tabular data structures. \n",
    "\n",
    ":::\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "\n",
    "## Tabular Data in Plain Text Files or Spreadsheets\n",
    "\n",
    "+ The data in this course will usually be stored in plain text files (with extensions like txt, csv, dat) or in spreadsheet formats. We recommend uing a good **text editor** for text files and limiting the use of **spreadsheets** only for that specific case. \n",
    "\n",
    "+ Pay attention to the structure of text files. Look for a **header line** with names for the variables in the table.  Each data row in the file corresponds to an observation (row). The values (columns) in the line are separated with a **separator character** (comma, space, tab, etc.)\n",
    "\n",
    ":::\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca9b29",
   "metadata": {},
   "source": [
    "# 2 Preprocessing and EDA: basic steps\n",
    "\n",
    "The second step in our plan to solve the classification problem is itself a multistep process:\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "\n",
    "### Basic Steps in Preprocessing\n",
    "\n",
    "* Step 1 : Import the dataset and do a preliminary exploration\n",
    "* Step 2 : Choose proper type, names and encoding for the variables\n",
    "* Step 3 : Check out the missing values and possible errors\n",
    "* Step 4 : Split the dataset into Training, Validation and Test Sets\n",
    "* Step 5 : Plot the numeric variables in training data and check out for outliers\n",
    "* Step 6 : EDA and feature selection or engineering. Pipelines.\n",
    "* Step 7 : Check out for class imbalances.\n",
    "\n",
    ":::\n",
    "\n",
    "We will next see a first basic example of how to perform these steps using Python.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f3447",
   "metadata": {},
   "source": [
    "## Python Example \n",
    "\n",
    "+ To start practicing with Python we will use the data file in this session's folder called `Simdata0.csv`. This file contains a synthetic dataset with several input variables and one categorical output variable `Y`.  \n",
    "\n",
    "::: {.callout-important}\n",
    "\n",
    "Always begin opening every new data file in a **text editor**. Look for the presence of a *header* line, the *separator* used and any other relevant characteristic of the data file. \n",
    "\n",
    ":::  \n",
    " \n",
    " * Do it now for this example file. \n",
    "\n",
    " ---\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde3682f",
   "metadata": {},
   "source": [
    "# 2 Preprocessing, Step 1 : Import the dataset and do a preliminary exploration  {.unnumbered .unlisted}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be365de9",
   "metadata": {},
   "source": [
    "We will usually begin importing some of the **standard Python Data Science libraries**, as in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f503fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf67ea",
   "metadata": {},
   "source": [
    "\n",
    "Then load the data and see the first lines of the table: run the code below (it may need some modification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02982fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Simdata0.csv', sep=';')\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2602a8e",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "### Exercise 000.\n",
    "This command as it stands will not work. Read the error message, also read the [`pd.read_csv` documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) and fix the error.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966fd444",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Data cleaning\n",
    "\n",
    "This is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting them.\n",
    "\n",
    "A good starting point is gathering basic information such as the dimensions of the table, names and types of variables and possible missing values. This is provided by the `ìnfo` method in Pandas.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c721c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c2df5",
   "metadata": {},
   "source": [
    "There seems to be missing values in `X1` and `X2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ed5ac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ef1bd",
   "metadata": {},
   "source": [
    "We should always check the first few rows of the table to get a first impression of the data. Use the pandas method `head` for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e824264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9849270a",
   "metadata": {},
   "source": [
    "::: {.callout-warning icon=false}\n",
    "\n",
    "## Formatting Output in Notebooks\n",
    "\n",
    "+ Let Jupyter care  about the table printing format. Do not use `print` for pandas DataFrames in notebooks!\n",
    "+ Always be mindful of the size and relevance of the output of your notebook cells!\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4699f7d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d07d4",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=false}\n",
    "\n",
    "### Remove Unused Columns from the Dataset\n",
    "\n",
    "Often the dataset will contain some **index variables**, that only serve the purpose of identifying particular data points. The simplest example would be a column that contains the row numbers for the dataset. This kind of variable should be removed before proceeding with the analysis. In our dataset this is the case for the first column `id`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84decee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"id\", inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e25d89",
   "metadata": {},
   "source": [
    "Always check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6ae628",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62379ea7",
   "metadata": {},
   "source": [
    "::: {.callout-warning icon=false}\n",
    "\n",
    "Reducing the number of variables is very important if we are to avoid [one big danger!](./https://ml-mic.github.io/MLMIIN_public/pages/TheCurse.html) You can read more (serious stuff) about this here.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d0ade0",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7476dd",
   "metadata": {},
   "source": [
    "# 2 Preproc; Step 2 : Choose proper type, names and encoding for the variables  {.unnumbered .unlisted}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e48feb",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=false}\n",
    "\n",
    "### Think about the proper type for each variable\n",
    "\n",
    "Make sure that the Python type of each variable in the dataset matches its meaning and our modeling choices. Use `nunique` to see the number of differeent values for each variable (e.g. to decide if we consider a numeric variable as discrete or continuous).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c997fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab9a69",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223d90ea",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=false}\n",
    "\n",
    "### Identify the output variable and the categorical inputs and set their type\n",
    "\n",
    "In particular, set the type of all categorical variables (aka factors or qualitative variables) to `category` as follows (and check the result!). Remeber that in classification problems the output is always categorical.\n",
    "\n",
    "We will also be using fixed names, such as `cat_inputs` below, to make the code reusable in the following sessions: \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b99303",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'Y'\n",
    "cat_inputs = ['X4']\n",
    "df[cat_inputs + [output]] = df[cat_inputs + [output]].astype(\"category\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b64df",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c382c4",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=false}\n",
    "\n",
    "### Numerical inputs\n",
    "\n",
    "Similarly we identify the set of numerical inputs and create an index of all input variables.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590d826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df.columns.drop(output)\n",
    "num_inputs = inputs.difference(cat_inputs).tolist()\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3c308c",
   "metadata": {},
   "source": [
    "::: {.callout-warning icon=false}\n",
    "\n",
    "### Notes about Setting the Variable Types.\n",
    "\n",
    "+ Setting the type as category is important for EDA, it will make Python visualization libraries work as intended. But below we will consider a different encoding of factors (one hot), to meet the requirements of `scikit-learn`.\n",
    "+ Do not automatically assume that variables read by Pandas as `object` are categorical. This is a common source of modeling mistakes.\n",
    "+ When missing values exist you may need to deal with them before properly setting the variable types. \n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c678469",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a3704",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=false}\n",
    "\n",
    "### One Hot Encoding of the Categorical Inputs\n",
    "\n",
    "The `scikit-learn` library requires numerical inputs. Therefore, we will encode the categorical inputs in a numeric format, called **One Hot Encoding**, illustrated in the figure below.\n",
    "\n",
    "![](./fig/2_1_001_OneHotEncoding.png){width=60% fig-align=\"center\" fig-alt=\"One Hot Encoding of Factors\"}\n",
    "\n",
    "As you can see, a categorical input with $k$ levels is encoded in a set of $k$ new variables (columns), all of which equal 0 except for the one that corresponds to the level of the current observation. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589540ee",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c92ca5",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=false}\n",
    "\n",
    "### One Hot Encoding (OHE) in scikit-learn\n",
    "\n",
    "The code that we use below performs this one hot encoding and adds the resulting new columns to the DataFrame. But it also keeps the columns with previous versions of the categorical inputs, because we will use them in EDA below. Besides,  \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb56912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe_result = ohe.fit_transform(df[cat_inputs])\n",
    "\n",
    "ohe_inputs = [ohe.categories_[k].tolist() for k in range(len(cat_inputs))]\n",
    "ohe_inputs = [cat_inputs[k] + \"_\" + str(a) for k in range(len(cat_inputs)) for a in ohe_inputs[k]]\n",
    "\n",
    "ohe_df = pd.DataFrame(ohe_result, columns=ohe_inputs)\n",
    "df[ohe_inputs] = ohe_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a657a484",
   "metadata": {},
   "source": [
    "Note that the lists `num_inputs` and `cat_inputs` and the `output` are unaffected by this.\n",
    "\n",
    "::: {.callout-tip  icon=false}\n",
    "### Exercise 001.\n",
    "\n",
    "Split the above code cell at the blank lines and use this to check the steps of the `ohe` process displaying `ohe_result`, `ohe_inputs` and `ohe_df`. Also display the resulting `df` and check it e.g. with `df.info()`. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1badf7",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c327d",
   "metadata": {},
   "source": [
    "# 2 Preprocessing, Step 3: Check out the missing values  {.unnumbered .unlisted}\n",
    "\n",
    "We can do a quick check of the number of missing values per column, to confirm what we saw before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e1c77",
   "metadata": {},
   "source": [
    "Dealing with missing values is usually non trivial. After identiying them we can either drop them or try to replace them with some meaningful values (imputation). We will discuss that later in the course. For now we just drop the table rows containing missing values. Note the `inplace` argument and check the result again with the preceding command after removing the missing data with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c10e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b84e730",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444961c",
   "metadata": {},
   "source": [
    "# 2 Preprocessing, Step 4: Split the dataset into Training and Test Sets  {.unnumbered .unlisted}\n",
    "\n",
    "This step is a crucial part of the method that we use to assess model performance. We will have plenty of oportunities to go into details in the course, but the central idea is that we need to keep a part of the data (test set) hidden from the model, so that we can use it at the end to get a better mesaure of the model's p erformance. We must do this in the early steps of preprocessing for this strategy to work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035fd27",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=false}\n",
    "\n",
    "### Using Standard Names for the Variables\n",
    "\n",
    "+ The classification problem for this dataset is trying to predict the value of `Y` (output variable) from the values of `X1, X2, X3, X4` (the inputs). \n",
    "\n",
    "+ In order to make our code useful for different datasets and problems we will try to use the same variable names in the modeling code. In particular we use the following code to distinguish the output and input variables in Python:\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=output)\n",
    "Y = df[output]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47255626",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "The actual splitting is now accomplished with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd2c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "XTR, XTS, YTR, YTS = train_test_split(X, Y,\n",
    "                                      test_size=0.2,  # percentage preserved as test data\n",
    "                                      random_state=1, # seed for replication\n",
    "                                      stratify = Y)   # Preserves distribution of y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f16624",
   "metadata": {},
   "source": [
    "The `stratify = y` option guarantees that the proportion of both output classes is (approx.) the same in the trainng and test datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6356e21",
   "metadata": {},
   "source": [
    "\n",
    "::: {.callout-tip  icon=false}\n",
    "### Exercise 002.\n",
    "Check this statement by making a frequency table for the output classes in training and test. \n",
    "\n",
    ":::\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07c50f",
   "metadata": {},
   "source": [
    "# 2 Preproc; Step 5: Plot the numeric variables in training data and check out for outliers  {.unnumbered .unlisted}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162df7fd",
   "metadata": {},
   "source": [
    "\n",
    "**Outliers** (also called *atypical values*) can be generally defined as *samples that are exceptionally far from the mainstream of the data*. Formally, a value $x$ of a numeric variable $X$ is considered an outlier if it is bigger than the *upper outlier limit*.\n",
    "   \n",
    "$$q_{0.75}(X) + 1.5\\text{IQR}(X)$$\n",
    "  \n",
    "or if it is smaller than the *lower outlier limit*\n",
    "    \n",
    "$$q_{0.25}(X) - 1.5\\text{IQR}(X),$$\n",
    "\n",
    "where $q_{0.25}(X), q_{0.45}(X)$  are respectively the first and third **quartiles** of $X$ and $\\operatorname{IQR}(X)$ is the **interquartilic range** of $X$. \n",
    "\n",
    "![](./fig/outliers_Imagen%20de%20MachineLearning_Ch2_Classification_1_Intro_preprocess_v3_%20p19.png){width=40% fig-align=\"center\" fig-alt=\"Outliers\"}\n",
    "\n",
    "A boxplot graph, like the one on the left, of the variable is often the easiest wy to spot the pressence of outliers.\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77afa34",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false}\n",
    "### Types of outliers:\n",
    "\n",
    "+ When one or more samples are suspected to be outliers, the first step is to make sure that the values are valid and that no **data recording errors** have occurred.\n",
    "\n",
    "+ With small sample sizes, **apparent outliers** might be a result of a skewed distribution where there are not yet enough data to see the skewness.\n",
    "\n",
    "+ Finally, there are **\"true\" informative outliers**.\n",
    "\n",
    ":::\n",
    "\n",
    "Just as we saw with missing data, dealing with outliers is not trivial (even when we are able to separate apparent from real outliers). Sometimes we will have to check their influence in the model before deciding how to proceed. \n",
    "\n",
    "In simple cases, where the outliers represent a very small fraction of the training data, we will simply remove them. But keep in mind that this is not always a good idea. In fact, the removal of *all* outliers often leads to the appearance of new outliers.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0aa60",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false\"}\n",
    "\n",
    "#### Boxplots for the Numerical Inputs\n",
    "\n",
    "The following code makes boxplots of all the numeric inputs in our example dataset:\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca747f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTR_numeric_boxplots = XTR[num_inputs].plot.box(subplots=True, \n",
    "                                            layout=(1, len(num_inputs)), \n",
    "                                            sharex=False, sharey=False, figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b973f8",
   "metadata": {},
   "source": [
    "The plot reveals some clearly atypical values in `X1, X3`, while some other outliers closer to the boxes can be considered as simply belonging to the tails of the distributions for the variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd0042b",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ade746",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "### Dropping outliers.\n",
    "\n",
    "We have used the basic boxplot provided by pandas. But in order to drop the outliers we need to locate them. The following function provides an alternative boxplot graph using the seaborn library. The function parameters are a Pandas Dataframe `df` and a list `num_vars` of names for numerical variables in `df`. The return value is a dictionary whose keys are the names in `num_vars`. The values are also dictionaries with (self-explanatory?) names `values`, `positions` and `indices`.   \n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cbook import boxplot_stats\n",
    "\n",
    "def explore_outliers(df, num_vars):\n",
    "    fig, axes = plt.subplots(nrows=len(num_vars), ncols=1, figsize=(7, 3), sharey=False, sharex=False)\n",
    "    fig.tight_layout()\n",
    "    outliers_df = dict()\n",
    "    for k in range(len(num_vars)):\n",
    "        var = num_vars[k]\n",
    "        sns.boxplot(df, x=var , ax=axes[k])\n",
    "        outliers_df[var] = boxplot_stats(df[var])[0][\"fliers\"]\n",
    "        out_pos = np.where(df[var].isin(outliers_df[var]))[0].tolist() \n",
    "        out_idx = [df[var].index.tolist()[ k ] for k in out_pos]\n",
    "        outliers_df[var] = {\"values\": outliers_df[var], \n",
    "                            \"positions\": out_pos, \n",
    "                            \"indices\": out_idx}\n",
    "    return outliers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a80776b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "::: {.callout-tip  icon=false}\n",
    "### Exercise 003.\n",
    "\n",
    "Use this function with the numeric inputs in `XTR`. Then use the result to drop all the outliers. Make sure that you actually remove them from `XTR`. Also make sure to remove the corresponding output values in `YTR`.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04def03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../exclude/code/2_1_Exercise_003.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0a15f",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8697bc",
   "metadata": {},
   "source": [
    "# 2 Preproc; Step 6: EDA and Feature selection or engineering. Pipelines.  {.unnumbered .unlisted}\n",
    "\n",
    "In this step we will look into the distribution of each of the input variables and at the same time we will study the possible relations between them. This includes relations between the input variables by themselves and also the relation of the inputs with the output $Y$. As a result of this **Exploratory Data Analysis (EDA)** we will decide if we need to apply a transformation or to create new input variables and, given the case, whether to select a smaller set of inputs. This last part belongs to the set of techniques called **Feature Selection and Engineering**.\n",
    "\n",
    "::: {.callout-note icon=\"false}\n",
    "### Tasks associated with EDA and Feature Selection\n",
    "\n",
    "In this section we will discuss several topics related to this kind of analysis:\n",
    "\n",
    "**EDA**\n",
    "\n",
    "* The `describe` function.\n",
    "* Pairplots.\n",
    "* Relation between numerical inputs and factor inputs.\n",
    "\n",
    "**Feature Selection and Engineering**\n",
    "\n",
    "* Correlation between numerical inputs.\n",
    "* Variable transformations and pipelines.\n",
    "\n",
    ":::\n",
    "\n",
    "We will also see that Scikit-learn provides a **pipeline** mechanism to ease the implementation of Feature Engineering and Modeling. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8cbe13",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false}\n",
    "### The `describe` function.\n",
    "\n",
    "This pandas function provides numerical summaries for the variables in a dataframe. If applied directly the output only considers numerical variables. Thus, it is best to deal with numerical and factor inputs separately.\n",
    "\n",
    ":::\n",
    "\n",
    "For numeric inputs (we transpose the resulting table because we have a small number of variables) we get standard statistical summaries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff453e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTR[num_inputs].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3accfc62",
   "metadata": {},
   "source": [
    "For input factors we obtain basic information (number of distinct values, the mode and its frequency):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ab9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTR[cat_inputs].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a7413c",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03330d1f",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "### Pairplots\n",
    "\n",
    "A pairplot for a set of variables $V_1, \\ldots, V_j$ is a set of plots arranged in a rectangular (or triangular) grid, describing the distribution and relations between those variables. Here we use [seaborn `pairplot` function](https://seaborn.pydata.org/generated/seaborn.pairplot.html) to draw a pairplot for the numerical inputs.\n",
    ":::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0491b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "plt_df = XTR[num_inputs].copy()\n",
    "plt_df[\"YTR\"] = YTR\n",
    "sns.pairplot(plt_df, hue=\"YTR\", corner=True, height=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5c2c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "::: {.callout-warning icon=\"false}\n",
    "\n",
    "### Notes about pairplot use and interpretation\n",
    "\n",
    "+ The plot in a position outside the diagonal describes the relation between two variables. The type of plot depends on the nature of these variables; e.g. a scatterplot if both are numerical. \n",
    "+ The plots in the diagonal describe the distribution of the corresponding variable; e.g. with density curves for numerical variables.\n",
    "+ The `corner` argument removes the (redundant) plots above the diagonal.\n",
    "+ **In a classification problem we can use the `hue` argument to include the information about the outplut classes into the plots (different colors for each class).** \n",
    "+ **Be careful when using pairplots for datasets with a large number of variables!** The pairplot is more difficult to interpret and your computer's memory can be exhausted.\n",
    "\n",
    ":::\n",
    "\n",
    "In our example the pairplot shows these remarkable features of the dataset:\n",
    "\n",
    "+ A clear and strong linear relation between $X_1$ and $X_3$. We will confirm this with the correlation matrix and use it in feature selection. \n",
    "+ A clearly visible patterm in the $X_1, X_2$ scatterplot, corresponding to the output $Y$ classes. But $X_1$ and $X_2$ seem uncorrelated. \n",
    "+ The $X_2, X_3$ is similar; not a surprise given the strong relation between $X_1$ and $X_3$.\n",
    "+ The diagonal shows approximately normal distributions for $X_1, X_2,X_3$ and hints at $X_2$ as the best single predictor for $Y$.\n",
    "\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d010876a",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "### Plots to Study Numerical Input vs Factor Input Relations\n",
    "\n",
    "The following code will plot parallel boxplots for each combination of a numerical input and a factor input. This information is useful for feature selection and engineering.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for numVar in num_inputs: \n",
    "    catvar_num = 0\n",
    "    if len(cat_inputs) > 1:\n",
    "        fig, axes = plt.subplots(1, len(cat_inputs))  # create figure and axes\n",
    "        print(f\"Analyzing the relation between factor inputs {cat_inputs[catvar_num]} and \", numVar)\n",
    "        for col, ax in zip(cat_inputs, axes):  # boxplot for each factor inpput            \n",
    "            catvar_num += 1\n",
    "            sns.boxplot(data=XTR, x = col, y = numVar, ax=ax) \n",
    "        # set subplot margins\n",
    "        plt.subplots_adjust(left=0.9, bottom=0.4, right=2, top=1, wspace=1, hspace=1)\n",
    "        plt.figure(figsize=(1, 1))\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Analyzing the relation between factor input {cat_inputs[0]} and \", numVar)\n",
    "        sns.boxplot(data=XTR, x = cat_inputs[0], y = numVar)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8aa9da",
   "metadata": {},
   "source": [
    "::: {.callout-tip icon=\"false}\n",
    "\n",
    "### Exercise 004\n",
    "+ Run that code. What is your conclussion after running the code?\n",
    "+ This code can be easily modified to use e.g. density curves or histograms. Do it now, using density curves.\n",
    "\n",
    "::: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63055732",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc9056b",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "### Correlation Matrix\n",
    "\n",
    "The correlation matrix completes the information provided by the pair plot. The values in the diagonal are of course all 1.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b90368",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTR[num_inputs].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547864a5",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "### Visualization of the Correlation Matrix\n",
    "\n",
    "Direct inspection of the correlation matrix is often not good enough for datasets with many inputs. The code below plots a version of the correlation matrix that uses a color scale to help locate interesting correlations. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f045a60",
   "metadata": {},
   "source": [
    "```\n",
    "plt.figure()\n",
    "plt.matshow(XTR[num_inputs].corr(), cmap='viridis')\n",
    "plt.xticks(range(len(num_inputs)), num_inputs, fontsize=14, rotation=45)\n",
    "plt.yticks(range(len(num_inputs)), num_inputs, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b90dea2",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdde46b",
   "metadata": {},
   "source": [
    "\n",
    "::: {.callout-tip  icon=false}\n",
    "### Exercise 005.\n",
    "\n",
    "Run that code (copy it to a Python cell in your notebook).\n",
    "\n",
    ":::\n",
    "\n",
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "### Use of the Correlation Matrix for Feature Selection\n",
    "\n",
    "The correlation matrix completes the information provided by the pair plot. And in the case of very high values of correlation we can decide to drop one of the variables. The rationale behind this is that two highly correlated variables contain essentially the same information. Besides, some models will suffer numerical stability issued when fed with highly correlated (colinear) variables. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf54167",
   "metadata": {},
   "source": [
    "In our case it confirms the very strong (negative) correlation between $X_1$ and $X_3$. Thus we will drop e.g. $X_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTR.drop(columns=\"X3\", inplace=True)\n",
    "num_inputs.remove(\"X3\") # Keep the list of inputs updated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af15712",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "### More on  Feature Selection\n",
    "\n",
    "There is not a universally accepted threshold that we can use to define a correlation to be *high enough*. EDA can be our guide in this. And later in the course we will discuss methods for systematic feature selection. Meanwhile, and in simple cases such as our example, this *manual* approach is enough. But in more complex situation we can set a correlation threshold and use the following heuristic:\n",
    "\n",
    "1. Calculate the correlation matrix of the predictors.  \n",
    "2. Determine the two predictors associated with the largest absolute pairwise correlation (call them predictors A and B).  \n",
    "3. Determine the average correlation between A and the other variables. Do the same for predictor B.  \n",
    "4. If A has a larger average correlation, remove it; otherwise, remove predictor B.  \n",
    "5. Repeat Steps 2-4 until no absolute correlations are above the threshold.  \n",
    "\n",
    ":::\n",
    "\n",
    "Later in the course we will discuss PCA (principal component analysis) and other *dimensionality reduction* techniques that can be used to perform feature selection even for hundreds or thousands of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6db530",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "### Transforming Variables. \n",
    "\n",
    "Many Machine Learning algorithms are affected by the scale of the predictors or require them to be (at least approximately) normal. Therefore we will sometimes apply transformations to the data. A typical example is **standardization** where an input variable `X` is transformed as:\n",
    "$$\n",
    "X^* = \\dfrac{X -\\bar X}{s_x}\n",
    "$$\n",
    "where $\\bar X$ is the mean of $X$ and $s_x$ its (sample) standard deviation. Similar transformations are used to ensure that all values belong to the $[0, 1]$ interval, etc.\n",
    "\n",
    ":::\n",
    "\n",
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "#### Box-Cox Transformations\n",
    "\n",
    "This uniparametric family of transformations is used to increase to bring the distribution of a variable closer to normality. It is defined by:\n",
    "$$\n",
    "X^* =\n",
    "\\begin{cases}\n",
    "\\dfrac{X^{\\lambda} - 1}{\\lambda} & \\text{ if }\\lambda\\neq 0 \\\\[3mm]\n",
    "\\log(X) & \\text{ if }\\lambda = 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4f242",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "### Adding New Variables.\n",
    "\n",
    "Sometimes adding new variables increases a model's performance. The scatterplot for $X_1$ and $X_2$ showed a clear non-linear boundary between the output classes, like a parabola. A model with **non linear functions** of $X_1$ as inputs, such as $X_1^2$, can do a better job at finding this boundary. We will explore this in future sessions, along with the concept of **interaction** between inputs. And to account for interactions we will also add new variables to the data, computed from the original ones.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e562a",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "### Dropping the Initial Categorical Inputs\n",
    "\n",
    "We are approaching the end of preprocessing after finishing graphical EDA. We kept the original categorical inputs for this, but we should now drop them.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTR.drop(columns=cat_inputs, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b32f6",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "### Pipelines for Preprocessing.\n",
    "\n",
    "Many preprocessing steps (i.e. variable transformations or new variables addition) benefit from the pipeline framework in `scikit-learn`. This also holds for the subject of the next paragraph: the sampling methods used to deal with imbalance. So we defer the discussion of pipelines until we have introduced imbalance.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8075da20",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322ada4",
   "metadata": {},
   "source": [
    "# 2 Preproc; Step 7 : Check out for Class Imbalances  {.unnumbered .unlisted}\n",
    "\n",
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "### The Problem of Imbalanced Data\n",
    "\n",
    "A classification problem is called **imbalanced** when there is a significative difference between the proportion of the output variable classes. In such cases we use the terminology **majority class** (or classes in multiclass problems) and **minority class** (or classes) to distinguish between classes that take up a bigger proportion of the dataset and those who do not. \n",
    "\n",
    "Severe imbalance, say like 90% of the majority class, is a challenge fpr classification algorithms because the model can take the easy way out of always predicting the majority class. And then its predictions will be correct in approximately 90% of the cases, for free! \n",
    "\n",
    "There are several ways in which this issue can be addressed. The common ones are:\n",
    "\n",
    "+ **Random undersampling:** randomly subset all the classes in the training set so that their class frequencies match the least prevalent class. \n",
    "+ **Random oversampling:** randomly sample (with replacement) the minority class to be the same size as the majority class.\n",
    "+ **Synthetic and hybrid methods:**  synthesize new data points in the minority class or use some criterion to select which majority class points to remove (SMOTE and ROSE methods and their relatives).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2306df65",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ebe6a",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false}\n",
    "     \n",
    "### Detecting Imbalance\n",
    "\n",
    "We look at the relative frequencies of the classes for the putput variable **in the training set**.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a39ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "YTR.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f375442",
   "metadata": {},
   "source": [
    "With a 65% vs 35% proportion this dataset is certainly not balanced. But this is an instance of what is considered *mild imbalance*. Therefore, we will not deal with imbalance right now for this particular example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e43bbdb",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445708c0",
   "metadata": {},
   "source": [
    "# Pipelines for Preprocessing\n",
    "\n",
    "::: {.callout-note icon=\"false}\n",
    "     \n",
    "### Steps in a pipeline.\n",
    "\n",
    "We will later seer that pipelines are a general tool that can be used to describe much of the modeling process. But to begin working with them we will first focus in their use for preprocessing. Essentially a pipeline is a description of the steps we want Python to follow when processing the data. \n",
    "\n",
    ":::\n",
    "\n",
    "For our first example we will consider a simple pipeline with just one step, in which the numeric variables are standardized. First we do the required imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f2acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d5900",
   "metadata": {},
   "source": [
    "And now we provide a description of the standardization step and give it a name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b9209",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689e2b0",
   "metadata": {},
   "source": [
    "We want to apply this transformer *only to the numeric variables*, leaving the (one hot encoded) categorical features unchanged. Let us see how we do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6491b2c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032fc55",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "# Combining Different Transformers with `ColumnTransformer`\n",
    "\n",
    "This is where `ColumnTransformer` comes into play: it can be used to combine transformers that affect separate sets of columns into a single global transformer. In this case we want to use our `num_transformer` for the variables in `num_inputs`. And to indicate that the remaining categorical inputs (those in `ohe_inputs`) are not transformed at all we use the string `\"passthrough\"` that `ColumnTransformer` reads as 'do nothing'.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c57815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transformer, num_inputs),\n",
    "        (\"cat\", \"passthrough\", ohe_inputs),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea320a3",
   "metadata": {},
   "source": [
    "With the transformer ready we use it to make a pipeline with a single step, named `prep` (as in preprocessing). We have selected pandas dataframe as output format for easier visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_pipe = Pipeline(steps=[('prep', preprocessor)])\n",
    "_ = preproc_pipe.set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630b93c",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "### Exercise 006\n",
    "\n",
    "In your Jupyter notebook remove the `_ =` that we have used to supress the output and run the above cell again. Play with the diagram!\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960de72f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4379b",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "### Applying the Pipeline \n",
    "\n",
    "To apply this pipeline to the data we have to **fit and transform** the data with the pipeline. You may conceptually think of the **fit** part as computing the mean and deviation needed for the standardization, and the second part where we actually **transform** the data. They can be done separately but for user convenience there is a method called `fit_transform` that we now apply to `XTR`. We will see what the output looks like:\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890427bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_XTR = preproc_pipe.fit_transform(XTR)\n",
    "preproc_XTR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391da5f",
   "metadata": {},
   "source": [
    "Note that the names of the variables in the transformed data set reflect their path through the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0dbfb6",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "### Exercise 007\n",
    "\n",
    "Check that the numeric inputs have been really standardized.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a2c11",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7763a788",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "### Applying the Preprocessing Pipeline to the Test Set\n",
    "\n",
    "Now that we have seen how to transform the training set it is only natural to apply the preprocessing to the test set as well. This is straightforward.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ab2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_XTS = preproc_pipe.fit_transform(XTS)\n",
    "preproc_XTS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428c1c5",
   "metadata": {},
   "source": [
    "The names of the transformed variables match those in the training set as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad327a",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "### Exercise 008\n",
    "\n",
    "Is this transformed test set **necessarily** standardized?  \n",
    "Also note that we never dropped the original categorical inputs or the highly correlated features from `XTS`. What happened to them?  \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688757e8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b309c2d",
   "metadata": {},
   "source": [
    "::: {.callout-note icon=\"false}\n",
    "\n",
    "### Inverse Transform. A Deeper Dive into Transformers\n",
    "\n",
    "Can we recover the training set from the preprocessed data? Indeed we can and it will give us a chance to get acquainted with the structure of pipelines. This is a more complex section than most of the preceding ones. So do not worry if you do not really get it the first time! You can return here once you have gained experience working with pipelines.\n",
    " \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed9268e",
   "metadata": {},
   "source": [
    "We need two ingredients. First we need to get to the (already fitted) scaler transformer. We can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preproc_pipe[\"prep\"].transformers_[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d313b00",
   "metadata": {},
   "source": [
    "We recommmend you to examine the right hand side incrementally. That is, execute first `preproc_pipe[\"prep\"]` in a cell and see its output. Then proceed to `preproc_pipe[\"prep\"][0]` and do the same, and so on until you get the full picture. The second ingredient is simpler, we need the names of the variables we are inverse-transforming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d256209",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs_preproc = preproc_XTR.columns[[0, 1]]\n",
    "num_inputs_preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f07409",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8664970",
   "metadata": {},
   "source": [
    "Now we can apply the inverse_transform method of the scaler to the transformed variables. Watch out: the format of the result is a numpy array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_inv = scaler.inverse_transform\n",
    "scaler_inv(preproc_XTR[num_inputs_preproc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77033054",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "### Exercise 009\n",
    "\n",
    "Check the result against the numerical inputs in `XTR`.\n",
    " \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42abf616",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf970f",
   "metadata": {},
   "source": [
    "::: {.callout-tip icon=false}\n",
    "\n",
    "# Next Session\n",
    "\n",
    "Now we have a preprocessed data set, and we are ready to move to the last three steps of our plan to solve a classification problem: \n",
    "\n",
    "3) Choose a model\n",
    "4) Fit the parameters of the model\n",
    "5) Assess the model quality\n",
    "\n",
    "In the next sessions we will study some important families of models: logistic regression and KNN models.  \n",
    "\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
