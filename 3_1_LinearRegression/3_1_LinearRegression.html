<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="F.San Segundo &amp; N.Rodríguez ">

<title>Chapter 3, Part 1: Introduction to Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="3_1_LinearRegression_files/libs/clipboard/clipboard.min.js"></script>
<script src="3_1_LinearRegression_files/libs/quarto-html/quarto.js"></script>
<script src="3_1_LinearRegression_files/libs/quarto-html/popper.min.js"></script>
<script src="3_1_LinearRegression_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="3_1_LinearRegression_files/libs/quarto-html/anchor.min.js"></script>
<link href="3_1_LinearRegression_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="3_1_LinearRegression_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="3_1_LinearRegression_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="3_1_LinearRegression_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="3_1_LinearRegression_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#session-setup" id="toc-session-setup" class="nav-link active" data-scroll-target="#session-setup">Session Setup</a></li>
  <li><a href="#regression" id="toc-regression" class="nav-link" data-scroll-target="#regression">Regression</a></li>
  <li><a href="#simple-linear-regression" id="toc-simple-linear-regression" class="nav-link" data-scroll-target="#simple-linear-regression">Simple Linear Regression</a>
  <ul class="collapse">
  <li><a href="#the-regression-line" id="toc-the-regression-line" class="nav-link" data-scroll-target="#the-regression-line">The Regression Line</a></li>
  <li><a href="#performance-measures-for-regression-root-mean-squared-error-rmse" id="toc-performance-measures-for-regression-root-mean-squared-error-rmse" class="nav-link" data-scroll-target="#performance-measures-for-regression-root-mean-squared-error-rmse">Performance Measures for Regression Root Mean Squared Error (RMSE)</a></li>
  </ul></li>
  <li><a href="#distributional-assumptions-of-the-linear-regression-model" id="toc-distributional-assumptions-of-the-linear-regression-model" class="nav-link" data-scroll-target="#distributional-assumptions-of-the-linear-regression-model">Distributional Assumptions of the Linear Regression Model</a>
  <ul class="collapse">
  <li><a href="#model-bias-and-variance-in-simple-linear-regression." id="toc-model-bias-and-variance-in-simple-linear-regression." class="nav-link" data-scroll-target="#model-bias-and-variance-in-simple-linear-regression.">Model Bias and Variance in Simple Linear Regression.</a></li>
  </ul></li>
  <li><a href="#linear-regression-in-higher-dimensions-multiple-linear-regression" id="toc-linear-regression-in-higher-dimensions-multiple-linear-regression" class="nav-link" data-scroll-target="#linear-regression-in-higher-dimensions-multiple-linear-regression">Linear Regression in Higher Dimensions: Multiple Linear Regression</a></li>
  <li><a href="#train-and-diagnose-multiple-regression-linear-models-in-python" id="toc-train-and-diagnose-multiple-regression-linear-models-in-python" class="nav-link" data-scroll-target="#train-and-diagnose-multiple-regression-linear-models-in-python">Train and Diagnose Multiple Regression Linear Models in Python</a></li>
  <li><a href="#failures-in-the-the-linear-model-requirements" id="toc-failures-in-the-the-linear-model-requirements" class="nav-link" data-scroll-target="#failures-in-the-the-linear-model-requirements">Failures in the the Linear Model Requirements</a></li>
  <li><a href="#the-problem-of-collinearity" id="toc-the-problem-of-collinearity" class="nav-link" data-scroll-target="#the-problem-of-collinearity">The Problem of Collinearity</a></li>
  <li><a href="#models-with-categorical-inputs" id="toc-models-with-categorical-inputs" class="nav-link" data-scroll-target="#models-with-categorical-inputs">Models with Categorical Inputs</a></li>
  <li><a href="#interactions" id="toc-interactions" class="nav-link" data-scroll-target="#interactions">Interactions</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Chapter 3, Part 1: Introduction to Regression</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li></ul></div></div>
<p class="subtitle lead">Machine Learning</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">F.San Segundo &amp; N.Rodríguez  </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
             Based on notes by Professors A.Muñoz, J.Portela &amp; J.Pizarroso
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 2025</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Recommended reading:
</div>
</div>
<div class="callout-body-container callout-body">
<p>See <a href="#References">References</a> section at the end for details.</p>
<ul>
<li>Chapter 3 of <span class="citation" data-cites="ISLP2023">(<a href="#ref-ISLP2023" role="doc-biblioref">James et al. 2023</a>)</span></li>
<li>Chapter 2 (pp.&nbsp;47 and following) of <span class="citation" data-cites="IMLPY">(<a href="#ref-IMLPY" role="doc-biblioref">Muller and Guido 2017</a>)</span></li>
<li><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html">Chapter 5 of <span class="citation" data-cites="PDSH">(<span>VanderPlas 2016</span>)</span></a></li>
<li>Chapters 2 and 3 of <span class="citation" data-cites="ESLI2009">(<a href="#ref-ESLI2009" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2009</a>)</span></li>
<li>Chapter 11 of <span class="citation" data-cites="pml1">(<a href="#ref-pml1" role="doc-biblioref">Murphy 2022</a>)</span>.</li>
</ul>
</div>
</div>
<hr>
<section id="session-setup" class="level1">
<h1>Session Setup</h1>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Libraries
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let us begin by loading the libraries we will use.</p>
</div>
</div>
<div class="cell" data-execution_count="1">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">### Load necessary modules -------------------------------</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># interactive plotting</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># %matplotlib inline</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>config InlineBackend.figure_format <span class="op">=</span> <span class="st">'png'</span> <span class="co"># ‘png’, ‘retina’, ‘jpeg’, ‘svg’, ‘pdf’</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting libraries</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># sns.set()</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Data management libraries</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Machine learning libraries</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, cross_val_score</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># others</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Connecting statsmodels with sklearn via sklearn2pmml and patsy </span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.api <span class="im">import</span> OLS</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.outliers_influence <span class="im">import</span> variance_inflation_factor </span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn2pmml.statsmodels <span class="im">import</span> StatsModelsRegressor</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># patsy for lineal model formula language</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> patsy <span class="im">as</span> ps</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> patsy_utils.patsy_formulaic_transformer <span class="im">import</span> FormulaTransformer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<hr>
</section>
<section id="regression" class="level1">
<h1>Regression</h1>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Introduction
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this session we begin our study of the second large family of supervised learning projects; namely, regression problems. The difference between regression and the classification problems that we have been discussing is the <strong>nature of the output variable <span class="math inline">\(Y\)</span></strong>. While in classification <span class="math inline">\(Y\)</span> was a factor (categorical), in regression <strong><span class="math inline">\(Y\)</span> is a numerical variable</strong>.</p>
<p>This change in <span class="math inline">\(Y\)</span> will impact some parts of our work. For example, the way we define the <strong>prediction error</strong> that we use to measure the model’s performance has to change. But we will see that many concepts are common to both classification and regression. For example we continue to assume that the true relation between inputs <span class="math inline">\(X\)</span> and output <span class="math inline">\(Y\)</span> is <span class="math display">\[Y = f(X) + \epsilon,\qquad\text{ with }E(\epsilon) = 0\]</span> where <span class="math inline">\(\epsilon\)</span> is a (0-mean) random variable, called <strong>random error</strong>. And the bias-variance trade-off that we met in Session 2_3 still holds without changes (because it was in fact expressed in a regression context): <span class="math display">\[
\underbrace{E\left(y_0 - \hat f(x_0)\right)^2}_{\text{Expected generalization error}} =
\underbrace{\operatorname{Var}\left(\hat f(x_0)\right)}_{\text{Variance}} +
\underbrace{\operatorname{Bias}\left(\hat f(x_0)\right)^2}_{\text{Bias (squared)}} +
\underbrace{\operatorname{Var}\left(\epsilon\right)}_{\text{Irreducible error}}
\]</span></p>
<p>Just as we did with classification, we are going to meet different type of regression models. And as we do we will see that the idea of the flexibility of the model and the role it plays in this bias-variance tradeoff is essentially the same.</p>
</div>
</div>
<hr>
</section>
<section id="simple-linear-regression" class="level1">
<h1>Simple Linear Regression</h1>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Introduction to Linear Models
</div>
</div>
<div class="callout-body-container callout-body">
<p>Our first family of regression models was historically also the first one. Linear models have been used for many years now and they continue to be one of the cornerstones of modeling. We said that Logistic Regression was the reference model in classification, and linear models play the same role in regression: they are easy to train, their theoretical foundation is very well understood and they are highly interpretable. The major drawback of the basic version of linear models is their lack of flexibility. They are essentially rigid models and in many cases they suffer from high bias.</p>
<p>We will begin our discussion as usual in a low dimensional setting and using synthetic datasets to help us visualize the core ideas of these methods.</p>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example: (Oxygen Consumption and Temperature in Common Blue Tits)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Authors Haftorn and Reinertsen wrote a 1985 paper (<span class="citation" data-cites="Haftorn85">(<a href="#ref-Haftorn85" role="doc-biblioref">Haftorn and Reinertsen 1985</a>)</span>, see the <a href="#References">References Section</a> below) about the relationship between oxygen consumption and air temperature in the incubating females of the <a href="https://en.wikipedia.org/wiki/Eurasian_blue_tit"><em>Eurasian blue tit (Cyanistes caeruleus)</em></a> a common small passerine bird like the one in the picture below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig/3_1_Herrerillo.png" class="img-fluid figure-img" style="width:40.0%" alt="Herrerillo Común"></p>
</figure>
</div>
<p>Think about this questions:<br>
+ What do you expect to happen with the oxygen consumption as the air temperature drops? + Are those two variables equally easy to measure?</p>
<p>The two variables involved in this study are both of them continuous, so the result of a set of measurements will be a collection of pairs <span class="math display">\[
\quad\\
(x_1, y_1),\, (x_2, y_2), \ldots,\, (x_n, y_n)
\quad\\
\]</span> that we can represent as points in a scatter plot like the ones we have seen before.</p>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Simple linear Model is about Describing a Trend
</div>
</div>
<div class="callout-body-container callout-body">
<p>This example can be described in symbols as <span class="math display">\[
O_2\sim T
\]</span> We are using air temperature <span class="math inline">\(T\)</span> as input variable and <span class="math inline">\(O_2\)</span> as output because of what your intuition told you after thinking about the questions above. That same intuition is surely leading you to the conjecture that when you place air temperature in the <span class="math inline">\(X\)</span> axis and oxygen consumption <span class="math inline">\((O_2)\)</span> on the <span class="math inline">\(Y\)</span> axis then the points of the scatter plot will describe a trend such as this one:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig/3_1_IntuicionProblemaHerrerillos.png" class="img-fluid figure-img" style="width:40.0%" alt="Intuition for the O2 consumption vs temperature"></p>
</figure>
</div>
<p>The data collected by the authors of that paper resulted in this scatter plot (there are two samples represented by different symbols, one for the incubation period and the other outside that period):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig/3_1_Figura05HaftornReinertsenHerrerillos.png" class="img-fluid figure-img" style="width:40.0%" alt="Haftorn and Reinertsen 1985 Scatterplot"></p>
</figure>
</div>
<p>This confirms our intuition and is a typical example of the kind of pattern we are looking for. You can see that the authors have drawn two straight lines (one for each sample). This straight lines are a <em>mathematical abstract model</em> of the trend that appears in the data. Many other variables, aside from air temperature, play a role in the <span class="math inline">\(O_2\)</span> consumption for these birds. So none expects the relation between these two variables to be perfectly described by such an abstraction as a straight line is! Every real world relation is noisy, even the most controlled lab experiments include some level of noise in their measurements. But the straight line in that picture is the model or signal that <em>best describes</em> the trend that we observe in the data.And of course we expect this model to be biased. But then, in what sense do we say that this line is <em>the best among all posable lines</em>? Note that there may be functions other than straight lines that do a better job. But for now the competition is restricted to straight lines.</p>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Signal and Noise and Linear Relations. Three different datasets example.
</div>
</div>
<div class="callout-body-container callout-body">
<p>The three scatter plots below show examples of noisy relations <span class="math inline">\(Y\sim X\)</span> between two continuous variables. The first one on the left is a relation that can be well described using a straight line as a model, like in the birds example. The central panel also shows a clear and well defined relation between the variables (<em>strong signal, weak noise</em>). But it would be foolish to try to describe it with a straight line. The last one, finally, does not seem to show any discernible pattern (<em>weak or no signal, very strong noise</em>), and so it seems that this variables are unrelated.</p>
<p>The code below makes these plots and creates the corresponding datasets in your folder (overwrites them if they are already there).</p>
</div>
</div>
<div class="cell" data-execution_count="2">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>run <span class="op">-</span>i <span class="st">"./3_1_models_01.py"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>
<hr>
<p>::: {.callout-note icon=false}</p>
<section id="the-regression-line" class="level3">
<h3 class="anchored" data-anchor-id="the-regression-line">The Regression Line</h3>
<p>So we will try to focus in a situation like the first panel from the left, where the relation <span class="math inline">\(Y\sim X\)</span> seems to be well suited to be described using a straight line. We want to choose the <strong>best possible</strong> straight line, and we will call it the <strong>regression line</strong> for <span class="math inline">\(Y\sim X\)</span>.</p>
<p>The data for that example (first panel) are contained in the <a href="./data/07_simple_linear_regression_01.csv">07_simple_linear_regression_01.csv</a> file. In this example the best line is the one that appears below. We will soon discuss the code details. But to get there we will follow these steps:</p>
<ol type="1">
<li>First we will clarify what we mean by <em>the best straight line.</em></li>
<li>Then we will see how to obtain that line, both theoretically and using Python.</li>
<li>We will see that sometimes even the best possible line is really bad.</li>
</ol>
<p>:::</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Linear Regression Basic Models in Scikit-learn
</div>
</div>
<div class="callout-body-container callout-body">
<p>Scikit provides the class <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"><code>LinearRegression</code></a> to fit linear regression models. This class can be used in pipelines just the ones we have been using until now.</p>
</div>
</div>
<div class="cell" data-execution_count="3">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"./3_1_simple_linear_regression_01.csv"</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span>[<span class="st">"X0"</span>]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> <span class="st">"Y"</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>sk_lm <span class="op">=</span> LinearRegression( ) </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>sk_lm.fit(df[inputs], df[output])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="3">
<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;LinearRegression<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html">?<span>Documentation for LinearRegression</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>LinearRegression()</pre></div> </div></div></div></div>
</div>
</div>
<div class="cell" data-execution_count="4">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>b0 <span class="op">=</span> sk_lm.coef_[<span class="dv">0</span>]</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> sk_lm.coef_[<span class="dv">0</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>b0, b1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(2.0029864676738933, 2.0029864676738933)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="5">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>sns.set_theme(rc<span class="op">=</span>{<span class="st">'figure.figsize'</span>:(<span class="dv">5</span>, <span class="dv">5</span>)})</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.linspace(df[inputs].<span class="bu">min</span>(), df[inputs].<span class="bu">max</span>(), <span class="dv">10</span>).ravel()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>Y_new <span class="op">=</span> sk_lm.predict(pd.DataFrame({<span class="st">"X0"</span>:  X_new}))</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>plt.plot(df[inputs], df[output], <span class="st">'bo'</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, Y_new, <span class="st">'r-'</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"The regression line is y = </span><span class="sc">{:.4}</span><span class="st"> + </span><span class="sc">{:.4}</span><span class="st"> x"</span>.<span class="bu">format</span>(b0, b1))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Situation with Linear Models in Python
</div>
</div>
<div class="callout-body-container callout-body">
<p>The problem with the LinearRegression class (and with scikit in general) is that it is focused on <em>modeling for prediction</em>. Which is fine if that is you care about. But for some models, and particularly in the case of linear models, we want more information. For example, about the significance of coefficients like we did for logistic regression.</p>
<p>As we saw then, the Python library that provides that type of information about linear models is called <a href="https://www.statsmodels.org/">Statsmodels</a>. Our work with Python would be much easier if Statsmodels and Scikit shared some common principles. But unfortunately that is not the case. There are, however, third party attempts to combine the information provided by both libraries. We will use some of these connections, but in order to do that we need to upgrade our Python environment.</p>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Conda environment upgrade.
</div>
</div>
<div class="callout-body-container callout-body">
<p>The code for this session will not work until you upgrade the <code>MLMIC25</code> environment. To do that follow the next instructions:</p>
<ol type="1">
<li>Make a clone (backup) of your environment by opening a terminal and running:<br>
<code>conda create --name MLMIC25bkup --clone MLMIC25</code><br>
Answer yes when you are asked if you want to proceed.<br>
<br><br>
</li>
<li><strong>Remember to activate your MLMIC25 environment!</strong><br>
<br><br>
</li>
<li>Run the following commands one by one (so that if you run into trouble we will know the culprit).<br>
<code>pip install patsy</code><br>
<code>pip install sklearn2pmml</code><br>
<br><br>
</li>
<li>After you have used the environment for several days, if things remain stable and you need to free the space in your disk, you can get rid of the backup by running<br>
<code>conda remove --name MLMIC25bkup --all</code></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A First Linear Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>The following code (uncomment and run the cell to load it) uses our upgraded environment to fit a regression model (from statsmodels) using the pipeline framework (from scikit) that we are used to.</p>
<p><strong>Note:</strong> you may get a warning that you can ignore by now.</p>
</div>
</div>
<div class="cell" data-execution_count="6">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>run <span class="op">-</span>i <span class="st">"3_1_models_02.py"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 001
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Browse the code and do not worry if you do not get it just now. It will become clear as we move forward. Ask questions, though!</p></li>
<li><p>Run the code for the datasets <code>3_1_simple_linear_regression_02.csv</code> and <code>3_1_simple_linear_regression_03.csv</code>. These correspond to the center and right panel of the examples above.</p></li>
<li><p>When you run this exercise you will see that the regression line does not seem to be a good model of the data in the last both cases, but for different reasons. Can you explain the difference?</p></li>
</ul>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Equation of the Regression Line. Predicted Values and Residuals.
</div>
</div>
<div class="callout-body-container callout-body">
<p>To carry on with our plan we introduce some notation. The equation of the regression line will be written as: <span class="math display">\[
y = b_0 + b_1\, x
\]</span> where <span class="math inline">\(b_1\)</span> is the <strong>slope</strong> of the line. The sign of <span class="math inline">\(b_1\)</span> reflects if the line is going up or down as we move to the right. Its absolute value equals how many units <span class="math inline">\(y\)</span> changes when <span class="math inline">\(x\)</span> changes by one unit. The value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x = 0\)</span> is <span class="math inline">\(b_0\)</span>, the <strong>intercept</strong>.</p>
<p>Let us assume that <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are known values. Then if the sample points are: <span class="math display">\[
(x_1, y_1),\, (x_2, y_2), \ldots,\, (x_n, y_n)
\]</span> when we plug each <span class="math inline">\(x_i\)</span> in the equation of the regression line we get <em>another value</em> of <span class="math inline">\(y\)</span> (usually different from <span class="math inline">\(y_i\)</span>), that we call the <strong>predicted value</strong> and denote by <span class="math display">\[
\hat y_i = b_0 + b_1\, x_i,\quad\text{for each }\quad i =1,\ldots,n
\]</span></p>
<p>The <strong>residuals</strong> are the differences: <span class="math display">\[e_1 = y_1 - \hat y_1,\,\quad  e_2 = y_2 - \hat y_2,\quad  \ldots\quad ,\,e_n = y_n - \hat y_n \]</span></p>
<p>All these terms are illustrated in the figure below: + The red dots correspond to the original sample points, with (vertical coordinates) <span class="math inline">\(y_1, \ldots, y_n\)</span>. + The green dots are the predicted values, with <span class="math inline">\(\hat y_1, \ldots, \hat y_n\)</span>. + The residuals <span class="math inline">\(e_1, \ldots, e_n\)</span> measure the lengths of the vertical segments connecting each red point to the corresponding green one.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig/3_1_Residuals.png" class="img-fluid figure-img" style="width:60.0%" alt="Residuals and Predicted Values in SImple Linear Regression"></p>
</figure>
</div>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mean Squared Error (MSE)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Since the residuals measure the vertical distance from the predicted to the actual value, a good regression line should give us a <strong>small residuals average</strong>. Note that we need to think about some kind of residual average, because otherwise the line could be good in some regions and really bad in some others.</p>
<p>The first idea that comes to mind is to take the mean of the residuals. But then the positive and negative ones could cancel and that would be a misleading measure of the quality of that line. We could take absolute values, but this way leads to complicated mathematics. Therefore we give a different definition.</p>
<p>The <strong>squared error</strong> or also <strong>RSS (residuals squared sum)</strong> for the line given by <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> is: <span class="math display">\[
RSS =
\sum_{i=1}^n e_i^2 =
\sum_{i=1}^n(y_i-\hat y_i)^2 =
\sum_{i=1}^n(y_i-b_0-b_1\cdot x_i)^2.
\]</span> and the <strong>(sample) square mean error</strong> is <span class="math display">\[\operatorname{SME} = \dfrac{RSS}{n-1}\]</span> You can probably guess the reason why we divide by <span class="math inline">\(n - 1\)</span> instead of <span class="math inline">\(n\)</span>, we will return to that issue below.</p>
<p>The idea of the square error (and the <em>least squares method</em>) is illustrated <a href="https://www.geogebra.org/m/pryzjnya">in this figure</a>: <img src="./fig/3_1_QuadraticErrorMeaning.png" class="img-fluid" style="width:80.0%" data-fig-align="center" alt="https://www.geogebra.org/m/pryzjnya"></p>
</div>
</div>
<hr>
<p>::: {.callout-note icon=false}</p>
</section>
<section id="performance-measures-for-regression-root-mean-squared-error-rmse" class="level3">
<h3 class="anchored" data-anchor-id="performance-measures-for-regression-root-mean-squared-error-rmse">Performance Measures for Regression Root Mean Squared Error (RMSE)</h3>
<p>We have met the concept of residuals measure in the context of simple linear regression. But whenever we have a model estimator <span class="math inline">\(\hat f\)</span> that estimates the true relationship <span class="math display">\[Y = f(\bar X) + \epsilon\]</span> the general definition of residual as <span class="math display">\[e_i = y_i - \hat y_i = f(\bar X_i) - \hat f(\bar X_i)\]</span><br>
can be applied. The residuals themselves are estimates of the error term in the true relation.</p>
<p>Therefor the mean squared error is defined for any regression model. The main problem using it is that its units are not those of the original problem. And as usual we take the square root to define the <strong>RMSE (Root Mean Squared Error)</strong> <span class="math display">\[
\operatorname{RSME} = \sqrt{\dfrac{\displaystyle\sum_{i=1}^n e_i^2}{n}}
\]</span> In classification problems we used accracy as the general performance measure. And in fact it is used as default in scikit classifiers. This RMSE measure plays a similar role for regression models. There are many alternatives and we will find some of them in the examples of future sessions. For example, an alternative using absolute values instead of squares is the <strong>mean absolute error (MAE)</strong> defined as: <span class="math display">\[
\operatorname{MAE} = \dfrac{\displaystyle\sum_{i=1}^n |e_i|}{n}
\]</span></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 002
</div>
</div>
<div class="callout-body-container callout-body">
<p>The residuals of the model we fitted in the previous exercise are stored in the <code>model.results_.resid</code>. Use it to compute directly the SME and RSME for the dataset in <code>3_1_simple_linear_regression_01.csv</code>.</p>
<p>What happens if you sum the residuals without squaring them first?</p>
</div>
</div>
<div class="cell" data-execution_count="10">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># %run -i "../exclude/code/3_1_Exercise_002.py"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Coefficients of the Regression Line
</div>
</div>
<div class="callout-body-container callout-body">
<p>The best line (the linear regression line) is the one with those values of <span class="math inline">\(b_0, b_1\)</span> that result in the least squared error. Using Calculus to solve that optimization problem leads to this:</p>
<p><strong>Equation of the Linear Regression Line.</strong> <span class="math display">\[
\hspace{1cm}(y-\bar y)=\dfrac{\text{Cov}(x,y)}{s^2(x)}\cdot (x-\bar x)
\]</span> where the <strong>sample covariance</strong> is: <span class="math display">\[\text{Cov}(x,y)=\dfrac{\displaystyle\sum_{i=1}^{n}(x_i-\bar x)(y_i-\bar y)}{n-1}
\]</span> Therefore the coefficients of the regression line (in the one dimensional setting) are: <span class="math display">\[
\begin{cases}
b_1= &amp; \dfrac{\text{Cov}(x,y)}{s^2(x)}\\[5mm]
b_0= &amp; \bar y - \dfrac{\text{Cov}(x,y)}{s^2(x)}\cdot\bar x
\end{cases}
\]</span></p>
<p><strong>Note</strong> in particular these results have two important consequences: + The regression line always goes through the <em>sample center</em>, the point <span class="math inline">\((\bar X, \bar Y)\)</span>. + The sum of the residuals for the regression line is always 0.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 003
</div>
</div>
<div class="callout-body-container callout-body">
<p>Locate in <code>3_1_models_02.py</code> how to get to the model coefficients.</p>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Analysis of Variance and ANOVA Identity for Simple Linear Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>Recall that the residual squared error, also called <strong>residual sum of squares</strong> or <strong>SSresidual</strong>, was defined as: <span class="math display">\[SSresidual =  \sum_{i=1}^n e_i^2 =  \sum_{i=1}^n(y_i-\hat y_i)^2 =\sum_{i=1}^n(y_i-b_0-b_1\cdot x_i)^2.\]</span> This squared error is defined from the residuals of the linear regression and it is therefore connected to the <em>noise</em> part of that <em>noise / signal</em> duality we have been talking about. That is why we are .</p>
<p>You may have noticed that the second term above resembles the numerator of the variance of <span class="math inline">\(y\)</span>. Playing with that idea and doing some algebra we arrive at this very important expression:</p>
<p><strong>Anova Identity for Simple Linear Regression</strong> <span class="math display">\[
\underbrace{\displaystyle\sum_{i=1}^{n}(y_i-\bar y)^2 }_{SStotal}=
\underbrace{\sum_{i=1}^n e_i^2}_{SSresidual} +
\underbrace{\sum_{i=1}^n(\hat y_i-\bar y)^2}_{SSmodel}
\]</span></p>
<p>The Anova identity is a purely algebraic equation, it does not depend on assumptions about the distribution of the variables in the model. This equation provides a decomposition of the total spread <span class="math inline">\(SStotal\)</span> of the response <span class="math inline">\(y\)</span> values. We have already mentioned that <span class="math inline">\(SSresidual\)</span> is related to the <em>noise</em> in the data. But the <span class="math inline">\(SSmodel\)</span> term is obtained from the predicted values of the model. That is, even if there was no noise at all and the original data were perfectly placed on this regression line, their <span class="math inline">\(y\)</span> coordinates would be vertically spread, <em>simply due to the slope of the regression line itself</em>. That means that this component of the total <span class="math inline">\(y\)</span> spread is the part that is fully <strong>explained by the model</strong>. The following picture shows two extreme situations in the Anova identity. In each case one of the terms in the right hand side sum equals zero.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig/3_1_Anova_Signal_Noise.png" class="img-fluid figure-img" style="width:80.0%" alt="Signal and Noise in the Anova identity for Linear Regression"></p>
</figure>
</div>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Goodness of Fit and Pearson’s Correlation Coefficient
</div>
</div>
<div class="callout-body-container callout-body">
<p>Dividing the Anova identity by <span class="math inline">\(SStotal\)</span> we arrive at: <span class="math display">\[
1 = \dfrac{SS_{residual}}{SS_{total}} + \dfrac{SS_{model}}{SS_{total}} =
\dfrac{SS_{residual}}{\displaystyle\sum_{i=1}^{n}(y_i-\bar y)^2}
+
\dfrac{\displaystyle\sum_{i=1}^n(\hat y_i-\bar y)^2}{\displaystyle\sum_{i=1}^{n}(y_i-\bar y)^2}
\]</span> The division guarantees that the terms on the right hand side:<br>
<span class="math inline">\((a)\)</span> are <em>dimensionless</em> (or <em>unit-less</em>) and do not depend on the scale of the problem.<br>
<span class="math inline">\((b)\)</span> are <em>positive and add up to 1</em>.<br>
<span class="math inline">\((c)\)</span> the first term refers to the <em>noise</em> component of the data, while the second refers to the <em>model</em> (the regression line).</p>
<p>In particular, this seems to indicate that quality of the regression line increases the bigger this second term is (and therefore, the smaller the first one is).</p>
<p>If we plug <span class="math inline">\((\hat y_i - \bar y) = \dfrac{\text{Cov}(x,y)}{s^2(x)}(x - x_i)\)</span> into <span class="math inline">\(SS_{model}\)</span> we arrive at <span class="math display">\[
1 = \dfrac{SS_{residual}}{\displaystyle\sum_{i=1}^{n}(y_i-\bar y)^2} + \left(
\dfrac{\text{Cov}(x,y)}{s(x)\cdot s(y)} \right)^2
\]</span> And the term in parenthesis is therefore a measure of the <strong>goodness of fit</strong> to the data provided by this regression line model. Let us give it a name:</p>
<p><strong>Pearson’s Correlation Coefficient <span class="math inline">\(R\)</span></strong> <span class="math display">\[
R = \text{Cor}(x,y) = \overbrace{\text{Cor}(y, x)}^{\text{is symmetrical }} = \dfrac{\text{Cov}(x,y)}{{s(x)\cdot s(y)}}
\]</span></p>
<p>Using the correlation coefficient we can rewrite some previous results: + The <strong>Anova identity</strong> is <span class="math display">\[1 = \dfrac{SS_{residual}}{SS_{total}} + R^2\]</span> + The <strong>regression line equation</strong> is <span class="math display">\[(y - \bar y) = \text{Cor}(x,y)\dfrac{s(y)}{{s(x)}} (x - \bar x)\]</span></p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Properties of the Correlation Coefficient
</div>
</div>
<div class="callout-body-container callout-body">
<p>Symmetry: <span class="math inline">\(\text{Cor(X, Y)} = \text{Cor(Y, X)}\)</span>. The correlation coefficient is a dimensionless number between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>. The sign of <span class="math inline">\(R\)</span> is the same the sign of the slope <span class="math inline">\(b_1\)</span> for the regression line. Thus, if <span class="math inline">\(R &gt; 0\)</span> the line increases and conversely.</p>
<p><span class="math inline">\(R\)</span> only equals <span class="math inline">\(1\)</span> or <span class="math inline">\(-1\)</span> when <strong>every point</strong> in the sample is perfectly aligned on the regression line.</p>
<p><span class="math inline">\(R^2\)</span> is called the <strong>coefficient of determination</strong> and it represents the proportion (or percent) of the total <span class="math inline">\(y\)</span> variation that can be explained by the model.</p>
<p>Let <span class="math inline">\(\tilde x_i = \dfrac{x_i - \bar x}{s_x}\)</span> be the centered and scaled values of the <span class="math inline">\(x_i\)</span> and similarly let <span class="math inline">\(\tilde y_i\)</span> be the centered and scaled values of the <span class="math inline">\(y_i\)</span>. The regression line can be written <span class="math display">\[\tilde y_i = R\cdot \tilde x_i\]</span> This can be seen as a regression line for <span class="math inline">\(\tilde y\)</span> and <span class="math inline">\(\tilde x\)</span>. Its slope has an absolute value smaller than one, thus explaining the phenomenon known as <em>regression to the mean</em>, that gives name to the whole method.</p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of the Correlation Coefficient
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Whenever <span class="math inline">\(R\)</span> is close to 0, the fit of the line to the data is certainly bad.<br>
</li>
<li>Whenever the fit of the line to the data is good bad, then <span class="math inline">\(|R|\)</span> is close to 1.</li>
<li><strong>Be careful, it does not work the other way round!</strong> If the value <span class="math inline">\(|R|\)</span> is close to 1 that <strong>does not guarantee by itself the goodness of fit. Always check the fit, at least graphically!</strong></li>
</ul>
</div>
</div>
<p>Take a look at this figure and note that <em>the lowest <span class="math inline">\(R\)</span> value corresponds precisely to the one case where the line is a indeed a good model for the data</em>.</p>
<p><img src="./fig/3_1_PearsonR_Interpretation_01.png" class="img-fluid">{width75% fig-align=“center” fig-alt=“Interpretation of the Correlation COefficient”}</p>
<p>Another important observation about correlation is that it should not be interpreted as causality. Too often the news headlines run sentences like “using (or eating) A linked to cases of B”. And this often comes after a study found a correlation between those two things, without the study proving (or even trying to prove) any sort of causality.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig/3_1_XKCD552correlationCausation.png" class="img-fluid figure-img" style="width:75.0%" alt="XKCD correlation vs causation"></p>
</figure>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 004
</div>
</div>
<div class="callout-body-container callout-body">
<p>Use the <code>model.results_</code> object to find the value of <span class="math inline">\(R^2\)</span> in this case.</p>
</div>
</div>
<div class="cell" data-execution_count="16">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># %run -i "../exclude/code/3_1_Exercise_004.py"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<hr>
</section>
</section>
<section id="distributional-assumptions-of-the-linear-regression-model" class="level1">
<h1>Distributional Assumptions of the Linear Regression Model</h1>
<p>::: {.callout-note icon=false}</p>
<section id="model-bias-and-variance-in-simple-linear-regression." class="level3">
<h3 class="anchored" data-anchor-id="model-bias-and-variance-in-simple-linear-regression.">Model Bias and Variance in Simple Linear Regression.</h3>
<p>The linear models (regression lines) that we fitted in previous examples were obtained from a <strong>single concrete sample</strong>. And the discussion about <em>goodness of fit</em> above is concerned with <strong>how the regression line fitted the particular sample</strong> from where it came. But in order to get a clear understanding and a wider perspective of the notion of model variance we can use this simple type of models, and think in terms of populations and not just individual samples.</p>
<p>Our starting point is the idea that the pattern we have spotted in a sample hints at a pattern when we move up to the population. That <em>population level pattern</em> is a <em>Linear Model</em>, which again is a mathematical abstraction defined as follows:</p>
<p><span class="math display">\[
\hspace{1cm}Y = \underbrace{\beta_0 + \beta_1 \bar X}_{\text{model}} +
    \underbrace{\epsilon}_{\text{noise}}
\]</span> where <span class="math inline">\(\beta_0, \beta_1\)</span> are the model coefficients. Here we assume that the true form of the relation is <span class="math display">\[Y = f(\bar X) + \epsilon, \textbf{ with } f(\bar X) = \beta_0 + \beta_1 \bar X\]</span> It may perfectly well be the case that the true nature of <span class="math inline">\(f\)</span> is not linear (think about the center panel in the three-datasets example above). This is related to the <strong>model bias</strong> and the only way to fight it here would be by considering more flexible models (e.g.&nbsp;a linear model with quartic terms). We will see examples in the next sessions. But the idea of model variance is related to the dependance of the fit with respect to the sample, and we are going to explore that below.</p>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Statistical Assumptions for the Linear Regression Model.
</div>
</div>
<div class="callout-body-container callout-body">
<p>As we have seen the linear model is given by <span class="math display">\[
Y = {\beta_0 + \beta_1 X} + {\epsilon}
\]</span> where <span class="math inline">\(\beta_0, \beta_1\)</span> are the <strong>model coefficients</strong>.</p>
<p>The minimal requirement for the <strong>error variables</strong> is that their conditional (on <span class="math inline">\(\bar X\)</span>) mean is equal to 0. That is <span class="math display">\[E(\epsilon|\bar X) = 0\]</span> Taking this conditional average above leads to <span class="math display">\[E(Y|\bar X) = E({\beta_0 + \beta_1 X} + {\epsilon}|\bar X)\]</span> and using linearity of expectations <span class="math display">\[E(Y|\bar X) = \beta_0 + \beta_1 X\]</span> The linear model equation represents the expected value of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\bar X\)</span>.</p>
<p>Usually we would like to go beyond this and perform e.g.&nbsp;statistical tests of significance for the coefficients of the model. In order to do that we add extra requirements:</p>
<p>The error variables <span class="math inline">\(\epsilon\)</span> are supposed to be conditionally independent (given the value of <span class="math inline">\(X\)</span>) and with identical (conditional) normal distributions, all equal to <span class="math inline">\(N(0, \sigma)\)</span>.</p>
<p>The parameter <span class="math inline">\(\sigma^2\)</span> is the common or <strong>homogeneous variance</strong> of all these error variables.</p>
<p>Now, of course in any particular data analysis problem this assumptions may not hold. There are many diagnosis tools developed to check if they do, and we encourage you to read more about this (in e.g. <span class="citation" data-cites="farawayLMP">(<a href="#ref-farawayLMP" role="doc-biblioref">Faraway 2021</a>)</span>).</p>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Fitting Regression Lines for Many Samples of the Same Population
</div>
</div>
<div class="callout-body-container callout-body">
<p>The following code cell does the following: we consider a <strong>population</strong> in which we have two random variables related through an abstract linear model<br>
<span class="math display">\[Y = 4 - 2 X + \epsilon\]</span> that is, the <strong>theoretical coefficients</strong> are <span class="math inline">\(\beta_0 = 4, \beta_1 = -2\)</span>. Then we run a simulation where we have 5 samples (of size <span class="math inline">\(n = 30\)</span> each) of We also assume that the common variance for the errors is <span class="math inline">\(\sigma^2 = 0.25\)</span>. Let us use numpy to:</p>
<ol type="1">
<li>Get those samples.</li>
<li>Fit a regression line for each sample</li>
<li>Do a scatterplot of the samples and their corresponding regression lines.</li>
<li>Also add a dashed line representing the theoretical population line.</li>
</ol>
<p>We will add color to the plot to identify the samples.</p>
</div>
</div>
<div class="cell" data-execution_count="17">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>run <span class="op">-</span>i <span class="st">"3_1_IllustrateModelVariance.py"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 003
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Load the code for the script. Change the number of lines drawn to be ten times bigger. Run the code and observe the plot. Keep increasing the number of samples. Can you see the shape of the region swept by the different regression lines?</p></li>
<li><p>Now keep a moderately high number of lines fixed, say <span class="math inline">\(N = 50\)</span>. And experiment with the sample size (number of points) <span class="math inline">\(n\)</span>. What do you observe?</p></li>
</ul>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Confidence and Prediction Bands
</div>
</div>
<div class="callout-body-container callout-body">
<p>Run the following code. As you examine the results of the script you have noticed that the shape of the region swept by the lines looks like the yellow shaded area in the plot resulting from the following script.</p>
<p>The narrow yellow region represents the expected position of the regression line. Note that all the lines are closer in the central part of the <span class="math inline">\(X\)</span> range, and they separate as we move to the boundary of the range because of the uncertainty of the population slope. The result is that <strong>for each fixed value of <span class="math inline">\(X\)</span> we get a confidence interval for the <span class="math inline">\(Y\)</span> mean</strong>. The yellow <strong>confidence band</strong> is made of these intervals. Our distributional assumptions allow us to get closed formulas for these intervals, but we will not see them here (see <span class="citation" data-cites="farawayLMP">(<a href="#ref-farawayLMP" role="doc-biblioref">Faraway 2021</a>)</span> or Section 3.2 of <span class="citation" data-cites="ESLI2009">(<a href="#ref-ESLI2009" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2009</a>)</span>).</p>
<p>The confidence band represents the expected <strong>average value</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. But sometimes we are interested in the <strong>average range</strong> of <span class="math inline">\(Y\)</span> values given <span class="math inline">\(X\)</span>. That is what the blue shaded <strong>prediction band</strong> represents.</p>
<p>We have seen these concepts for linear regression models but you will often see these bands (especially the confidence band) used with many regression models, with a similar interpretation for it.</p>
</div>
</div>
<div class="cell" data-execution_count="18">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>run <span class="op">-</span>i <span class="st">"3_1_ConfidencePredictionBands.py"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 004
</div>
</div>
<div class="callout-body-container callout-body">
<p>Continue to experiment with the sample size and number of samples and look at the behavior of the confidence and prediction bands as you do so.</p>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hypothesis Tests about the Linear Model Coefficients
</div>
</div>
<div class="callout-body-container callout-body">
<p>Our assumptions about the distribution of the error in the linear model can be used to test several null hypothesis about the coefficients of the model. We can use the results of our previous example to see a table that includes some of them.</p>
<p>By the way, you can not get this kind of summary out of the basic scikit class <code>LinearRegression</code>. At least not easily, and that is why we are using patsy to connect to statsmodels.</p>
</div>
</div>
<div class="cell" data-execution_count="19">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.results_.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.795
Model:                            OLS   Adj. R-squared:                  0.793
Method:                 Least Squares   F-statistic:                     380.5
Date:                Thu, 20 Feb 2025   Prob (F-statistic):           1.62e-35
Time:                        11:30:00   Log-Likelihood:                -22.754
No. Observations:                 100   AIC:                             49.51
Df Residuals:                      98   BIC:                             54.72
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      3.0034      0.058     51.402      0.000       2.887       3.119
X0             2.0030      0.103     19.507      0.000       1.799       2.207
==============================================================================
Omnibus:                        0.219   Durbin-Watson:                   2.144
Prob(Omnibus):                  0.896   Jarque-Bera (JB):                0.162
Skew:                          -0.096   Prob(JB):                        0.922
Kurtosis:                       2.953   Cond. No.                         4.19
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of this Table
</div>
</div>
<div class="callout-body-container callout-body">
<p>First, note that the value of <span class="math inline">\(R^2\)</span> that we asked you to compute in a previous exercise is displayed at the top right side of the table. The next value of interest for us is the <span class="math inline">\(F\)</span>-statistic in that column and the <span class="math inline">\(p\)</span>-value right below it (named <code>Prob (F-statistic)</code>). This is a global significance test of the model, where the null hypothesis test is: this model does not explain the data better than a <em>constant (null) model</em>.</p>
<p>Next we would check the fitted values and significance tests for the individual coefficients, in the <code>coef</code> and <code>P&gt;|t|</code> columns of the central part of the summary. The null hypothesis is that the corresponding coefficient is 0. In this example all p-values are clearly so small that we can reject all the nulls.</p>
</div>
</div>
<hr>
</section>
</section>
<section id="linear-regression-in-higher-dimensions-multiple-linear-regression" class="level1">
<h1>Linear Regression in Higher Dimensions: Multiple Linear Regression</h1>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Matrix Notation for HIgher Dimensional Problems
</div>
</div>
<div class="callout-body-container callout-body">
<p>Until now we have been illustrating the discussion with <strong>simple linear regression</strong> examples. That is, examples with a single numeric input. But the extension of this ideas to higher dimensional settings using matrix notation is straightforward. We follow the notation in <span class="citation" data-cites="ESLI2009">(<a href="#ref-ESLI2009" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2009, 11</a>)</span>, except that we use <span class="math inline">\(n\)</span> instead of <span class="math inline">\(N\)</span> for the number of samples.</p>
<p>A linear model is a model of the form <span class="math display">\[Y = f(\bar X) +\epsilon \quad \text{ where }\quad f(\bar X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p\]</span> where the error functions <span class="math inline">\(\epsilon\)</span> are assumed to be (conditionally on <span class="math inline">\(\bar X\)</span>) independent identically distributed normals <span class="math inline">\(N(0, \sigma)\)</span>.</p>
<p>Our starting point, as usual, will be a dataset with <span class="math inline">\(n\)</span> observation of <span class="math inline">\(p\)</span> variables that we can think of as a <span class="math inline">\(n\times p\)</span> matrix. This is called the <strong>design matrix</strong> of our model (the name comes from the realm of <em>Experimental Design</em>). <span class="math display">\[
{\mathbf X} =
\left(
\begin{array}{ccccc}
1&amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p}\\
1&amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p}\\
\vdots &amp; \vdots &amp;        &amp; \vdots \\
\vdots &amp; \vdots &amp;        &amp; \vdots \\
1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np}\\
\end{array}
\right)
\]</span> Some remarks: + We have added an extra first column with all entries equal to 1, because this simplifies the matrix notation below. + Each of the remaining columns of the matrix, starting with the second, corresponds to a variable (as indicated on top of the column) and each row corresponds to an observation. + We assume that all the entries are numerical. In particular that means that some kind of encoding (e.g.&nbsp;one hot) has been applied to the categorical predictors.We will discuss this further when we talk about collinearity below.</p>
<p>The matrix <span class="math inline">\(\mathbf X\)</span> together with the output vector (considered as usual as a column vector) <span class="math display">\[
\bar Y =
\left(\begin{array}{c}y_0\\y_1\\\vdots\\y_n\end{array}\right)
\]</span> constitute the sample data that we use to estimate the model. Due to the random nature of the model we can not expect to obtain true values of the (vector of) <span class="math inline">\(\beta_i\)</span> coefficients: Instead we will obtain <strong>estimates</strong> of these coefficients, and we will use the symbols <span class="math inline">\(\hat\beta_i\)</span> or <span class="math inline">\(b_i\)</span> to refer to these estimates: <span class="math display">\[\hat\beta = \left(\begin{array}{c}\hat\beta_0\\\hat\beta_1\\\vdots\\\hat\beta_p\end{array}\right)\]</span> In order to obtain them let us first begin defining the <strong>predicted values</strong> (check the matrix dimensions below): <span class="math display">\[
\bar Y =\left(\begin{array}{c}y_0\\y_1\\\vdots\\y_n\end{array}\right) =
\left(
\begin{array}{ccccc}
1&amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p}\\
1&amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p}\\
\vdots &amp; \vdots &amp;        &amp; \vdots \\
\vdots &amp; \vdots &amp;        &amp; \vdots \\
1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np}\\
\end{array}
\right)
\left(\begin{array}{c}\hat\beta_0\\\hat\beta_1\\\vdots\\\hat\beta_p\end{array}\right)
= {\mathbf X}\hat\beta
\]</span></p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Design Matrices in Python. Model Formulae in Patsy.
</div>
</div>
<div class="callout-body-container callout-body">
<p>One of the additions to our environment was <code>patsy</code>. According to its <a href="https://patsy.readthedocs.io/en/latest/overview.html">webpage</a> <em>patsy is a Python package for describing statistical models (especially linear models, or models that have a linear component) and building design matrices. It is closely inspired by and compatible with the formula mini-language used in R and S.</em></p>
<p>The formula language mentioned above provides an easy way to specify model inputs.</p>
<p><strong>Examples:</strong></p>
<p>In these examples the symbol <code>~</code> can be interpreted as <em>depends on</em></p>
<ul>
<li>If we want to describe a linear model with output <code>Y</code> and numerical inputs <code>X0, X1</code> we can use the formula <code>Y ~ X0 + X1</code>.<br>
<br></li>
<li>If the model has one squared predictor we simply add one term to the formula: <code>Y ~ X0 + X1 + I(X0**2)</code>.<br>
<br></li>
<li>Next, if the linear has one numerical input <span class="math inline">\(X_0\)</span> and a categorical input <code>X1</code> with levels <code>A, B, C</code>, then we use <code>Y ~ X0 + X1</code> and patsy is clever enough to figure out the right encoding for <code>X1</code>.<br>
<br></li>
<li>But if <code>X1</code> is categorical with levels <code>1, 2, 3</code> then <code>Y ~ X0 + X1</code> would make patsy think that <code>X1</code> is numeric. In this case we need to use <code>Y ~ X0 + C(X1)</code> <br></li>
</ul>
<p>The code cells below shows how to obtain the design matrix for a dataset like the first of these examples. Note as well that a column of ones is automatically added to the matrix.</p>
</div>
</div>
<p>First let us examine the beginning of the dataset. Note the names and types of inputs and output.</p>
<div class="cell" data-execution_count="21">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"./3_1_data01.csv"</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="21">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
<th data-quarto-table-cell-role="th">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>-0.083784</td>
<td>-0.268621</td>
<td>9.325180</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>-0.982944</td>
<td>-0.118862</td>
<td>7.881259</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>-1.875067</td>
<td>0.065455</td>
<td>5.441091</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Now let us write the model formula and use it to obtain a design matrix. We show the first four rows of the design matrix.</p>
<div class="cell" data-execution_count="22">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>model_Formula <span class="op">=</span> <span class="st">"y ~ x1 + x2"</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>Y, X <span class="op">=</span> ps.dmatrices(model_Formula, df)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>X[:<span class="dv">4</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>array([[ 1.        , -0.08378436, -0.26862147],
       [ 1.        , -0.98294375, -0.11886232],
       [ 1.        , -1.87506732,  0.06545456],
       [ 1.        , -0.18614466,  1.29091527]])</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 006
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Check that <code>Y</code> above is precisely the column vector <span class="math inline">\(\bar Y\)</span> in the preceding matrix notation.</li>
<li>Load the <code>3_1_data07.csv</code> and inspect the first 10 rows. Then obtain a design matrix for a linear model of this dataset.</li>
</ul>
</div>
</div>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># %run -i "../exclude/code/3_1_Exercise_006py"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Residuals and Least Squares. Hat Matrix.
</div>
</div>
<div class="callout-body-container callout-body">
<p>The vector of <strong>residuals</strong> <span class="math inline">\(\bar\epsilon\)</span> is then <span class="math inline">\(\bar e = \bar Y - \hat Y = \bar Y - {\mathbf X}\hat\beta\)</span> (observed minus predicted values). With this matrix notation the residual sum of squares is simply: <span class="math display">\[
SS_{residual} = (\bar Y - {\mathbf X}\hat\beta)^T (\bar Y - {\mathbf X}\hat\beta)
\]</span> where <span class="math inline">\(T\)</span> indicates the transpose. The solution to the problem of optimizing this sum leads to <span class="math display">\[
\hat\beta = ({\mathbf X}^T{\mathbf X})^{-1}{\mathbf X}^T \bar Y
\]</span> provided of course that the <span class="math inline">\(p\times p\)</span> matrix <span class="math inline">\(({\mathbf X}^T{\mathbf X})\)</span> is non-singular.</p>
<p>If that is the case then <span class="math display">\[\hat Y =  {\mathbf X}\hat\beta = X (X^TX)^{-1}X^T Y\]</span> and that is why <span class="math inline">\(H = X (X^TX)^{-1}X^T\)</span> is called the <strong>hat matrix</strong> of the model (it puts a hat on <span class="math inline">\(Y\)</span>).</p>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Variance Covariance Matrix
</div>
</div>
<div class="callout-body-container callout-body">
<p>The confidence intervals and hypothesis tests about the coefficients of the model all use estimates of the corresponding standard errors which are derived from the variance covariance matrix: <span class="math display">\[\operatorname{Var}(\hat\beta) = (X^TX)^{-1}\sigma^2\]</span> and recall that <span class="math inline">\(\sigma^2\)</span> is the common error variance under our assumptions. It is usually estimated as <span class="math display">\[\hat\sigma^2 = \dfrac{SS_{residual}}{n - p - 1}\]</span></p>
<p>The variance-covariance matrix and the estimated error variance can be obtained on Python with the code in the next cells.</p>
</div>
</div>
<div class="cell" data-execution_count="29">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>model.results_.cov_params()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="29">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Intercept</th>
<th data-quarto-table-cell-role="th">X0</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Intercept</td>
<td>0.003414</td>
<td>-0.005106</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">X0</td>
<td>-0.005106</td>
<td>0.010544</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell" data-execution_count="30">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>hat_sigma2 <span class="op">=</span> np.<span class="bu">sum</span>(model.results_.resid<span class="op">**</span><span class="dv">2</span>)<span class="op">/</span>(df.shape[<span class="dv">0</span>] <span class="op">-</span> df.shape[<span class="dv">1</span>] <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>hat_sigma2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>0.009266302458613939</code></pre>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Adjusted Coefficient of Determination
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(R^2\)</span> is defined as <span class="math display">\[R^2 = 1 - \dfrac{SS_{residual}}{SS_{total}}\]</span> then the definition only depends on the residuals and therefore it extends as well to the multiple linear regression (because the Anova identity holds as well).</p>
<p>The problem when trying to interpret <span class="math inline">\(R^2\)</span> in multiple linear regression is that, because of its definition, if we add a new input variable to the model then <strong><span class="math inline">\(R^2\)</span> will always increase even if the new model is totally unrelated to the output <span class="math inline">\(Y\)</span></strong>.</p>
<p>It is better therefore to use the <strong>Adjusted Coefficient of Determination</strong> <span class="math display">\[\bar R^2 = 1 - \left((1 - R^2)\dfrac{n - 1}{n - p - 1}\right)\]</span> that takes this phenomenon into account and has an interpretation that closely resembles that of <span class="math inline">\(R^2\)</span> in simple linear regression.</p>
</div>
</div>
<hr>
</section>
<section id="train-and-diagnose-multiple-regression-linear-models-in-python" class="level1">
<h1>Train and Diagnose Multiple Regression Linear Models in Python</h1>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A Collection of Example Datasets for Multiple Regression Linear Models
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the rest of this session we will be using a collection of datasets, <code>3_1_data01.csv</code> to illustrate several aspects of multiple linear regression models. We start wit the first one, a very simple example, and then we will try to build up our understanding of these models as we move forward.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning about EDA in these Examples
</div>
</div>
<div class="callout-body-container callout-body">
<p>Keep in mind that these are teaching examples and they do not contain missing data, we do not care much about preprocessing, etc. We will perform some basic EDA, but only in relation to the aspect of linear modeling that we are discussing. <strong>In real problems you always need to do a full EDA and preprocessing</strong>.</p>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
First Basic Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>This example serves as warm up for the ones below. All the steps we take here will always be included in our work and, depending on the dataset, additional ones will be required.</p>
</div>
</div>
<p>First we load the dataset, visualize it, and create standard names for the variables:</p>
<div class="cell" data-execution_count="31">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"./3_1_data01.csv"</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="31">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
<th data-quarto-table-cell-role="th">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>-0.083784</td>
<td>-0.268621</td>
<td>9.325180</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>-0.982944</td>
<td>-0.118862</td>
<td>7.881259</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>-1.875067</td>
<td>0.065455</td>
<td>5.441091</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>-0.186145</td>
<td>1.290915</td>
<td>13.861506</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell" data-execution_count="32">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> <span class="st">"y"</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>num_inputs <span class="op">=</span> [<span class="st">"x1"</span>, <span class="st">"x2"</span>]</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>cat_inputs <span class="op">=</span> []</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> num_inputs <span class="op">+</span> cat_inputs</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[inputs]</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df[output]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Next we create the train/test split and associated datasets.</p>
<div class="cell" data-execution_count="33">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>XTR, XTS, YTR, YTS <span class="op">=</span> train_test_split(</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    X, Y,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    test_size <span class="op">=</span> <span class="fl">0.2</span>,  </span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    random_state <span class="op">=</span> <span class="dv">1</span>) </span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>dfTR <span class="op">=</span> pd.DataFrame(XTR, columns<span class="op">=</span>inputs)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>dfTR[output] <span class="op">=</span> YTR</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>dfTS <span class="op">=</span> pd.DataFrame(XTS, columns<span class="op">=</span>inputs)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>dfTS[output] <span class="op">=</span> YTS</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 007
</div>
</div>
<div class="callout-body-container callout-body">
<p>Do you see any difference with the classification setting?</p>
</div>
</div>
<section id="eda" class="level4">
<h4 class="anchored" data-anchor-id="eda">EDA</h4>
<p>Let us do a basic EDA exploring with pairplots:</p>
<div class="cell" data-execution_count="34">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>sns.pairplot(dfTR, corner<span class="op">=</span><span class="va">True</span>, height<span class="op">=</span><span class="fl">1.5</span>, aspect<span class="op">=</span><span class="fl">1.5</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The plot of the output vs each one of the inputs shows linear trends. The correlation between inputs seems very low. We confirm that using the correlation matrix.</p>
<div class="cell" data-execution_count="35">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>XTR.corr()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="35">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">x1</td>
<td>1.00000</td>
<td>0.00622</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">x2</td>
<td>0.00622</td>
<td>1.00000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="model-formula-and-fit" class="level4">
<h4 class="anchored" data-anchor-id="model-formula-and-fit">Model Formula and Fit</h4>
<p>Let us use for this model the formula language that we have described above inside the scikit pipeline framework. Two remarks: + In order to use it in a pipeline we only need the right hand side of the formula, the part after ´~´ + When you just want your formula to say <em>“using all inputs”</em> you can use the following construction with <a href="https://docs.python.org/3/library/stdtypes.html#str.join"><code>join</code></a>.</p>
<div class="cell" data-execution_count="36">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>model_Formula <span class="op">=</span> <span class="st">" + "</span>.join(inputs)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>model_Formula</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>'x1 + x2'</code></pre>
</div>
</div>
<p>And now we can use <code>FormulaTransformer</code> with this formula in the first step of the pipeline (it is defined in a taylor-made module <code>patsy_utils</code> inspired by <a href="https://juanitorduz.%20github.io/formula_transformer/">this post</a> by Juan Orduz). The <code>FormulaTransformer</code> will create (through its fit_transform method) the design matrix of the model for us as illustrated below. Then in the second step we use <code>LinearRegression</code> (provided by <code>sklearn</code>) to access all the statistical information provided by <code>statsmodels</code>.</p>
<div class="cell" data-execution_count="37">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>FormulaTransformer(model_Formula).fit_transform(dfTR)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="37">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Intercept</th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">382</td>
<td>1.0</td>
<td>0.144882</td>
<td>-1.491031</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">994</td>
<td>1.0</td>
<td>1.443869</td>
<td>-1.596614</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">982</td>
<td>1.0</td>
<td>1.345697</td>
<td>-0.053474</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">47</td>
<td>1.0</td>
<td>1.075970</td>
<td>0.288895</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">521</td>
<td>1.0</td>
<td>-1.313146</td>
<td>-0.029207</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">767</td>
<td>1.0</td>
<td>-0.576899</td>
<td>0.636284</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">72</td>
<td>1.0</td>
<td>-0.084902</td>
<td>-0.462210</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">908</td>
<td>1.0</td>
<td>-0.336037</td>
<td>1.811883</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">235</td>
<td>1.0</td>
<td>-1.191780</td>
<td>0.924044</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">37</td>
<td>1.0</td>
<td>1.129219</td>
<td>-1.852739</td>
</tr>
</tbody>
</table>

<p>800 rows × 3 columns</p>
</div>
</div>
</div>
<div class="cell" data-execution_count="41">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline <span class="op">=</span> Pipeline([</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>(<span class="st">"formula"</span>, FormulaTransformer(model_Formula)),</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>(<span class="st">"regressor"</span>, StatsModelsRegressor(OLS, fit_intercept <span class="op">=</span> <span class="va">False</span>))])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Once the pipeline is created, fit proceeds as usual. We create a <code>model</code> object, which is in fact a <code>statsmodels</code> regressor:</p>
<div class="cell" data-execution_count="42">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline.fit(dfTR[inputs], dfTR[output])</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> lm_pipeline._final_estimator</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="42">
<style>#sk-container-id-3 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-3 {
  color: var(--sklearn-color-text);
}

#sk-container-id-3 pre {
  padding: 0;
}

#sk-container-id-3 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-3 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-3 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-3 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-3 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-3 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-3 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-3 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-3 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-3 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-3 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-3 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-3 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-3 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-3 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-3 div.sk-label label.sk-toggleable__label,
#sk-container-id-3 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-3 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-3 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-3 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-3 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-3 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-3 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-3 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-3 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>StatsModelsRegressor(fit_intercept=False,
                     model_class=&lt;class 'statsmodels.regression.linear_model.OLS'&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" checked=""><label for="sk-estimator-id-3" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;StatsModelsRegressor<span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>StatsModelsRegressor(fit_intercept=False,
                     model_class=&lt;class 'statsmodels.regression.linear_model.OLS'&gt;)</pre></div> </div></div></div></div>
</div>
</div>
</section>
<section id="extracting-information-about-the-model" class="level4">
<h4 class="anchored" data-anchor-id="extracting-information-about-the-model">Extracting Information about the Model</h4>
<p>The initial information we need is stored in the <code>results_</code> property of the model. For example, this is the model’s summary, like the one we discussed previously in this session:</p>
<p>(In this case I recommend using print for a nicer display)</p>
<div class="cell" data-execution_count="45">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.results_.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.838
Model:                            OLS   Adj. R-squared:                  0.838
Method:                 Least Squares   F-statistic:                     2066.
Date:                Thu, 20 Feb 2025   Prob (F-statistic):          4.83e-316
Time:                        11:45:33   Log-Likelihood:                -1474.4
No. Observations:                 800   AIC:                             2955.
Df Residuals:                     797   BIC:                             2969.
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      9.9043      0.054    182.954      0.000       9.798      10.011
x1             1.9984      0.054     37.300      0.000       1.893       2.104
x2             2.9342      0.056     52.114      0.000       2.824       3.045
==============================================================================
Omnibus:                        4.181   Durbin-Watson:                   2.082
Prob(Omnibus):                  0.124   Jarque-Bera (JB):                4.153
Skew:                           0.130   Prob(JB):                        0.125
Kurtosis:                       3.239   Cond. No.                         1.06
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
</div>
<p><strong>Always check if the coefficients of the input variables are significant.</strong> In this case they are. And the adjusted <span class="math inline">\(R^2\)</span> shows that this model accounts for 84% of the variability of <span class="math inline">\(Y\)</span> in the training set.</p>
</section>
<section id="using-residual-plots-to-analyze-the-model" class="level4">
<h4 class="anchored" data-anchor-id="using-residual-plots-to-analyze-the-model">Using Residual Plots to Analyze the Model</h4>
<p>The following script contains the definition of a convenience function that when provided with a model and its variables plots:</p>
<ul>
<li>The density curve of the residuals. You should check if it looks approximately normal. Besides it you have a QQ-plot of the residuals to help you assess their normality in a different way. They should follow the red line very closely.</li>
<li>A plot of fitted values (the <span class="math inline">\(\hat Y\)</span>) against the corresponding residuals. IN this and the following plots, look for unexpected patterns.</li>
<li>The rest of the plots show the residuals against each of the numerical (with scatterplots) or categorical inputs (in this case with parallel boxplots). For the numerical input scatterplots, the distribution of the points should appear random. And in the case of the boxplots, they should look very similar across all levels of the categorical input.</li>
</ul>
<div class="cell" data-execution_count="46">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>run <span class="op">-</span>i <span class="st">"./3_1_ResidualPlots.py"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We get those plots in the next cell and comment on them below.</p>
<div class="cell" data-execution_count="47">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>ResidualPlots(model<span class="op">=</span>model, data<span class="op">=</span>dfTR, num_inputs<span class="op">=</span>num_inputs, cat_inputs<span class="op">=</span>cat_inputs, output<span class="op">=</span>output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>--------------------------------------------------
Density Curve and QQ-plot of Residuals: ['x1', 'x2']
--------------------------------------------------
[&lt;Axes: &gt; &lt;Axes: &gt;]
--------------------------------------------------
Fitted Values vs Residuals:
--------------------------------------------------
--------------------------------------------------
Numerical inputs: ['x1', 'x2']
--------------------------------------------------
--------------------------------------------------
No categorical inputs exist.
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-29-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-29-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-29-output-4.png" class="img-fluid"></p>
</div>
</div>
<p>In this our first example everything works as expected. Think of the plots that we obtain here as a model fitting goal: your job is to get the model plots look like these.</p>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Second Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let us try to load a second dataset and repeat the previous steps.</p>
</div>
</div>
<div class="cell" data-execution_count="48">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"./3_1_data02.csv"</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="48">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
<th data-quarto-table-cell-role="th">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>-0.667721</td>
<td>7.616991</td>
<td>1.969010</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>-3.296181</td>
<td>3.811367</td>
<td>7.340756</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>-6.743649</td>
<td>5.733155</td>
<td>-2.591455</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>-2.077600</td>
<td>-8.420373</td>
<td>-52.285824</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell" data-execution_count="49">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> <span class="st">"y"</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>num_inputs <span class="op">=</span> [<span class="st">"x1"</span>, <span class="st">"x2"</span>]</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>cat_inputs <span class="op">=</span> []</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> num_inputs <span class="op">+</span> cat_inputs</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[inputs]</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df[output]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Next we create the train/test split.</p>
<div class="cell" data-execution_count="50">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>XTR, XTS, YTR, YTS <span class="op">=</span> train_test_split(</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    X, Y,</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    test_size <span class="op">=</span> <span class="fl">0.2</span>,  </span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>    random_state <span class="op">=</span> <span class="dv">1</span>) </span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>dfTR <span class="op">=</span> pd.DataFrame(XTR, columns<span class="op">=</span>inputs)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>dfTR[output] <span class="op">=</span> YTR</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>dfTS <span class="op">=</span> pd.DataFrame(XTS, columns<span class="op">=</span>inputs)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>dfTS[output] <span class="op">=</span> YTS</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="eda-1" class="level4">
<h4 class="anchored" data-anchor-id="eda-1">EDA</h4>
<div class="cell" data-execution_count="51">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>sns.pairplot(dfTR, corner<span class="op">=</span><span class="va">True</span>, height<span class="op">=</span><span class="fl">1.5</span>, aspect<span class="op">=</span><span class="fl">1.5</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The obvious non linear pattern in the <code>Y</code> vs <code>X2</code> plot is our first warning that things are different now. The correlation between inputs however looks fine.</p>
<div class="cell" data-execution_count="52">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>XTR.corr()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="52">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">x1</td>
<td>1.000000</td>
<td>-0.017873</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">x2</td>
<td>-0.017873</td>
<td>1.000000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="model-formula-and-fit-1" class="level4">
<h4 class="anchored" data-anchor-id="model-formula-and-fit-1">Model Formula and Fit</h4>
<p>Let us first use the same formula as in the previous model.</p>
<div class="cell" data-execution_count="53">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>model_Formula <span class="op">=</span> <span class="st">" + "</span>.join(inputs)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>model_Formula</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>'x1 + x2'</code></pre>
</div>
</div>
<p>We create the pipeline and fit the model.</p>
<div class="cell" data-execution_count="55">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline <span class="op">=</span> Pipeline([</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>(<span class="st">"formula"</span>, FormulaTransformer(model_Formula)),</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>(<span class="st">"regressor"</span>, StatsModelsRegressor(OLS, fit_intercept <span class="op">=</span> <span class="va">False</span>))])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="56">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline.fit(dfTR[inputs], dfTR[output])</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> lm_pipeline._final_estimator</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="extracting-information-about-the-model-1" class="level4">
<h4 class="anchored" data-anchor-id="extracting-information-about-the-model-1">Extracting Information about the Model</h4>
<p>The model’s summary looks ok, all coefficients are significative, but the adjusted <span class="math inline">\(R^2\)</span> is lower than expected.</p>
<div class="cell" data-execution_count="57">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.results_.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.664
Model:                            OLS   Adj. R-squared:                  0.663
Method:                 Least Squares   F-statistic:                     788.4
Date:                Thu, 20 Feb 2025   Prob (F-statistic):          1.29e-189
Time:                        11:48:02   Log-Likelihood:                -3306.9
No. Observations:                 800   AIC:                             6620.
Df Residuals:                     797   BIC:                             6634.
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -6.9606      0.538    -12.946      0.000      -8.016      -5.905
x1             1.8640      0.095     19.595      0.000       1.677       2.051
x2             3.2072      0.092     34.883      0.000       3.027       3.388
==============================================================================
Omnibus:                      115.842   Durbin-Watson:                   1.941
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               70.131
Skew:                          -0.594   Prob(JB):                     5.91e-16
Kurtosis:                       2.168   Cond. No.                         5.88
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
</div>
</section>
<section id="using-residual-plots-to-analyze-the-model-1" class="level4">
<h4 class="anchored" data-anchor-id="using-residual-plots-to-analyze-the-model-1">Using Residual Plots to Analyze the Model</h4>
<p>The residual plots in this example are the key to discovering that something is wrong with this model.</p>
<div class="cell" data-execution_count="58">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>ResidualPlots(data<span class="op">=</span> dfTR, model<span class="op">=</span>model, num_inputs<span class="op">=</span>num_inputs, cat_inputs<span class="op">=</span>cat_inputs, output<span class="op">=</span>output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>--------------------------------------------------
Density Curve and QQ-plot of Residuals: ['x1', 'x2']
--------------------------------------------------
[&lt;Axes: &gt; &lt;Axes: &gt;]
--------------------------------------------------
Fitted Values vs Residuals:
--------------------------------------------------
--------------------------------------------------
Numerical inputs: ['x1', 'x2']
--------------------------------------------------
--------------------------------------------------
No categorical inputs exist.
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-39-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-39-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-39-output-4.png" class="img-fluid"></p>
</div>
</div>
<p>The non normality of the residuals is the first thing that we see. But then the parabolic patterns in some of the residual plots, in particular in the <code>x2</code> plot, tell us that <strong>there is still signal</strong> in the residuals waiting to be captured by our models. The remedy in this case seems clear: try adding polynomial terms in <code>x2</code>.</p>
</section>
<section id="fitting-a-second-model-with-quadratic-terms" class="level4">
<h4 class="anchored" data-anchor-id="fitting-a-second-model-with-quadratic-terms">Fitting a Second Model with Quadratic Terms</h4>
<p>We will not repeat the EDA and proceed directly to the model formula. It only needs updating with the addition of a new term</p>
<div class="cell" data-execution_count="59">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>model_Formula <span class="op">=</span> model_Formula <span class="op">+</span> <span class="st">"+ I(x2**2)"</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>model_Formula</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>'x1 + x2+ I(x2**2)'</code></pre>
</div>
</div>
<p>We fit this new model:</p>
<div class="cell" data-execution_count="60">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline <span class="op">=</span> Pipeline([</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>(<span class="st">"formula"</span>, FormulaTransformer(model_Formula)),</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>(<span class="st">"regressor"</span>, StatsModelsRegressor(OLS, fit_intercept <span class="op">=</span> <span class="va">False</span>))])</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>lm_pipeline.fit(dfTR[inputs], dfTR[output])</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> lm_pipeline._final_estimator  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The first piece of good news is that the model’s summary shows that all coefficients are significative, <strong>including the quadratic term</strong>. And also check the boost in the value of the adjusted <span class="math inline">\(R^2\)</span>. This model seems a better fit for the data.</p>
<div class="cell" data-execution_count="61">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.results_.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.995
Model:                            OLS   Adj. R-squared:                  0.995
Method:                 Least Squares   F-statistic:                 4.894e+04
Date:                Thu, 20 Feb 2025   Prob (F-statistic):               0.00
Time:                        11:48:09   Log-Likelihood:                -1654.3
No. Observations:                 800   AIC:                             3317.
Df Residuals:                     796   BIC:                             3335.
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     10.2091      0.103     98.726      0.000      10.006      10.412
x1             1.9968      0.012    165.332      0.000       1.973       2.020
x2             2.9832      0.012    254.915      0.000       2.960       3.006
I(x2 ** 2)    -0.5056      0.002   -220.837      0.000      -0.510      -0.501
==============================================================================
Omnibus:                        0.931   Durbin-Watson:                   2.053
Prob(Omnibus):                  0.628   Jarque-Bera (JB):                0.810
Skew:                           0.070   Prob(JB):                        0.667
Kurtosis:                       3.070   Cond. No.                         69.0
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
</div>
<p>The residual plots of this second model now look as expected. The residuals seem normal, and there are no obvious patterns (no signal) remaining in the scatterplots. We consider this a good model from the statistical point of view.</p>
<div class="cell" data-execution_count="62">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>ResidualPlots(data<span class="op">=</span>dfTR, model<span class="op">=</span>model, num_inputs<span class="op">=</span>num_inputs, cat_inputs<span class="op">=</span>cat_inputs, output<span class="op">=</span>output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>--------------------------------------------------
Density Curve and QQ-plot of Residuals: ['x1', 'x2']
--------------------------------------------------
[&lt;Axes: &gt; &lt;Axes: &gt;]
--------------------------------------------------
Fitted Values vs Residuals:
--------------------------------------------------
--------------------------------------------------
Numerical inputs: ['x1', 'x2']
--------------------------------------------------
--------------------------------------------------
No categorical inputs exist.
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-43-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-43-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-43-output-4.png" class="img-fluid"></p>
</div>
</div>
<hr>
</section>
</section>
<section id="failures-in-the-the-linear-model-requirements" class="level1">
<h1>Failures in the the Linear Model Requirements</h1>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Third Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>Our third dataset illustrates a possible problem that you may find yourself in when trying to ft a linear model.</p>
</div>
</div>
<div class="cell" data-execution_count="63">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"./3_1_data03.csv"</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="63">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
<th data-quarto-table-cell-role="th">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>2.333070</td>
<td>3.808496</td>
<td>23.584776</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1.675955</td>
<td>1.905683</td>
<td>18.272128</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.814088</td>
<td>2.866577</td>
<td>20.441051</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1.980600</td>
<td>-4.210186</td>
<td>11.557789</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Let us move quickly through the modeling steps until we reach the step where a new situation appears:</p>
<div class="cell" data-execution_count="64">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> <span class="st">"y"</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>num_inputs <span class="op">=</span> [<span class="st">"x1"</span>, <span class="st">"x2"</span>]</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>cat_inputs <span class="op">=</span> []</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> num_inputs <span class="op">+</span> cat_inputs</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[inputs]</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df[output]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="65">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>XTR, XTS, YTR, YTS <span class="op">=</span> train_test_split(</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    X, Y,</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>    test_size <span class="op">=</span> <span class="fl">0.2</span>,  </span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    random_state <span class="op">=</span> <span class="dv">1</span>) </span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>dfTR <span class="op">=</span> pd.DataFrame(XTR, columns<span class="op">=</span>inputs)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>dfTR[output] <span class="op">=</span> YTR</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>dfTS <span class="op">=</span> pd.DataFrame(XTS, columns<span class="op">=</span>inputs)</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>dfTS[output] <span class="op">=</span> YTS</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="eda-2" class="level4">
<h4 class="anchored" data-anchor-id="eda-2">EDA</h4>
<div class="cell" data-execution_count="66">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>sns.pairplot(dfTR, corner<span class="op">=</span><span class="va">True</span>, height<span class="op">=</span><span class="fl">1.5</span>, aspect<span class="op">=</span><span class="fl">1.5</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-47-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Do you notice anything unexpected here?<br>
The correlation between inputs is ok.</p>
<div class="cell" data-execution_count="67">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>XTR.corr()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="67">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">x1</td>
<td>1.000000</td>
<td>-0.017873</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">x2</td>
<td>-0.017873</td>
<td>1.000000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="model-formula-and-fit-2" class="level4">
<h4 class="anchored" data-anchor-id="model-formula-and-fit-2">Model Formula and Fit</h4>
<p>We use the basic <em>all in</em> formula as starting point.</p>
<div class="cell" data-execution_count="68">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>model_Formula <span class="op">=</span> <span class="st">" + "</span>.join(inputs)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>model_Formula</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="68">
<pre><code>'x1 + x2'</code></pre>
</div>
</div>
<p>We create the pipeline and fit the model.</p>
<div class="cell" data-execution_count="69">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline <span class="op">=</span> Pipeline([</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>(<span class="st">"formula"</span>, FormulaTransformer(model_Formula)),</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>(<span class="st">"regressor"</span>, StatsModelsRegressor(OLS, fit_intercept <span class="op">=</span> <span class="va">False</span>))])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="70">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline.fit(dfTR[inputs], dfTR[output])</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> lm_pipeline._final_estimator</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="extracting-information-about-the-model-2" class="level4">
<h4 class="anchored" data-anchor-id="extracting-information-about-the-model-2">Extracting Information about the Model</h4>
<p>The model’s summary again looks ok, but notice that the adjusted <span class="math inline">\(R^2\)</span> is quite low.</p>
<div class="cell" data-execution_count="71">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.results_.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.369
Model:                            OLS   Adj. R-squared:                  0.368
Method:                 Least Squares   F-statistic:                     233.2
Date:                Thu, 20 Feb 2025   Prob (F-statistic):           1.83e-80
Time:                        11:48:19   Log-Likelihood:                -3050.8
No. Observations:                 800   AIC:                             6108.
Df Residuals:                     797   BIC:                             6122.
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     10.0687      0.772     13.049      0.000       8.554      11.583
x1             1.9683      0.276      7.124      0.000       1.426       2.511
x2             2.7387      0.134     20.512      0.000       2.477       3.001
==============================================================================
Omnibus:                       37.784   Durbin-Watson:                   2.008
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              117.220
Skew:                           0.033   Prob(JB):                     3.51e-26
Kurtosis:                       4.874   Cond. No.                         6.33
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
</div>
</section>
<section id="using-residual-plots-to-analyze-the-model-2" class="level4">
<h4 class="anchored" data-anchor-id="using-residual-plots-to-analyze-the-model-2">Using Residual Plots to Analyze the Model</h4>
<p>The residual plots in this example are the key to discovering that something is wrong with this model.</p>
<div class="cell" data-execution_count="72">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>ResidualPlots(data<span class="op">=</span>dfTR, model<span class="op">=</span>model, num_inputs<span class="op">=</span>num_inputs, cat_inputs<span class="op">=</span>cat_inputs, output<span class="op">=</span>output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>--------------------------------------------------
Density Curve and QQ-plot of Residuals: ['x1', 'x2']
--------------------------------------------------
[&lt;Axes: &gt; &lt;Axes: &gt;]
--------------------------------------------------
Fitted Values vs Residuals:
--------------------------------------------------
--------------------------------------------------
Numerical inputs: ['x1', 'x2']
--------------------------------------------------
--------------------------------------------------
No categorical inputs exist.
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-53-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-53-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-53-output-4.png" class="img-fluid"></p>
</div>
</div>
<p>Again there are clear signs of non normality of the residuals. And the most striking feature appears in the residuals vs <code>x1</code> plot. There is a clear wedge shape that says that the variance of the residuals depends on the values of <code>x1</code>. This goes directly against the assumption of equal conditional variance of the errors.</p>
<p><strong>Be careful:</strong> this model can still be a good predictor of <code>Y</code>. But we can not use it for inference and in particular we can not trust the p-values in the model’s summary or compute confidence intervals, etc. Always keep in mind that the Machine Learning goals and the statistical goals are often not exactly the same.</p>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Fourth Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>Our third dataset illustrates a possible problem that you may find yourself in when trying to ft a linear model.</p>
</div>
</div>
<div class="cell" data-execution_count="74">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"./3_1_data04.csv"</span>)</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="74">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
<th data-quarto-table-cell-role="th">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>2.333070</td>
<td>3.808496</td>
<td>32.086970</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1.675955</td>
<td>1.905683</td>
<td>20.262496</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.814088</td>
<td>2.866577</td>
<td>41.888154</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1.980600</td>
<td>-4.210186</td>
<td>6.126667</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Again let us go quickly to the interesting part:</p>
<div class="cell" data-execution_count="75">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> <span class="st">"y"</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>num_inputs <span class="op">=</span> [<span class="st">"x1"</span>, <span class="st">"x2"</span>]</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>cat_inputs <span class="op">=</span> []</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> num_inputs <span class="op">+</span> cat_inputs</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[inputs]</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df[output]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="76">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>XTR, XTS, YTR, YTS <span class="op">=</span> train_test_split(</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>    X, Y,</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>    test_size <span class="op">=</span> <span class="fl">0.2</span>,  </span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>    random_state <span class="op">=</span> <span class="dv">1</span>) </span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>dfTR <span class="op">=</span> pd.DataFrame(XTR, columns<span class="op">=</span>inputs)</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>dfTR[output] <span class="op">=</span> YTR</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>dfTS <span class="op">=</span> pd.DataFrame(XTS, columns<span class="op">=</span>inputs)</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>dfTS[output] <span class="op">=</span> YTS</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="eda-3" class="level4">
<h4 class="anchored" data-anchor-id="eda-3">EDA</h4>
<div class="cell" data-execution_count="77">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>sns.pairplot(dfTR, corner<span class="op">=</span><span class="va">True</span>, height<span class="op">=</span><span class="fl">1.5</span>, aspect<span class="op">=</span><span class="fl">1.5</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-57-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Probably nothing pops out here. The correlation between inputs is also ok.</p>
<div class="cell" data-execution_count="78">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>XTR.corr()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="78">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">x1</td>
<td>1.000000</td>
<td>-0.017873</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">x2</td>
<td>-0.017873</td>
<td>1.000000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="model-formula-and-fit-3" class="level4">
<h4 class="anchored" data-anchor-id="model-formula-and-fit-3">Model Formula and Fit</h4>
<p>We use the basic <em>all in</em> formula as starting point.</p>
<div class="cell" data-execution_count="79">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>model_Formula <span class="op">=</span> <span class="st">" + "</span>.join(inputs)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>model_Formula</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="79">
<pre><code>'x1 + x2'</code></pre>
</div>
</div>
<p>We create the pipeline and fit the model.</p>
<div class="cell" data-execution_count="80">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline <span class="op">=</span> Pipeline([</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>(<span class="st">"formula"</span>, FormulaTransformer(model_Formula)),</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>(<span class="st">"regressor"</span>, StatsModelsRegressor(OLS, fit_intercept <span class="op">=</span> <span class="va">False</span>))])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="81">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline.fit(dfTR[inputs], dfTR[output])</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> lm_pipeline._final_estimator</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="extracting-information-about-the-model-3" class="level4">
<h4 class="anchored" data-anchor-id="extracting-information-about-the-model-3">Extracting Information about the Model</h4>
<p>The p-values are ok, but the low adjusted <span class="math inline">\(R^2\)</span> is intriguing.</p>
<div class="cell" data-execution_count="82">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.results_.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.438
Model:                            OLS   Adj. R-squared:                  0.437
Method:                 Least Squares   F-statistic:                     310.5
Date:                Thu, 20 Feb 2025   Prob (F-statistic):          1.96e-100
Time:                        11:48:37   Log-Likelihood:                -2977.1
No. Observations:                 800   AIC:                             5960.
Df Residuals:                     797   BIC:                             5974.
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     21.9059      0.704     31.130      0.000      20.525      23.287
x1             2.0598      0.252      8.175      0.000       1.565       2.554
x2             2.8836      0.122     23.682      0.000       2.645       3.123
==============================================================================
Omnibus:                      281.395   Durbin-Watson:                   1.982
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              918.216
Skew:                           1.711   Prob(JB):                    4.09e-200
Kurtosis:                       6.979   Cond. No.                         6.33
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
</div>
</section>
<section id="using-residual-plots-to-analyze-the-model-3" class="level4">
<h4 class="anchored" data-anchor-id="using-residual-plots-to-analyze-the-model-3">Using Residual Plots to Analyze the Model</h4>
<p>The residual plots in this example are the key to discovering that something is wrong with this model.</p>
<div class="cell" data-execution_count="83">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>ResidualPlots(data<span class="op">=</span>dfTR, model<span class="op">=</span>model, num_inputs<span class="op">=</span>num_inputs, cat_inputs<span class="op">=</span>cat_inputs, output<span class="op">=</span>output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>--------------------------------------------------
Density Curve and QQ-plot of Residuals: ['x1', 'x2']
--------------------------------------------------
[&lt;Axes: &gt; &lt;Axes: &gt;]
--------------------------------------------------
Fitted Values vs Residuals:
--------------------------------------------------
--------------------------------------------------
Numerical inputs: ['x1', 'x2']
--------------------------------------------------
--------------------------------------------------
No categorical inputs exist.
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-63-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-63-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-63-output-4.png" class="img-fluid"></p>
</div>
</div>
<p>In this case the non normality of the residuals is the culprit of the problems we see in the plots. The distribution of the residuals seems skewed (non symmetrical) in most of them.</p>
<p>The caveats we made before about models like this being statistically flawed but possible good predictors of <code>Y</code> still hold here.</p>
<hr>
</section>
</section>
<section id="the-problem-of-collinearity" class="level1">
<h1>The Problem of Collinearity</h1>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Collinearity and When is it a Problem? Variance Inflation Factors.
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Collinearity or Multicollinearity</strong> refer to the situation when the input variables of a model are or are very close to being linearly dependent. That means that the rank of the design matrix <span class="math inline">\(\mathbf X\)</span> is not maximal. In some cases this causes numerical problems in the fit of the model. However, many modern implementations can deal with this and still fit models and return accurate predictions for the training set even in this rank deficient setting. We will see an example below. And if the test set has (as expected) a similar distribution to the training set, then even perfect collinearity is often not a problem for prediction. We will see this in the example below.</p>
<p>Why then should we worry about collinearity? Because one of the advantages of linear models is their interpretability, and collinearity makes all the coefficient estimates and inferential results unreliable. Different samples or different software libraries can result in arbitrarily different coefficients for the model.</p>
<p>When two variables are highly correlated we can expect to detect this in EDA using the correlation matrix. But if the the correlation structure is more complicated (multicollinearity) then this may not be enough. In order to detect multicollinearity in such cases we usually employ the <strong>variable inflation factors (VIF)</strong>. This is essentially measuring the result of regressing each input variable over the rest of the original inputs. The VIFs (one per input variable) are a numerical measure of the fit of such models. As a rule of thumb, a VIF value greater than 5 is usually considered an indicator of the presence of multicollinearity in the data set.</p>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Fifth Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>The following example illustrates the above discussion in a simple scenario in which there is a strong multicollinearity between the inputs. First we explore the data and fit the model quickly as we have done before.</p>
</div>
</div>
<div class="cell" data-execution_count="84">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"./3_1_data05.csv"</span>)</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="84">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
<th data-quarto-table-cell-role="th">x3</th>
<th data-quarto-table-cell-role="th">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>2.333070</td>
<td>3.808496</td>
<td>8.206014</td>
<td>22.750828</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1.675955</td>
<td>1.905683</td>
<td>5.138731</td>
<td>17.417961</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.814088</td>
<td>2.866577</td>
<td>4.560207</td>
<td>16.625784</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1.980600</td>
<td>-4.210186</td>
<td>1.041929</td>
<td>4.490612</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell" data-execution_count="85">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> <span class="st">"y"</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>num_inputs <span class="op">=</span> [<span class="st">"x1"</span>, <span class="st">"x2"</span>, <span class="st">"x3"</span>]</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>cat_inputs <span class="op">=</span> []</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> num_inputs <span class="op">+</span> cat_inputs</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[inputs]</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df[output]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="86">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>XTR, XTS, YTR, YTS <span class="op">=</span> train_test_split(</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>    X, Y,</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>    test_size <span class="op">=</span> <span class="fl">0.2</span>,  </span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>    random_state <span class="op">=</span> <span class="dv">1</span>) </span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>dfTR <span class="op">=</span> pd.DataFrame(XTR, columns<span class="op">=</span>inputs)</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>dfTR[output] <span class="op">=</span> YTR</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>dfTS <span class="op">=</span> pd.DataFrame(XTS, columns<span class="op">=</span>inputs)</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>dfTS[output] <span class="op">=</span> YTS</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="eda-4" class="level4">
<h4 class="anchored" data-anchor-id="eda-4">EDA</h4>
<div class="cell" data-execution_count="87">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>sns.pairplot(dfTR, corner<span class="op">=</span><span class="va">True</span>, height<span class="op">=</span><span class="fl">1.5</span>, aspect<span class="op">=</span><span class="fl">1.5</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-67-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In this example <strong>EDA already indicates the problem, but that will not always be the case.</strong></p>
<div class="cell" data-execution_count="88">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>XTR.corr()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="88">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
<th data-quarto-table-cell-role="th">x3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">x1</td>
<td>1.000000</td>
<td>-0.017873</td>
<td>0.672476</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">x2</td>
<td>-0.017873</td>
<td>1.000000</td>
<td>0.689660</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">x3</td>
<td>0.672476</td>
<td>0.689660</td>
<td>1.000000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="model-formula-and-fit-4" class="level4">
<h4 class="anchored" data-anchor-id="model-formula-and-fit-4">Model Formula and Fit</h4>
<p>As usual</p>
<div class="cell" data-execution_count="89">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>model_Formula <span class="op">=</span> <span class="st">" + "</span>.join(inputs)</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>model_Formula</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="89">
<pre><code>'x1 + x2 + x3'</code></pre>
</div>
</div>
<p>Pipeline and fit</p>
<div class="cell" data-execution_count="90">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline <span class="op">=</span> Pipeline([</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>(<span class="st">"formula"</span>, FormulaTransformer(model_Formula)),</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>(<span class="st">"regressor"</span>, StatsModelsRegressor(OLS, fit_intercept <span class="op">=</span> <span class="va">False</span>))])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="91">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline.fit(XTR, YTR)</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> lm_pipeline._final_estimator</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="extracting-information-about-the-model-4" class="level4">
<h4 class="anchored" data-anchor-id="extracting-information-about-the-model-4">Extracting Information about the Model</h4>
<p>Note that the p-values and <span class="math inline">\(R^2\)</span> all look ok. But looking back at the EDA pairplot, how come that the <code>x3</code> coefficient is negative?</p>
<div class="cell" data-execution_count="92">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.results_.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.976
Model:                            OLS   Adj. R-squared:                  0.976
Method:                 Least Squares   F-statistic:                 1.076e+04
Date:                Thu, 20 Feb 2025   Prob (F-statistic):               0.00
Time:                        11:48:49   Log-Likelihood:                -1147.9
No. Observations:                 800   AIC:                             2304.
Df Residuals:                     796   BIC:                             2323.
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     10.0200      0.072    140.006      0.000       9.880      10.161
x1             4.0574      0.079     51.410      0.000       3.902       4.212
x2             3.0287      0.039     77.696      0.000       2.952       3.105
x3            -1.0469      0.038    -27.918      0.000      -1.121      -0.973
==============================================================================
Omnibus:                        4.235   Durbin-Watson:                   2.077
Prob(Omnibus):                  0.120   Jarque-Bera (JB):                4.202
Skew:                           0.132   Prob(JB):                        0.122
Kurtosis:                       3.237   Cond. No.                         18.0
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
</div>
</section>
<section id="using-residual-plots-to-analyze-the-model-4" class="level4">
<h4 class="anchored" data-anchor-id="using-residual-plots-to-analyze-the-model-4">Using Residual Plots to Analyze the Model</h4>
<p>The residual plots also look perfectly well in this model.</p>
<div class="cell" data-execution_count="93">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>ResidualPlots(data <span class="op">=</span> XTR, model<span class="op">=</span>model, num_inputs<span class="op">=</span>num_inputs, cat_inputs<span class="op">=</span>cat_inputs, output<span class="op">=</span>output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>--------------------------------------------------
Density Curve and QQ-plot of Residuals: ['x1', 'x2', 'x3']
--------------------------------------------------
[&lt;Axes: &gt; &lt;Axes: &gt;]
--------------------------------------------------
Fitted Values vs Residuals:
--------------------------------------------------
--------------------------------------------------
Numerical inputs: ['x1', 'x2', 'x3']
--------------------------------------------------
--------------------------------------------------
No categorical inputs exist.
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-73-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-73-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-73-output-4.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="computing-the-vifs-and-checking-predictive-performance" class="level4">
<h4 class="anchored" data-anchor-id="computing-the-vifs-and-checking-predictive-performance">Computing the VIFs and Checking Predictive Performance</h4>
<p>Now let us examine the VIFS for this model. Remember that a VIF &gt; 5 is considered a signal of multicollinearity.</p>
<div class="cell" data-execution_count="94">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.outliers_influence <span class="im">import</span> variance_inflation_factor </span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>vif_df <span class="op">=</span> pd.DataFrame() </span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>vif_df[<span class="st">"Variable"</span>] <span class="op">=</span> df[inputs].columns </span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a><span class="co"># calculating VIF for each feature b</span></span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>vif_df[<span class="st">"VIF"</span>] <span class="op">=</span> [variance_inflation_factor(dfTR[inputs].values, i) </span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>                          <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(dfTR[inputs].columns))] </span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>vif_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="94">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Variable</th>
<th data-quarto-table-cell-role="th">VIF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>x1</td>
<td>34.832504</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>x2</td>
<td>9.974203</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>x3</td>
<td>41.052912</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>As you can see, these are clear indicators of multicollinearity. But the predictive performance is not hurt because of this, as you can see in the following trains, validation and test score comparisons.</p>
<div class="cell" data-execution_count="95">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Create dataset to store model predictions</span></span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>dfTR_eval <span class="op">=</span> XTR.copy()</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>dfTR_eval[output] <span class="op">=</span> YTR</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>dfTS_eval <span class="op">=</span> XTS.copy()</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>dfTS_eval[output] <span class="op">=</span> YTS</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="96">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>dfTR_eval[<span class="st">'model_pred'</span>] <span class="op">=</span> lm_pipeline.predict(XTR)</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>dfTS_eval[<span class="st">'model_pred'</span>] <span class="op">=</span> lm_pipeline.predict(XTS)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now let us compute the RMSE score in training, validation and test:</p>
<div class="cell" data-execution_count="97">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>np.sqrt(mean_squared_error(YTR, dfTR_eval[<span class="st">"model_pred"</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="97">
<pre><code>1.0160912747617759</code></pre>
</div>
</div>
<div class="cell" data-execution_count="98">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> cross_val_score(lm_pipeline, XTR, YTR, cv<span class="op">=</span><span class="dv">10</span>, scoring<span class="op">=</span><span class="st">"neg_root_mean_squared_error"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="98">
<pre><code>array([1.08376295, 1.01528054, 1.01276996, 1.04037752, 0.98126073,
       1.0291144 , 0.98930486, 1.04502459, 1.07973088, 0.93764737])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="99">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>np.sqrt(mean_squared_error(YTS, dfTS_eval[<span class="st">"model_pred"</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="99">
<pre><code>0.9559781249766746</code></pre>
</div>
</div>
<p>As we see, the scores in train, validation and test are coherent.</p>
</section>
<section id="fitting-a-second-model" class="level4">
<h4 class="anchored" data-anchor-id="fitting-a-second-model">Fitting a Second Model</h4>
<p>In order to fight multicollinearity we will fit a second model removing the variable with the highest VIF value, in this case <code>x1</code> (if two variables have very similar VIFs you may keep e.g.&nbsp;the most easily interpretable one).</p>
<div class="cell" data-execution_count="100">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> [<span class="st">"x1"</span>, <span class="st">"x2"</span>]</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>inputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="100">
<pre><code>['x1', 'x2']</code></pre>
</div>
</div>
<div class="cell" data-execution_count="101">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>model_Formula <span class="op">=</span> <span class="st">" + "</span>.join(inputs)</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>model_Formula</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="101">
<pre><code>'x1 + x2'</code></pre>
</div>
</div>
<p>We create the pipeline and fit the model.</p>
<div class="cell" data-execution_count="102">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline <span class="op">=</span> Pipeline([</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>(<span class="st">"formula"</span>, FormulaTransformer(model_Formula)),</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>(<span class="st">"regressor"</span>, StatsModelsRegressor(OLS, fit_intercept <span class="op">=</span> <span class="va">False</span>))])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="103">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline.fit(dfTR[inputs], dfTR[output])</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> lm_pipeline._final_estimator</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="extracting-information-about-the-model-5" class="level4">
<h4 class="anchored" data-anchor-id="extracting-information-about-the-model-5">Extracting Information about the Model</h4>
<p>Everything looks good here:</p>
<div class="cell" data-execution_count="104">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.results_.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.952
Model:                            OLS   Adj. R-squared:                  0.952
Method:                 Least Squares   F-statistic:                     7969.
Date:                Thu, 20 Feb 2025   Prob (F-statistic):               0.00
Time:                        11:49:03   Log-Likelihood:                -1421.0
No. Observations:                 800   AIC:                             2848.
Df Residuals:                     797   BIC:                             2862.
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      9.9864      0.101     99.260      0.000       9.789      10.184
x1             1.9733      0.036     54.781      0.000       1.903       2.044
x2             1.9968      0.017    114.703      0.000       1.963       2.031
==============================================================================
Omnibus:                        1.824   Durbin-Watson:                   2.107
Prob(Omnibus):                  0.402   Jarque-Bera (JB):                1.745
Skew:                          -0.046   Prob(JB):                        0.418
Kurtosis:                       3.210   Cond. No.                         6.33
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
</div>
<p><strong>Note the changes in the model coefficients</strong> for the remaining variables. Even the sign has changed! Let us see the new VIFs:</p>
<div class="cell" data-execution_count="105">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>vif_df <span class="op">=</span> pd.DataFrame() </span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>vif_df[<span class="st">"Variable"</span>] <span class="op">=</span> dfTR[inputs].columns </span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a><span class="co"># calculating VIF for each feature </span></span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a>vif_df[<span class="st">"VIF"</span>] <span class="op">=</span> [variance_inflation_factor(dfTR[inputs].values, i) </span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a>                          <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(dfTR[inputs].columns))] </span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a>vif_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="105">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Variable</th>
<th data-quarto-table-cell-role="th">VIF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>x1</td>
<td>1.006232</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>x2</td>
<td>1.006232</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>As you can see the VIF values are much more moderate in this second model. This makes the coefficient estimates more reliable (the real coefficients in this example are <code>10 + 2 x1 + 2 x2</code>)</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 008
</div>
</div>
<div class="callout-body-container callout-body">
<p>Check the prediction scores for this second model. You should not expect a big difference with the previous one.</p>
</div>
</div>
<hr>
</section>
</section>
<section id="models-with-categorical-inputs" class="level1">
<h1>Models with Categorical Inputs</h1>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A Simple Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>The following example simply illustrates how we incorporate categorical inputs in our models in the most basic settings.</p>
</div>
</div>
<div class="cell" data-execution_count="106">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"./3_1_data06.csv"</span>)</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="106">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
<th data-quarto-table-cell-role="th">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>2.333070</td>
<td>A</td>
<td>23.870012</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1.675955</td>
<td>B</td>
<td>17.088453</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.814088</td>
<td>A</td>
<td>11.796125</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1.980600</td>
<td>B</td>
<td>22.636752</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>As we can see, <code>x2</code> is a factor encoded with non numeric labels. This is the easiest case as our code will be able to recognize this automatically and build the right design matrix. We store the names for the inputs and make the train/test split as in previous models:</p>
<div class="cell" data-execution_count="107">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> <span class="st">"y"</span></span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>num_inputs <span class="op">=</span> [<span class="st">"x1"</span> ]</span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>cat_inputs <span class="op">=</span> [<span class="st">"x2"</span>]</span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> num_inputs <span class="op">+</span> cat_inputs</span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[inputs]</span>
<span id="cb114-6"><a href="#cb114-6" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df[output]</span>
<span id="cb114-7"><a href="#cb114-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-8"><a href="#cb114-8" aria-hidden="true" tabindex="-1"></a>XTR, XTS, YTR, YTS <span class="op">=</span> train_test_split(</span>
<span id="cb114-9"><a href="#cb114-9" aria-hidden="true" tabindex="-1"></a>    X, Y,</span>
<span id="cb114-10"><a href="#cb114-10" aria-hidden="true" tabindex="-1"></a>    test_size <span class="op">=</span> <span class="fl">0.2</span>,  </span>
<span id="cb114-11"><a href="#cb114-11" aria-hidden="true" tabindex="-1"></a>    random_state <span class="op">=</span> <span class="dv">1</span>) </span>
<span id="cb114-12"><a href="#cb114-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-13"><a href="#cb114-13" aria-hidden="true" tabindex="-1"></a>dfTR <span class="op">=</span> pd.DataFrame(XTR, columns<span class="op">=</span>inputs)</span>
<span id="cb114-14"><a href="#cb114-14" aria-hidden="true" tabindex="-1"></a>dfTR[output] <span class="op">=</span> YTR</span>
<span id="cb114-15"><a href="#cb114-15" aria-hidden="true" tabindex="-1"></a>dfTS <span class="op">=</span> pd.DataFrame(XTS, columns<span class="op">=</span>inputs)</span>
<span id="cb114-16"><a href="#cb114-16" aria-hidden="true" tabindex="-1"></a>dfTS[output] <span class="op">=</span> YTS</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="eda-5" class="level4">
<h4 class="anchored" data-anchor-id="eda-5">EDA</h4>
<p>With a categorical input it is usually a good idea to incorporate its levels into the pairplots. Note, for example. the vertical arrangement of colors in the <code>Y</code> vs <code>x1</code> scatterplot.</p>
<div class="cell" data-execution_count="108">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>plt_df <span class="op">=</span> XTR.copy()</span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>plt_df[<span class="st">"Y"</span>] <span class="op">=</span> YTR</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>sns.pairplot(plt_df, corner<span class="op">=</span><span class="va">True</span>, height<span class="op">=</span><span class="fl">1.5</span>, aspect<span class="op">=</span><span class="fl">1.5</span>, hue<span class="op">=</span><span class="st">"x2"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-88-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="model-formula-and-fit-5" class="level4">
<h4 class="anchored" data-anchor-id="model-formula-and-fit-5">Model Formula and Fit</h4>
<p>In this example if we use a basic formula incorporating all inputs</p>
<div class="cell" data-execution_count="109">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>model_Formula <span class="op">=</span> <span class="st">" + "</span>.join(inputs)</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>model_Formula</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="109">
<pre><code>'x1 + x2'</code></pre>
</div>
</div>
<p>then the design matrix will automatically include a column with the right encoding of the categorical input (read the warning!). Let us see how:</p>
<div class="cell" data-execution_count="110">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>dmY, dmX <span class="op">=</span> ps.dmatrices(<span class="st">"y ~ "</span> <span class="op">+</span> model_Formula, df)</span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>dmX[:<span class="dv">4</span>, :]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="110">
<pre><code>array([[1.        , 0.        , 2.33306972],
       [1.        , 1.        , 1.67595477],
       [1.        , 0.        , 0.81408781],
       [1.        , 1.        , 1.98060009]])</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important: this is not One Hot Encoding
</div>
</div>
<div class="callout-body-container callout-body">
<p>One hot encodings as we have seen them are always multicollinear because they add up to one. The encoding used here is uses only a column, the second one, removing that redundancy in ohe to avoid multicollinearity.</p>
</div>
</div>
<p>The following steps are familiar. We create the pipeline and fit the model.</p>
<div class="cell" data-execution_count="111">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline <span class="op">=</span> Pipeline([</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>(<span class="st">"formula"</span>, FormulaTransformer(model_Formula)),</span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a>(<span class="st">"regressor"</span>, StatsModelsRegressor(OLS, fit_intercept <span class="op">=</span> <span class="va">False</span>))])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="112">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>lm_pipeline.fit(XTR, YTR)</span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> lm_pipeline._final_estimator</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="check-the-model-results" class="level4">
<h4 class="anchored" data-anchor-id="check-the-model-results">Check the Model Results</h4>
<p>The rest of the assessment of the model follows.</p>
<div class="cell" data-execution_count="113">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.results_.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.784
Model:                            OLS   Adj. R-squared:                  0.784
Method:                 Least Squares   F-statistic:                     1449.
Date:                Thu, 20 Feb 2025   Prob (F-statistic):          3.60e-266
Time:                        11:49:15   Log-Likelihood:                -2024.7
No. Observations:                 800   AIC:                             4055.
Df Residuals:                     797   BIC:                             4069.
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     11.0499      0.243     45.408      0.000      10.572      11.528
x2[T.B]       -3.0235      0.216    -14.026      0.000      -3.447      -2.600
x1             3.9377      0.077     51.364      0.000       3.787       4.088
==============================================================================
Omnibus:                        1.980   Durbin-Watson:                   1.930
Prob(Omnibus):                  0.372   Jarque-Bera (JB):                1.949
Skew:                           0.071   Prob(JB):                        0.377
Kurtosis:                       2.805   Cond. No.                         7.80
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
</div>
<p>Note that the coefficient of the categorical input indicates the level of the factor used as reference. Here we are using a model with:</p>
<p><span class="math display">\[f(x_1, x_2) = \beta_0 + \beta_1 x_1 + \beta_2 x_2^{enc}\]</span></p>
<p>Here <span class="math inline">\(x_2^{enc}\)</span> corresponds to the encoding of <span class="math inline">\(x_2\)</span> in the design matrix, which is 1 when <span class="math inline">\(x_0 = B\)</span> and 0 when <span class="math inline">\(X_2 = A\)</span>. In particular that means that the above equation can be interpreted as a straight line in <span class="math inline">\(x_1\)</span> where the intercept depends on the level of <span class="math inline">\(x_2\)</span>: it equals <span class="math inline">\(\beta_0\)</span> for <span class="math inline">\(x_2 = A\)</span> (reference level) and <span class="math inline">\(\beta_0 + \beta_2\)</span> for <span class="math inline">\(x_2 = B\)</span>. After defining the evaluation datasets we can plot a regression line for each level of <code>x2</code> to illustrate this.</p>
<div class="cell" data-execution_count="114">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>dfTR_eval <span class="op">=</span> XTR.copy()</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>dfTR_eval[output] <span class="op">=</span> YTR</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>dfTS_eval <span class="op">=</span> XTS.copy()</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a>dfTS_eval[output] <span class="op">=</span> YTS</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="115">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>sns.lmplot(data<span class="op">=</span>dfTR, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"y"</span>, hue<span class="op">=</span><span class="st">"x2"</span>, scatter_kws<span class="op">=</span>{<span class="st">'alpha'</span>:<span class="fl">0.25</span>})<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-95-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="using-residual-plots-to-analyze-the-model-5" class="level4">
<h4 class="anchored" data-anchor-id="using-residual-plots-to-analyze-the-model-5">Using Residual Plots to Analyze the Model</h4>
<p>The residual plots for this model now include parallel boxplots of the residuals across different levels of the categorical input(s). There should be no significant difference in the boxplots for each level, as in this example.</p>
<div class="cell" data-execution_count="116">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>ResidualPlots(data <span class="op">=</span> dfTR, model<span class="op">=</span>model, num_inputs<span class="op">=</span>num_inputs, cat_inputs<span class="op">=</span>cat_inputs, output<span class="op">=</span>output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>--------------------------------------------------
Density Curve and QQ-plot of Residuals: ['x1']
--------------------------------------------------
[&lt;Axes: &gt; &lt;Axes: &gt;]
--------------------------------------------------
Fitted Values vs Residuals:
--------------------------------------------------
--------------------------------------------------
Numerical inputs: ['x1']
--------------------------------------------------
--------------------------------------------------
Categorical inputs: ['x2']
--------------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-96-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-96-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-96-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-96-output-5.png" class="img-fluid"></p>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
An Example with a MultiLevel Factor
</div>
</div>
<div class="callout-body-container callout-body">
<p>When an input factor has more than two levels and they are represented with numbers you have to be careful and define the formula providing extra information to make sure that the factor is not confused with a numerical input. The following example briefly illustrates that point with a factor that has 4 levels.</p>
</div>
</div>
<div class="cell" data-execution_count="117">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"./3_1_data07.csv"</span>)</span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="117">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
<th data-quarto-table-cell-role="th">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>2.333070</td>
<td>1</td>
<td>22.866299</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1.675955</td>
<td>2</td>
<td>22.709764</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.814088</td>
<td>3</td>
<td>16.807665</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1.980600</td>
<td>4</td>
<td>30.362348</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>0.151959</td>
<td>1</td>
<td>13.194814</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>0.604424</td>
<td>2</td>
<td>15.083150</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>2.130828</td>
<td>3</td>
<td>27.724743</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>3.089289</td>
<td>4</td>
<td>36.682985</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Here, if we simply run the next two cells as in the previous example:</p>
<div class="cell" data-execution_count="118">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>model_Formula <span class="op">=</span> <span class="st">" + "</span>.join(inputs)</span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>model_Formula</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="118">
<pre><code>'x1 + x2'</code></pre>
</div>
</div>
<p>then the design matrix will include a single column for the categorical input, and it will not be really considered as such.</p>
<div class="cell" data-execution_count="119">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>dmY, dmX <span class="op">=</span> ps.dmatrices(<span class="st">"y ~ "</span> <span class="op">+</span> model_Formula, df)</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a>dmX[:<span class="dv">4</span>, :]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="119">
<pre><code>array([[1.        , 2.33306972, 1.        ],
       [1.        , 1.67595477, 2.        ],
       [1.        , 0.81408781, 3.        ],
       [1.        , 1.98060009, 4.        ]])</code></pre>
</div>
</div>
<p>In order to get the right encoding of categorical inputs we need to be more specific when writing the model formula:</p>
<div class="cell" data-execution_count="120">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb133"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>model_Formula <span class="op">=</span> <span class="st">"x1 + C(x2)"</span></span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>model_Formula</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="120">
<pre><code>'x1 + C(x2)'</code></pre>
</div>
</div>
<p>Now the design matrix correctly encodes the categorical input in three columns, leaving the first level of <code>x2</code> as reference level. Again we insist this is not the same as one hot encoding (which would include all four columns).</p>
<div class="cell" data-execution_count="121">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb135"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a>dmY, dmX <span class="op">=</span> ps.dmatrices(<span class="st">"y ~ "</span> <span class="op">+</span> model_Formula, df)</span>
<span id="cb135-2"><a href="#cb135-2" aria-hidden="true" tabindex="-1"></a>dmX[:<span class="dv">8</span>, :]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="121">
<pre><code>array([[1.        , 0.        , 0.        , 0.        , 2.33306972],
       [1.        , 1.        , 0.        , 0.        , 1.67595477],
       [1.        , 0.        , 1.        , 0.        , 0.81408781],
       [1.        , 0.        , 0.        , 1.        , 1.98060009],
       [1.        , 0.        , 0.        , 0.        , 0.15195865],
       [1.        , 1.        , 0.        , 0.        , 0.60442435],
       [1.        , 0.        , 1.        , 0.        , 2.13082829],
       [1.        , 0.        , 0.        , 1.        , 3.0892894 ]])</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 009
</div>
</div>
<div class="callout-body-container callout-body">
<p>Finish the EDA and model fit for this dataset and think about the interpretation of the coefficients of the model.</p>
</div>
</div>
<hr>
</section>
</section>
<section id="interactions" class="level1">
<h1>Interactions</h1>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A More Complex Relation between Input Variables
</div>
</div>
<div class="callout-body-container callout-body">
<p>In our first example above for categorical inputs (dataset in <code>data06.csv</code>) we saw how the levels of the factor could be thought of as determining the intercept of the regression line for a numerical input. In the next example we will see that sometimes the slope of the line is also under the influence of the factor.</p>
</div>
</div>
<div class="cell" data-execution_count="122">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb137"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"./3_1_data08.csv"</span>)</span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="122">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x1</th>
<th data-quarto-table-cell-role="th">x2</th>
<th data-quarto-table-cell-role="th">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1.058582</td>
<td>B</td>
<td>0.311131</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1.683179</td>
<td>B</td>
<td>-3.298882</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>2.241449</td>
<td>A</td>
<td>-0.489514</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>3.509912</td>
<td>B</td>
<td>-3.546480</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Let us define the split and the associated names:</p>
<div class="cell" data-execution_count="123">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> <span class="st">"y"</span></span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a>num_inputs <span class="op">=</span> [<span class="st">"x1"</span> ]</span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a>cat_inputs <span class="op">=</span> [<span class="st">"x2"</span>]</span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> num_inputs <span class="op">+</span> cat_inputs</span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[inputs]</span>
<span id="cb138-6"><a href="#cb138-6" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df[output]</span>
<span id="cb138-7"><a href="#cb138-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-8"><a href="#cb138-8" aria-hidden="true" tabindex="-1"></a>XTR, XTS, YTR, YTS <span class="op">=</span> train_test_split(</span>
<span id="cb138-9"><a href="#cb138-9" aria-hidden="true" tabindex="-1"></a>    X, Y,</span>
<span id="cb138-10"><a href="#cb138-10" aria-hidden="true" tabindex="-1"></a>    test_size <span class="op">=</span> <span class="fl">0.2</span>,  </span>
<span id="cb138-11"><a href="#cb138-11" aria-hidden="true" tabindex="-1"></a>    random_state <span class="op">=</span> <span class="dv">1</span>, </span>
<span id="cb138-12"><a href="#cb138-12" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>) </span>
<span id="cb138-13"><a href="#cb138-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-14"><a href="#cb138-14" aria-hidden="true" tabindex="-1"></a>dfTR <span class="op">=</span> pd.DataFrame(XTR, columns<span class="op">=</span>inputs)</span>
<span id="cb138-15"><a href="#cb138-15" aria-hidden="true" tabindex="-1"></a>dfTR[output] <span class="op">=</span> YTR</span>
<span id="cb138-16"><a href="#cb138-16" aria-hidden="true" tabindex="-1"></a>dfTS <span class="op">=</span> pd.DataFrame(XTS, columns<span class="op">=</span>inputs)</span>
<span id="cb138-17"><a href="#cb138-17" aria-hidden="true" tabindex="-1"></a>dfTS[output] <span class="op">=</span> YTS</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>An exploratory plot like the one we did before reveals the difference between this and the preceding situation:</p>
<div class="cell" data-execution_count="124">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb139"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a>sns.lmplot(data<span class="op">=</span>dfTR, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"y"</span>, hue<span class="op">=</span><span class="st">"x2"</span>, scatter_kws<span class="op">=</span>{<span class="st">'alpha'</span>:<span class="fl">0.25</span>})<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-104-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interactions
</div>
</div>
<div class="callout-body-container callout-body">
<p>The plot above illustrates the basic case of what is called an interaction between input variables. In this case the slope of the regression line for <code>y</code> over <code>x1</code> also depends on the level of <code>x2</code>. One way to express this using the equation of the linear model is to modify the previous version writing:</p>
<p><span class="math display">\[f(x_1, x_2) = \beta_0 + (\beta_1  + \beta_3 x_2^{enc}) x_1  + \beta_2\]</span></p>
<p>for certain coefficient <span class="math inline">\(\beta_3\)</span>. This can be rearranged as:</p>
<p><span class="math display">\[f(x_1, x_2) = \beta_0 + \beta_1  x_1 + \beta_2 x_2^{enc} + \beta_3 x_1\, x_2^{enc}\]</span></p>
<p>where now the model incorporates a new so called <strong>interaction term</strong> which is the product of the two variables. Note that when <code>x_2= A</code> or equivalently <code>x_2^{enc} =  0</code> then the model turns into a regression line for <code>x1</code> with slope <span class="math inline">\(\beta_1\)</span> and intercept <span class="math inline">\(\beta_0\)</span>:</p>
<p><span class="math display">\[f(x_1, x_2) = \beta_0 + \beta_1  x_1\]</span></p>
<p>whereas if <code>x_2 = B</code> or equivalently <code>x_2^{enc} =  1</code> it turns into a different line, with slope <span class="math inline">\(\beta_1 + \beta_3\)</span> and intercept <span class="math inline">\(\beta_0 + \beta_2\)</span>:</p>
<p><span class="math display">\[f(x_1, x_2) = (\beta_0 + \beta_2) + (\beta_1  + \beta_3) x_1\]</span></p>
</div>
</div>
<p>The pairplot using <code>x2</code> for color sends a similar warning.</p>
<div class="cell" data-execution_count="125">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>plt_df <span class="op">=</span> XTR.copy()</span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a>plt_df[<span class="st">"Y"</span>] <span class="op">=</span> YTR</span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a>sns.pairplot(plt_df, corner<span class="op">=</span><span class="va">True</span>, height<span class="op">=</span><span class="fl">1.5</span>, aspect<span class="op">=</span><span class="fl">1.5</span>, hue<span class="op">=</span><span class="st">"x2"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="3_1_LinearRegression_files/figure-html/cell-105-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 010
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Important:</strong> quickly fit a model with the formula <code>y ~ x1 + x2</code> and look at the residual plots of the model. If you missed the interaction in the EDA this will give you a second chance to spot the need for a different model.</p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Model Formulas for Interactions
</div>
</div>
<div class="callout-body-container callout-body">
<p>In Python we can extend the formula language to describe a model such as</p>
<p><span class="math display">\[f(x_1, x_2) = \beta_0 + \beta_1  x_1 + \beta_2 x_2^{enc} + \beta_3 x_1\, x_2^{enc}\]</span></p>
<p>by writing:</p>
<p><code>y ~ x1 + x2 + x1:x2</code></p>
<p>or simply</p>
<p><code>y ~ (x1 * x2)</code></p>
<p>This last version means <em>the variables and their (first order) interactions.</em> The code below displays the first rows of the design matrix for this model. Note that the last column is simply the product of the preceding two.</p>
</div>
</div>
<div class="cell" data-execution_count="126">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb141"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a>model_Formula <span class="op">=</span> <span class="st">" * "</span>.join(inputs)</span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a>model_Formula</span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a>dmY, dmX <span class="op">=</span> ps.dmatrices(<span class="st">"y ~ "</span> <span class="op">+</span> model_Formula, df)</span>
<span id="cb141-5"><a href="#cb141-5" aria-hidden="true" tabindex="-1"></a>dmX[:<span class="dv">8</span>, :]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="126">
<pre><code>array([[1.        , 1.        , 1.05858222, 1.05858222],
       [1.        , 1.        , 1.68317947, 1.68317947],
       [1.        , 0.        , 2.24144939, 0.        ],
       [1.        , 1.        , 3.50991229, 3.50991229],
       [1.        , 0.        , 4.93437   , 0.        ],
       [1.        , 0.        , 0.60442435, 0.        ],
       [1.        , 1.        , 4.32156963, 4.32156963],
       [1.        , 0.        , 2.96544222, 0.        ]])</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 011
</div>
</div>
<div class="callout-body-container callout-body">
<p>Now fit a second model with the formula with interactions and compare its results with the previous no-interaction model.</p>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 012
</div>
</div>
<div class="callout-body-container callout-body">
<p>Interactions are not limited to factors. If <code>x1</code> and <code>x2</code> are both numeric inputs, their interaction can be represented by the same type of product formula</p>
<p><span class="math display">\[\beta_0 + \beta_1  x_1 + \beta_2 x_2 + \beta_3 x_1\, x_2\]</span></p>
<p>Use the dataset to explore a situation like that.</p>
</div>
</div>
<hr>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
In the Next Session
</div>
</div>
<div class="callout-body-container callout-body">
<p>We will talk about feature selection and regularization in linear models.</p>
</div>
</div>
<hr>
</section>
<section id="references" class="level1 unnumbered">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-farawayLMP" class="csl-entry" role="listitem">
Faraway, Julian James. 2021. <em>Linear Models with Python</em>. First edition. Chapman &amp; Hall/CRC Texts in Statistical Science. <a href="https://julianfaraway.github.io/LMP/">https://julianfaraway.github.io/LMP/</a>.
</div>
<div id="ref-Haftorn85" class="csl-entry" role="listitem">
Haftorn, S., and R. E. Reinertsen. 1985. <span>“The Effect of Temperature and Clutch Size on the Energetic Cost of Incubation in a Free-Living Blue Tit (Parus Caeruleus).”</span> <em>The Auk</em>, 470--478.
</div>
<div id="ref-ESLI2009" class="csl-entry" role="listitem">
Hastie, Trevor J, Robert John Tibshirani, and Jerome H Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Springer. <a href="https://hastie.su.domains/ElemStatLearn">https://hastie.su.domains/ElemStatLearn</a>.
</div>
<div id="ref-ISLP2023" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. 2023. <em>An Introduction to Statistical Learning with Applications in Python</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-38747-0">https://doi.org/10.1007/978-3-031-38747-0</a>.
</div>
<div id="ref-IMLPY" class="csl-entry" role="listitem">
Muller, Andreas C, and Sarah Guido. 2017. <em>Introduction to Machine Learning with Python</em>. O’Reilly.
</div>
<div id="ref-pml1" class="csl-entry" role="listitem">
Murphy, Kevin P. 2022. <em>Probabilistic Machine Learning: An Introduction</em>. MIT Press. <a href="https://probml.github.io/pml-book/">https://probml.github.io/pml-book/</a>.
</div>
<div id="ref-PDSH" class="csl-entry" role="listitem">
VanderPlas, J. 2016. <em>Python Data Science Handbook: Essential Tools for Working with Data</em>. O’Reilly. <a href="https://jakevdp.github.io/PythonDataScienceHandbook/">https://jakevdp.github.io/PythonDataScienceHandbook/</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>