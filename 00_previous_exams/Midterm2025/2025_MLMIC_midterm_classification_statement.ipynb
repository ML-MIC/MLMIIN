{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc77de0",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING                                                  \n",
    "\n",
    "# Midterm exam 2025, Classification Problem\n",
    "\n",
    "<h1 style=\"color:red;\">Instructions: Read Carefully!</h1>\n",
    "\n",
    "\n",
    "- **[Use this Jupyter notebook]{.underline}** to complete the required tasks and submit it to Moodle. Keep the sectioning structure of the notebook and insert the code cells you need in the corresponding sections.\n",
    "\n",
    "- The notebook should contain the code with your **analysis** and **it must be reproducible**. Set the random seeds to ensure that.\n",
    "\n",
    "- The **most important part of your work is the comments and interpretation** of the analysis results obtained. **Do not include uncommented figures**. Remember to include a **conclusion section** at the end.                          \n",
    "\n",
    "- **[Use OBS to record your screen]{.underline}**. Upload the video file (max. 500Mb) to Moodle. Alternatively, make sure to copy it to one of the pendrives that will be provided.\n",
    "\n",
    "- The exam has **two notebooks:** one for the Regression problem (70% of the grading) and this one for the **Classification Problem** (30%). You must submit both of them to Moodle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494f1a49",
   "metadata": {},
   "source": [
    "## Statement of the Classification Problem\n",
    "\n",
    "### Dataset \n",
    "\n",
    "+ Look for your student code in the `student_codes.txt` file. Use the corresponding zip file cpntaining the data files for your analysis.  \n",
    "  **IMPORTANT: An exam done with a wrong dataset implies a failed exam.**\n",
    "\n",
    "+ Load the data set **fdata_cls_XX.csv** corresponding to your student code. Make sure you read it correctly. \n",
    "\n",
    "+ The binary target variable is called `cardio`: presence or absence of cardiovascular disease.\n",
    "\n",
    "+ The dataset contains 11 input variables:\n",
    "  + Age (years, as float)\n",
    "  + Gender (Male, Female)\n",
    "  + Height (cm)\n",
    "  + Weight (kg)\n",
    "  + ap_hi (Systolic blood pressure)\n",
    "  + ap_lo (Diastolic blood pressure)\n",
    "  + cholesterol (1: normal, 2: above normal, 3: well above normal)\n",
    "  + gluc (Glucose 1: normal, 2: above normal, 3: well above normal)\n",
    "  + smoke (Smoking 1: yes, 0: no)\n",
    "  + alco  (Alcohol intake 1: yes, 0: no)\n",
    "  + active  (Physical activity 1: yes, 0: no)\n",
    "\n",
    "\n",
    "### External Code and Imports\n",
    "\n",
    "+ The first code cell below contains standard imports that we have used in the sessions. With these imports you should be able to do all the tasks in the exam; that is not to say that you need to use all of them, and you are invited to use extra imports if you feel the need.\n",
    "\n",
    "+ To speed up your worl we **strongly** recommend you to adapt the code in the file `2_4_Performance_Analysis_Binary_Classifier.py` to assess the performance of the classifier models. We have included a copy of the file with the exam files.  We have also included the Python script `auxiliary_code.py` with a function called `explore_outliers` that will be available when you run the second cell in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5361c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png' # ‘png’, ‘retina’, ‘jpeg’, ‘svg’, ‘pdf’\n",
    "\n",
    "# plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Data management libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Scikit transformers and pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Scikit model selection and cross-validation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "\n",
    "# Scikit metrics and model performance\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "\n",
    "\n",
    "# Scikit classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ec1857",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"auxiliary_code.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad38873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your student code to select the data files.\n",
    "df = pd.read_csv(\"./dataClassification/fdata_cls_XX.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77603e03",
   "metadata": {},
   "source": [
    "### 2.1  Exploratory analysis and train/test split of data\n",
    "\n",
    "+ **Use 20% of the data as test set.**\n",
    "+ **When performing the split make sure to use `random_state=1` for reproducibility.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa6dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dfb281e",
   "metadata": {},
   "source": [
    "### 2.2 Random Forest classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39efb87",
   "metadata": {},
   "source": [
    "#### 2.2.1 Model\n",
    "\n",
    "+ **Train a Random forest classifier for the data. Use grid search to adjust two hyperparameters: minimum samples leaf and the maximum depth.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497526a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f3e9fec",
   "metadata": {},
   "source": [
    "#### 2.2.2 Model Performance\n",
    "\n",
    "+ **Evaluate its performance using appropriate metrics and plots.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55be322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcdd772c",
   "metadata": {},
   "source": [
    "#### 2.2.3 Variable Importance\n",
    "\n",
    "+ **Use the model to analyze the variable importance.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8b7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58241c54",
   "metadata": {},
   "source": [
    "### 2.3 Optional second model and conclusions\n",
    "\n",
    "+ **Optionally re-train an optimized model using the findings above.** \n",
    "+ **In any case, whether you train a second model or not, write a conclusion section with the main findings of your analysis.** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d4b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLMIC25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
