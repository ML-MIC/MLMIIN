{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd20b87",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Chapter 2, Part 5: Ensemble Methods and Boosting\"\n",
    "subtitle: \"Machine Learning\"\n",
    "date: \"January 2026\"\n",
    "date-format: \"MMMM YYYY\"\n",
    "author: \n",
    "  - F.San Segundo & N.Rodríguez\n",
    "bibliography: ../exclude/mlmiin.bib\n",
    "execute:\n",
    "  echo: true\n",
    "code-overflow: wrap\n",
    "format: \n",
    "  html: \n",
    "    toc: true\n",
    "    code-tools: true\n",
    "    code-fold: show\n",
    "    code-summary: \"Hide the code\"\n",
    "    embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54971ff5",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "\n",
    "## COPY THIS NOTEBOOK FIRST\n",
    "\n",
    "<h2 style=\"color:blue; font-weight:bold;\">Checklist</h2>\n",
    "\n",
    "+ Have you started Docker Desktop?\n",
    "+ Have you launched Docker from the `MLMIIN` repository folder?\n",
    "+ Have you connected VS Code to the running container?\n",
    "\n",
    "If you have missed any of these steps you may need to restart VS Code after completing them.  \n",
    "Also if Python seems unresponsive at first, try restarting the kernel.\n",
    "\n",
    "<h2 style=\"color:red; font-weight:bold;\">IMPORTANT</h2>\n",
    "\n",
    "+ Remember to **make a copy of this notebook** (in the same folder) before starting to work on it.\n",
    "+ If you make changes to the original notebook, save it with another name, and use Git to undo the changes in the original notebook (ask for help with this if you need it).\n",
    "\n",
    ":::  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187a9ad",
   "metadata": {},
   "source": [
    "::: {.callout-warning icon=false}\n",
    "\n",
    "##### Setting the working directory\n",
    "\n",
    "We begin by using cd to make sure we are in the right folder.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a7fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd 2_5_EnsembleMethods_Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c04b5",
   "metadata": {},
   "source": [
    "# Introduction to Ensemble Methods: the wisdom of the (wise!) crowds.\n",
    "\n",
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Fighting Overfitting as a Consequence of High Model Variance\n",
    "\n",
    "The major problem with decision trees is their **high variance**, which makes them prone to overfitting. That means that training a model with different training sets leads to highly variable, *volatile* decision boundaries. A possible way to reduce the variance of the predictions is to use a different training set to train a family or **ensemble** of predictors and then average their results. \n",
    "\n",
    "Of course, we often do not have enough data to spend in building independent training sets. A compromise solution then consists in making **resampled (bootstrapped)** training sets by sampling with replacement the training data. The resampled data sets will be of the same size as the original training set, but some points can be repeated and some others can be missing (about one third approx.)  We then use these resampled datasets to train models and, in order to get predictions, we average their results. For binary classification that means that we can average the probability predicted by each one of the models. The models for each resample need not be of the same type, but they usually have a common *base model type*. This method is called **bagging** and it is usually applied with decision trees as base models. \n",
    "\n",
    "We will begin exploring bagging to  discover some shortcomings of this method. Then we move to **Random Forests** to address the limitations of bagging. And finally we will discuss **gradient boosting** methods, that approach the problem with a different perspective. \n",
    "\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cdd0c2",
   "metadata": {},
   "source": [
    "<a title=\"Walter Baxter / A murmuration of starlings at Gretna\" href=\"https://commons.wikimedia.org/wiki/File:Starling_murmuration.jpg\"><img width=\"1024\" alt=\"Starling murmuration\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/8d/Starling_murmuration.jpg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595de40c",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Recommended reading:\n",
    "\n",
    "See [References](#References) section at the end for details.\n",
    "\n",
    "+ Chapter 8 of [@ISLP2023]\n",
    "+ Chapter 2 of [@IMLPY]\n",
    "+ Chapter 12 of [@serrano2021grokking]\n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e3fdad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b3349",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### A Higher Dimensional Data Set for this Session\n",
    "\n",
    "Some of the finer points of the discussion that follows only apply to datasets with a higher number of predictors than the examples we have ben using. So we begin by creating a new dataset that meets our requirements. \n",
    "\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7804735",
   "metadata": {},
   "source": [
    "Let us begin by loading the basic libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as col\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# This is a custom module that contains some plotting utilities, the source code is in the file \n",
    "# plot_utils.py in the same directory as this notebook\n",
    "from plot_utils import plot_2d_data, plot_2d_classifier\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f09fd0",
   "metadata": {},
   "source": [
    "We use the [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function to build the example. This is an utility function that you will use when you need to test your ideas, benchmark methods, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf4c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "X, Y = make_classification(n_classes=2, n_samples=N, n_informative=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f431de",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"X\" + str(k) for k in range(X.shape[1])]\n",
    "output = \"Y\"\n",
    "\n",
    "df = pd.DataFrame(X, columns = inputs)\n",
    "df[output] = Y\n",
    "df.iloc[:,:12].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "XTR, XTS, YTR, YTS = train_test_split(df[inputs], df[output],\n",
    "\t\t\t\t\t\t\t\t\t  test_size=0.2,  # percentage preserved as test data\n",
    "\t\t\t\t\t\t\t\t\t  random_state=1, # seed for replication\n",
    "\t\t\t\t\t\t\t\t\t  stratify = df[output])   # Preserves distribution of y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5702a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fde628",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f3fcb",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Constructing a Bagging Classifier\n",
    "\n",
    "Let us use this dataset to explore bagging, with decision trees as base models. We begin choosing a number of trees and their maximum depth in the next cell. Later you can return here and modify these values to see their impact in the results below. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 250\n",
    "md = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb1f1c",
   "metadata": {},
   "source": [
    "Bagging with a fixed base classifier is easily accomplished with [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html). All the parameters of the BagggingClassifier have self describing names, except for the  `oob_score` parameter. We will explain what this score means below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c843e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "BagDT = BaggingClassifier(estimator= DecisionTreeClassifier(\n",
    "                          max_depth=md),\n",
    "                          n_estimators=n_trees,\n",
    "                          oob_score=True,\n",
    "                          random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa6a8f",
   "metadata": {},
   "source": [
    "Fitting the model and getting the scores follows the usual steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff161fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BagDT.fit(XTR, YTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BagDT.score(XTR, YTR), BagDT.score(XTS, YTS), BagDT.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b2e7b6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bcb051",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Datasets for Predictions\n",
    "\n",
    "As usual we create datasets to store the predictions of the different models we train. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_datasets = ['dfTR' in globals(), 'dfTR_eval' in globals(), 'dfTS' in globals(), 'dfTS_eval' in globals()]\n",
    "print('Checking existence of dfTR, dfTR_eval and dfTS, dfTS_eval:', check_datasets)\n",
    "\n",
    "if not('dfTR' in globals()):\n",
    "    # Dataset for Training Predictions\n",
    "    dfTR = XTR.copy()\n",
    "    dfTR['Y'] = YTR\n",
    "    print(\"Created dfTR\")\n",
    "\n",
    "if not('dfTS' in globals()):\n",
    "    # Dataset for Training Predictions\n",
    "    dfTS = XTS.copy()\n",
    "    dfTS['Y'] = YTS\n",
    "    print(\"Created dfTS\")\n",
    "\n",
    "if not('dfTR_eval' in globals()):\n",
    "    # Dataset for Training Predictions\n",
    "    dfTR_eval = dfTR.copy()\n",
    "    print(\"Created dfTR_eval\")\n",
    "\n",
    "if not('dfTS_eval' in globals()):\n",
    "    # Dataset for Training Predictions\n",
    "    dfTS_eval = dfTS.copy()\n",
    "    print(\"Created dfTS_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BagDT\n",
    "model_name = \"BagDT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ffe4",
   "metadata": {},
   "source": [
    "::: {.callout-warning icon=false}\n",
    "\n",
    "### The `insert` function in `pandas` \n",
    "\n",
    "The code below uses the [`insert`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.insert.html) function in `pandas` to insert a new column in a dataframe. In this case this is not really necessary, as the result is equivalent to the following code we have been using so far:\n",
    "\n",
    "\n",
    "```python    \n",
    "    df['column_name'] = values\n",
    "```        \n",
    "\n",
    "But using insert will be convenient   when you want to insert a column in a specific position. The syntax is:\n",
    "\n",
    "```python\n",
    "    df.insert(loc, column, value)\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the actual predictions\n",
    "newCol = 'Y_'+ model_name +'_prob_neg'; \n",
    "dfTR_eval.insert(loc=dfTR_eval.shape[1], column=newCol, value=model.predict_proba(XTR)[:, 0])\n",
    "newCol = 'Y_'+ model_name +'_prob_pos'; \n",
    "dfTR_eval.insert(loc=dfTR_eval.shape[1], column=newCol, value=model.predict_proba(XTR)[:, 1])\n",
    "dfTR_eval[newCol] = model.predict_proba(XTR)[:, 1]\n",
    "newCol = 'Y_'+ model_name +'_pred'; \n",
    "dfTR_eval.insert(loc=dfTR_eval.shape[1], column=newCol, value=model.predict(XTR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3d95b",
   "metadata": {},
   "source": [
    "::: {.callout-warning icon=false}\n",
    "\n",
    "### The `filter` function in `pandas` \n",
    "\n",
    "The code below uses the [`filter`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.filter.html) function in `pandas` to display a subset of columns (`axis = 1`) of the dataframe, whose names contain a given string (specified by the `like` parameter):\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efcbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTR_eval.filter(like=\"BagDT\", axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3ae88",
   "metadata": {},
   "source": [
    "We do the same for the test set (this time we do not use `insert`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469accba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions dataset\n",
    "dfTS_eval = XTS.copy()\n",
    "dfTS_eval['Y'] = YTS \n",
    "newCol = 'Y_'+ model_name +'_prob_neg'; \n",
    "dfTS_eval[newCol] = model.predict_proba(XTS)[:, 0]\n",
    "newCol = 'Y_'+ model_name +'_prob_pos'; \n",
    "dfTS_eval[newCol] = model.predict_proba(XTS)[:, 1]\n",
    "newCol = 'Y_'+ model_name +'_pred'; \n",
    "dfTS_eval[newCol] = model.predict(XTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4a56b",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Model Dictionary\n",
    "\n",
    "We add this as first model to our model dictionary for comparison. Remember the code from the previous session to see how to use such a dictionary of models.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDict = {\"BagDT\" : {\"model\" : model, \"inputs\" : inputs}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08580997",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dadb5b1",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "### Exercise 001\n",
    "\n",
    "+ Try setting a number of trees ten times bigger and get the scores for that model. What happens?\n",
    "+ Set back the number of trees to 200 and change the max depth value to 6. What happens now?\n",
    "+ Play a bit more with the number of trees in the  bag and their depths.\n",
    "+ Now open the code from the previous session, and using it fit the best decision tree model that you can find to this dataset. Find the scores and store the predictions of this model in `dfTR`, `dfTS`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b30fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load \"../exclude/MLMIINprv/exercises/2_5_Exercise001.py\"\n",
    "# %run -i \"../exclude/MLMIINprv/exercises/2_5_Exercise001.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d032fc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a13e6c",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Understanding the Structure of the Bagging Classifier\n",
    "\n",
    "This bagging classifier is, at its core, a collection of **resamples of the training set** and decision trees fitted to each such resample. The code below shows how to examine these components.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd02684c",
   "metadata": {},
   "source": [
    "For example, the first decision tree in our *bag of trees* is obtained with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34354c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "BagDT.estimators_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd7b3c",
   "metadata": {},
   "source": [
    "This tree was trained using using a subsample with indices which you can see here (only the first few are shown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "BagDT.estimators_samples_[0][0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c7517b",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "### Exercise 002\n",
    "\n",
    "We said before that there was about one third of the elements of the training set missing from this resample. Can you check that statement?  \n",
    "\n",
    "*Bonus points from the Maths Department:* can you guess why? \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7db2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../exclude/MLMIINprv/exercises/2_5_Exercise002.py\"\n",
    "# %run -i \"../exclude/MLMIINprv/exercises/2_5_Exercise002.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744cf034",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### The out-of-bag score (`oob_score`) and the `bootstrap` and `samples` parameter.\n",
    "\n",
    "We have just said that about one third of the elements of the training set are missing from each resample. This means that we can use precisely those elements as a validation set to measure the model generalization performance. This is the idea behind the `oob_score` parameter. In a way it is as if the bagging classifier comes with a built-in cross-validation mechanism (note that there is no cv parameter in Bagging Classifier). \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330962f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BagDT.score(XTR, YTR), BagDT.score(XTS, YTS), BagDT.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d5633",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831e16a",
   "metadata": {},
   "source": [
    "### The Problem with Bagging\n",
    "\n",
    "\n",
    "::: {.callout-note  icon=false}\n",
    "\n",
    "##### We have a biodiversity problem lurking...\n",
    "\n",
    "We turned to bagging hoping to reduce the variance associated with different samples. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b298f3ae",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "\n",
    "### Exercise 003\n",
    "\n",
    "Here is an even simpler proposal compared to bagging: take a large number of identical copies of the training set (not resamples!), train a decision tree in each copy, and average the predictions. Is this a good idea?\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0fab51",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Variance Reduction and Independence\n",
    "\n",
    "A basic principle in Statistics is that in order to decrease the variance by using averages you need to consider independent measures (or as close to independent as possible). That is why the *identical copies* idea is just a waste of time and resources. How about our bag of trees? How *independent* are they? The following fragment of code plots the first splits of the 12 first trees in the bag. Remove the comments, run the code and look at them closely (you can see them [here as well](./fig/BaggedTrees.png)).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917cc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(12, 16), dpi = 300)\n",
    "\n",
    "# for i in range(12):    \n",
    "#     DT = BagDT.estimators_[i]\n",
    "#     plot_tree(DT, max_depth=2, ax=axes[i//3, i%3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654018b1",
   "metadata": {},
   "source": [
    "It feels as if we were aiming for this jungle-like rich diversity\n",
    "\n",
    "![](./fig/CostaRica-RainForest.jpg){width=35% fig-align=\"center\" fig-alt=\"A jungle of diverse trees\"}.  \n",
    "\n",
    "but we stumbled upon this non at all diverse plantation of quasi identical trees\n",
    "\n",
    "![](./fig/Palm_Tree_Plantation_Surrounding_Kiwumulo_Cave.jpg){width=35% fig-align=\"center\" fig-alt=\"A non diverse plantation of quasi identical trees\"}.  \n",
    "[Source: Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Palm_Tree_Plantation_Surrounding_Kiwumulo_Cave.jpg?uselang=es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e71f5fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ecd00",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Our Bag of Trees Looks Like a Palm Plantation, not Like a Rain Forest. \n",
    "\n",
    "Even though they are trained on different resamples, these trees seem to be making very similar questions, at least in the first splits. How does this reflect on their predictions, specifically in terms of probabilities?  \n",
    "\n",
    "Keep in mind that these models are trained on different resamples. To be able to make a sensible comparison we need a common set, and we will use the test set for this. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979d769",
   "metadata": {},
   "source": [
    "We create an array of the right size beforehand and use it to store the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bag_probs_array = np.zeros([YTS.shape[0], n_trees])\n",
    "\n",
    "for i in range(n_trees):\n",
    "    probs_i = BagDT.estimators_[i].predict_proba(XTS.values)[:,1]\n",
    "    test_bag_probs_array[:, i] = probs_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13fa892",
   "metadata": {},
   "source": [
    "These is what the first ones look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bag_probs_array[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae0fdd8",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "### Exercise 004\n",
    "\n",
    "What is the size of this array? What does a column represent? And a row?\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../exclude/MLMIINprv/exercises/2_5_Exercise004.py\"\n",
    "# %run -i \"../exclude/MLMIINprv/exercises/2_5_Exercise004.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3ada6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996bbfad",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Correlated Predictions.\n",
    "\n",
    "A simple way to check the similarity of these predictions is to compute [the correlation of the columns (read the docs!)](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html). As you can see below, the correlations are really high! It seems like the trees are essentially making very similar predictions, far from the independence that we were hoping would reduce variance.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a2420",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_bag_probs_array, columns=[\"T\"+ str(i) for i in range(n_trees) ]).corr().iloc[0:10, 0:10]\n",
    "print(f'The typical correlation between the predictions for two of these trees is {pd.DataFrame(test_bag_probs_array, columns=[\"T\"+ str(i) for i in range(n_trees) ]).corr().median().median():.{2}f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce17b610",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "### Exercise 005\n",
    "\n",
    "As we did before, play with the number of trees and their depths and see how this impacts on the correlations above.\n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b57563",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "### Exercise 006\n",
    "\n",
    "Before we move on,  suppose that you use a bag of ten trees with a fixed max depth. What is the difference (or differences) between doing this and using 10 fold cross validation with decision trees of the same fixed depth?\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292480b",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "### Exercise 007\n",
    "\n",
    "Do a performance analysis for the bagging classifier with `n_trees = 200` and `md = 4` (include confusion matrices, roc curves, calibration and probability histograms for both training and test sets). Make an initial global assessment of this model performance (before we compare it to other models).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../exclude/MLMIINprv/exercises/2_5_Exercise007.py\"\n",
    "# %run -i \"../exclude/MLMIINprv/exercises/2_5_Exercise007.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814df82",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9a60f",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78bbdf6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5171a7",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Decorrelating the Predictions.\n",
    "\n",
    "Why did this *palm tree plantation* phenomenon happened with bagging? One possible explanation, discussed at [@ISLP2023, Section 8.2.2.] is that the presence of strong predictors in the inputs causes the trees to be driven by them in too similar ways. Another factor is the bootstrapping (resampling) process itself.\n",
    "\n",
    "Random Forest classifiers are based in the same idea as bagging but the try to *lead us back to the jungle* by tweaking the split selection procedure used by the collection of trees. The way this is done is by:\n",
    "\n",
    "+ continue to use resamples as training sets\n",
    "+ force the tree to use a **randomly selected and small subset of the inputs at every split**.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687eab3",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Random Forests in Scikit.\n",
    "\n",
    "The code below shows how to implement a random forest classifier with scikit. For this part of the discussion we are keeping the depth of the trees equal to the one we used in bagging, so that you can focus on the difference caused by the random selection of inputs at the splits. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ef370f",
   "metadata": {},
   "source": [
    "::: {.callout-warning  icon=false}\n",
    "\n",
    "## Parallelism\n",
    "\n",
    "**Ask us about the `n-jobs=-1` option in this code.**\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(n_estimators=n_trees,\n",
    "                                       max_depth=md,\n",
    "                                       max_features=\"sqrt\",\n",
    "                                       random_state=1,\n",
    "                                       n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151d4db1",
   "metadata": {},
   "source": [
    "Let us fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad01bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF.fit(XTR, YTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6852c",
   "metadata": {},
   "source": [
    "The scores in training and test are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62105e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF.score(XTR, YTR), RF.score(XTS, YTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cda5d",
   "metadata": {},
   "source": [
    "These do not seem to be a major improvement from bagging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cdb940",
   "metadata": {},
   "outputs": [],
   "source": [
    "BagDT.score(XTR, YTR), BagDT.score(XTS, YTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c22d2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348cc367",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### But the Trees Look Much More Promising\n",
    "\n",
    "The estimators (individual trees) of the random forest can be accessed like we did before. And so we use similar code to check the trees:\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(12, 16), dpi = 300)\n",
    "\n",
    "for i in range(12):\n",
    "    # fig, axes = plt.subplots(figsize=(3, 4), dpi = 300)\n",
    "    DT = RF.estimators_[i]\n",
    "    plot_tree(DT, max_depth=1, ax=axes[i//3, i%3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c40bd5",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### What about the probabilities predicted?\n",
    "\n",
    "Again, this is the same we did before.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f24f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RF_probs_array = np.zeros([YTS.shape[0], n_trees])\n",
    "\n",
    "for i in range(n_trees):\n",
    "    probs_i = RF.estimators_[i].predict_proba(XTS.values)[:,1]\n",
    "    test_RF_probs_array[:, i] = probs_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cedabd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RF_probs_array[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816a2f7",
   "metadata": {},
   "source": [
    "And the correlation matrix confirms that we have succeeded in decreasing the correlations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_RF_probs_array, columns=[\"T\"+ str(i) for i in range(n_trees) ]).corr().iloc[0:10, 0:10]\n",
    "print(f'The typical correlation between the predictions for two of these trees is {pd.DataFrame(test_RF_probs_array, columns=[\"T\"+ str(i) for i in range(n_trees) ]).corr().median().median():.{2}f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad9537b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bab9a86",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Hyperparameter Selection for Random Forest\n",
    "\n",
    "The main idea behind the random forest method is that the combined work of *sufficiently diverse* and *sufficiently many* decision trees improves the performance and robustness of the resulting ensemble classifier. In general, the more trees in the forest, the more robust the classifier is, at the expense of higher training times. Here we will settle for a number of trees that keeps training time moderate and we will play with some of the hyperparameters of the individual trees. In many cases the default values for random forests are good enough, but we do this to give you another example of grid search for hyperparameter selection. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b4905",
   "metadata": {},
   "source": [
    "We use the minimum number of samples that a terminal node should contain and the total depth of the tree as hyperparameters. We include a special use case of the 'n_estimators' parameter (the number of trees in the forest) in which we provide a single, fixed value of that hyperparameter. By commenting and uncommenting lines in the cell below you can explore the impact of the number of trees in the performance of the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27726827",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_grid = {'RF__min_samples_leaf':range(5, 8),\n",
    "            # 'RF__n_estimators': range(100, 1001, 100),\n",
    "            'RF__max_depth': range(1, 6)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65569fc",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Fitting the Grid Search\n",
    "\n",
    "Constructing and fitting  the grid search is as usual. Note that we set here  the number of features and the maximum number of variables to select randomly at each split. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c275071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(random_state=1, max_features=\"sqrt\", n_estimators=500)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "RF_pipe = Pipeline(steps=[('RF', RF)]) \n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "RF_gridCV = GridSearchCV(estimator=RF_pipe, \n",
    "                        param_grid=hyp_grid, \n",
    "                        cv=num_folds,\n",
    "                        return_train_score=True,\n",
    "                        n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d6f76",
   "metadata": {},
   "source": [
    "Give the model a name and fit it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0fe0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RF\"\n",
    "model = RF_gridCV\n",
    "model.fit(XTR, YTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0422e0",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Model Dictionary\n",
    "\n",
    "We add this to our model dictionary for comparison.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe99fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDict[model_name] = {\"model\" : model, \"inputs\" : inputs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1bca38",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba70a7",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Comparison with Bagging\n",
    "\n",
    "The results below show an improvement of the predictive performance, while keeping overfitting under control.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a4ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(XTR, YTR), model.score(XTS, YTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "BagDT.score(XTR, YTR), BagDT.score(XTS, YTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f93bb5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e947bbf8",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Examining the Grid Search \n",
    "\n",
    "Let us see the selected hyperparameter values. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e45e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hyp_grid)\n",
    "for param in hyp_grid.keys():\n",
    "    print(f' Best value for {param}  = {model.best_params_[param]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb153ac",
   "metadata": {},
   "source": [
    "The following script plots a graph that illustrates the hyperparameter search behind this choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8828e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"./2_5_GridSearch_Plot.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22cf38",
   "metadata": {},
   "source": [
    "The diagram shows that the highest value of accuracy was obtained when considering the deepest trees in the grid. Therefore we may suspect that growing deeper trees can lead to further accuracy improvements. But we have to keep in mind the risk of overfitting that comes with deeper trees. In order to keep it in line we should probably increase the number of estimators, that would in turn result in higher training times. Hyperparameter tuning is always a tradeoff of several aspects of modeling . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec55b3f",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "### Exercise 008\n",
    "\n",
    "Do a performance analysis for this random forest model like we did before for bagging. And begin thinking about model comparison. We will return to this after we discuss some other models. \n",
    "\n",
    "**Python expertise exercise:** examine the code that produced that plot. Below we will see an example of grid search with a single hyperparameter. Does this code work in that case? *Can you fix it?*\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c682a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../exclude/MLMIINprv/exercises/2_5_Exercise008.py\"\n",
    "# %run -i \"../exclude/MLMIINprv/exercises/2_5_Exercise008.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f67654",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40472d20",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Variable Importance\n",
    "\n",
    "Just like the tree models they are based on, random forest provide a natural way to measure variable importance. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a70ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_importances = pd.DataFrame({'var':XTR.columns, \n",
    "                                'importance': model.best_estimator_.named_steps[\"RF\"].feature_importances_}\n",
    "                                ).sort_values(by=\"importance\", ascending = False).set_index('var')\n",
    "\n",
    "var_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a746b7b",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "### Exercise 009\n",
    "\n",
    "Use this table to find out which are the most important variables that account for 75% of the total importance.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a155a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../exclude/MLMIINprv/exercises/2_5_Exercise009.py\"\n",
    "# %run -i \"../exclude/MLMIINprv/exercises/2_5_Exercise009.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debfd1dc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9908b3",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "## Other Variants of Bagging\n",
    "\n",
    "The bagging method uses resampling of the data (row resampling) to train the models in order to improve the model performance in terms of the bias-variance tradeoff. As we have seen the main limitation of this methods is that the trees in the bag tend to be very similar. In contrast, in random forests we resample both the data (rows) and the features (columns). And, importantly, we do the feature resampling *at the split level*. That is, we get a different random set of features to consider at each split.\n",
    "\n",
    "There are other variants of bagging that use different resampling strategies that can be considered midway between bagging and random forests. One of them is the **Random Subspaces** method, in which we take a random resample of the features for every tree in the bag, but we then use this set of variables for all the splits in the tree. Another one is the **Random Patches** method, in which we resample both the data and the variables, but again the random resample of the features is fixed for all the splits in each tree we train.\n",
    "\n",
    "![](./fig/2_5_BaggingVariants.png){width=75% fig-align=\"center\" fig-alt=\"Bagging Variants\"}.\n",
    "\n",
    "The [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) class in scikit-learn allows you to use these variants as follows, by selecting the values of `max_features` and `max_samples`. \n",
    "\n",
    "+ `max_features` parameter: this is the fraction of features used by a particular tree in the ensemble (the same set of features is used for all splits in a fixed tree). The default value is 1, which means that all the features are used at each split. \n",
    "+ `max_samples` parameter: this is the fraction of samples used by a particular tree in the ensamble. The default value is 1, which means that all the samples are used by the trees.\n",
    "\n",
    "That means that if you set `max_samples`to 1 while setting `max_features` to a value between 0 and 1, you will be using the Random Subspaces method. If you set both `max_samples` and `max_features` to a value between 0 and 1, you will be using the Random Patches method, as illustrated above. The basic bagging method is obtained by setting `max_features` to 1 while setting `max_samples` to a value between 0 and 1.\n",
    "\n",
    "There are even more variants of bagging that can be implemented in scikit-learn. For example, in the preceding paragraphs we are assuming that sampling is done with replacement. But you can also use sampling without replacement, or even use a different sampling strategy. To sample without replacement (data or features) you can set the `bootstrap` and/or `bootstrap_features` parameters to `False`. The choice of the sampling strategy is usually guided by the size of the dataset and the number of features, and by the computational resources available. These resampling strategies can also be incorporated as hyperparameters in a grid search, to see if they improve the model performance. We recommend reading Chapter 2 of [@Kunapuli2023] for a detailed discussion of these methods.\n",
    "\n",
    "Where does the Random Forests method fit in this picture? Keep in mind that the Bagging, Random Subspaces and Random Patches methods can be applied for base learners (`estimators` in scikit parlance) other than decision trees. Random Forests, on the other hand, extend the Random Patches method by taking resampling to the split level. In particular, that *requires* decision trees as base learners.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c81ed",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "### Exercise 010\n",
    "\n",
    "You may have noticed that we did not perform a grid search to select hyperparameter values for `BagDT`. \n",
    "\n",
    "+ Do it now. In fact, take this opportunity to do a grid search that explores the above variants of bagging. \n",
    "+ What kind of ensemble method is used by the final selected model: bagging, random subspaces, random patches?  \n",
    "+ How does this model compare to the random forest model?\n",
    "\n",
    "Keep in mind that one drawback of this approach is that the BaggingClassifier does not directly provide variable importance measures. They can be obtained by examining the individual trees in the bag, but this is not as straightforward as in the case of random forests.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c297fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i \"../exclude/MLMIINprv/exercises/2_5_Exercise010.py\"\n",
    "# %load \"../exclude/MLMIINprv/exercises/2_5_Exercise010.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ff586",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Introduction to Boosting Methods.\n",
    "\n",
    "---\n",
    "\n",
    "$\\quad$\n",
    "\n",
    "::: {.callout-note  icon=false}\n",
    "\n",
    "\n",
    "### Sequential Ensemble Methods\n",
    "\n",
    "The bagging or random forest methods that we have discussed in this session are examples of **parallel ensemble methods**. Both bagging and random forests share the idea of training many learners on different resamples of the training data and combining/averaging their results to obtain a stronger model. The difference between them is the additional injection of randomness in the splitting achieved by random forests. \n",
    "In the this section we will introduce **sequential ensemble methods**, in which the models are trained sequentially, and each new model tries to correct the errors of the previous ones. Many of the methods we will see use the boosting technique, which is a general method to improve the performance of a weak learner by combining it with other weak learners. **Boosting** models also obtain their predictions as the combination of an ensemble of simple models, often called **weak models**, (typically shallow decision trees), but in this case instead of resampling the training set **we grow the models in the ensemble sequentially** in such a way that **each model focuses and tries to correct the mistakes made by the preceding one**. The main difference between boosting methods is in the way that this error correction is implemented and they are therefore divided into two major categories: \n",
    "\n",
    "+ **Adaptive boosting:** AdaBoost and related methods, the first generation of boosting.\n",
    "+ **Gradient boosting:** LightGBM, XGBoost, CatBoost, etc. that represent a newer approach to boosting.\n",
    "\n",
    "The *Ensemble Taxonomy* figure below (also from [@Kunapuli2023]) shows the difference between parallel and sequential ensemble methods.\n",
    "\n",
    "![](./fig/2_5_Kunapuli2023_EnsembleTaxonomy.png){width=50% fig-align=\"center\" fig-alt=\"Ensemble Methods for Machine Learning\"}\n",
    "\n",
    "\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd207a",
   "metadata": {},
   "source": [
    "::: {.callout-warning  icon=false}\n",
    "\n",
    "## Reference\n",
    "\n",
    "**Our discussion in this notebook follows closely the exposition in Chapters 4 to 6 of [@Kunapuli2023]. In fact, we will use some of the examples in this book as illustration of the concepts. We strongly recommend Kunauli's book if you look for a recent, thorough and clearly written introduction to ensemble methods.**\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f0701",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a0829",
   "metadata": {},
   "source": [
    "# Sequential Adaptive Boosting. AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb81860",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### AdaBoost guided example\n",
    "\n",
    "AdaBoost was introduced by h was introduced by Freund and Schapire in 1995 (see  @Freund1995). It works as follows: after training each model we modify the way that the next model sees the training set. We do that by **making the misclassified samples bigger in a sense**, by giving them more **weight**. That size or weight drives the next model to pay extra attention to them. The following example [@Kunapuli2023], section 4.2.1) illustrates this idea. The weak learners in this case are decision stumps, i.e., decision trees with only one split: a single yes/no question that divides the feature space in two regions, either horizontally or vertically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0794cc",
   "metadata": {},
   "source": [
    "Let us begin by loading the basic libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1361b7ca",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "\n",
    "In Boosting methods (and in some other Machine Learning methods) it is customary to use 1 and -1 as the target values for binary classification problems. This is because these methods use the sign of the predictions to classify the samples (you may think that 0 defined the decision boundary that separates the two classes; this is not entirely true, as we will see later, but it is a useful guiding principle).\n",
    "\n",
    "This is the reason why we convert the 0,1 labels in our dataset to 1,-1 in the following code.\n",
    "\n",
    ":::  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.05, random_state=13)\n",
    "\n",
    "y = 2 * y - 1  # Convert labels from [0, 1] to [-1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_samples, n_features = X.shape\n",
    "ensemble = []                                   # Initialize an empty ensemble\n",
    "\n",
    "# cmap = cm.get_cmap('Blues')\n",
    "cmap = mpl.colormaps.get_cmap('Blues')\n",
    "colors = cmap(np.linspace(0, 0.5, num=2))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "ax.scatter(X[y <= 0, 0], X[y <= 0, 1], marker='o', c=col.rgb2hex(colors[0]), edgecolors='k', alpha=0.5)\n",
    "ax.scatter(X[y > 0, 0], X[y > 0, 1], marker='s', c=col.rgb2hex(colors[1]), edgecolors='k', alpha=0.5)\n",
    "ax.set_title('Initial classification problem for Adaboost')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.show();plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5afd6c5",
   "metadata": {},
   "source": [
    "In this example we will train an AdaBoost ensemble model with only three stumps, using `sklearn`. Keep in mind that in practice you will use more stumps, and you will need to tune the hyperparameters of the model. Also it is worth mentioning that the `AdaBoostClassifier` in `sklearn` uses decision trees as weak learners by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b37908",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 3\n",
    "h = DecisionTreeClassifier(max_depth=1)     # Initialize a decision tree stump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9886530d",
   "metadata": {},
   "source": [
    "Next we create an array to store the weights assigned by the Adaboost iterations to the samples. Initially the weight of all samples is the same, $1/n$, where $n$ is the number of samples in the training set. We also create an empty list to add the (weak learners) stumps as we train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd18d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.ones((n_samples, ))        \n",
    "D = D / np.sum(D)                           \n",
    "\n",
    "ensemble = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233b770",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### First step: Fit the first weak learner (stump)\n",
    "\n",
    "Now we fit the first weak learner, a stump, using this sample weights to drive the model to pay more attention to the misclassified samples. In this first step, of course, all samples are equally weighted so the model makes no distinctions yet. We will plot the stump and the samples in the feature space. We will also use the predictiones of the stump to compute the **weighted error** of the model, which is the sum of the weights of the misclassified samples. In this first step, this is still the same as the unweighted error rate of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a050e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.fit(X, y, sample_weight=D)      \n",
    "\n",
    "ypred = h.predict(X)          \n",
    "\n",
    "e = 1 - accuracy_score(y, ypred, sample_weight=D)   # Weighted error of the weak learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1079a9",
   "metadata": {},
   "source": [
    "The stump is also assigned a (collective) weight of its own. Then the stump and its weight are added to the ensemble. When the Adaboost iterations are completed (all stumps trained and weighted) we will use the ensemble to make predictions on the test set by adding up the predictions of all stumps, each one weighted by its own weight (so more reliable stumps will have a bigger say in the final prediction).\n",
    "\n",
    "The weight of the stump is computed as \n",
    "$$a = \\dfrac{1}{2} \\log \\left( \\dfrac{1 - \\text{weighted error of the samples}}{\\text{weighted error of the samples}} \\right).$$\n",
    "We will return to this formula later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.5 * np.log((1 - e) / e)     # Weak learner weight\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f894b",
   "metadata": {},
   "source": [
    "The Adaboost algorithm next updates the weights of the samples. The weight of the misclassified samples is increased, and the weight of the correctly classified samples is decreased, according to this formula:\n",
    "1. Increase the weight of misclassified examples to $D_i e^{\\alpha_t}$  \n",
    "2. Decrease the weight of correctly classified examples to $\\dfrac{D_i}{e^{\\alpha_t}}$ \n",
    "\n",
    "In the code below `m` is used to identify correctly classified and misclassified points and assign them 1 or -1 labels. After the weights of the stump and the samples are updated, we can add the stump to the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2896239",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = (y == ypred) * 1 + (y != ypred) * -1    \n",
    "\n",
    "D = D * np.exp(- a * m)   # Update the sample weights\n",
    "\n",
    "ensemble.append((a, h))    # Add the weak learner to the ensemble "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c4d87",
   "metadata": {},
   "source": [
    "Let us visualize the decission boundary corresponding to the first stump and the weights assigned by this first iteration to the samples. We will use the following code to assign different sized markers to the samples according to their weights. We also compute the error (1 - accuracy) of the predictions of the first stump (as a percent).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c766e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = D / np.max(D)\n",
    "s[(0.00 <= s) & (s < 0.25)] = 16\n",
    "s[(0.25 <= s) & (s < 0.50)] = 32\n",
    "s[(0.50 <= s) & (s < 0.75)] = 64\n",
    "s[(0.75 <= s) & (s <= 1.00)] = 128\n",
    "\n",
    "err = (1 - accuracy_score(y, ypred)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2c886",
   "metadata": {},
   "source": [
    "Now we can plot the decision boundary and the samples with their updated weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ddecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "\n",
    "ax.scatter(X[y <= 0, 0], X[y <= 0, 1], s=s[y <= 0], marker='o', c=col.rgb2hex(colors[0]), edgecolors='k', alpha=0.5)\n",
    "ax.scatter(X[y > 0, 0], X[y > 0, 1], s=s[y > 0], marker='s', c=col.rgb2hex(colors[1]), edgecolors='k', alpha=0.5)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "title = f'First Weak Learner (error = {err:3.1f}%, weight of this stump = {a:3.2f})'\n",
    "plot_2d_classifier(ax, X, y, predict_function=h.predict, \n",
    "                    alpha=0.25, xlabel=None, ylabel=None, \n",
    "                    title=title, colormap='Blues')\n",
    "\n",
    "pos_err = (y > 0) & (y != ypred)\n",
    "pos_cor = (y > 0) & (y == ypred)\n",
    "neg_err = (y <= 0) & (y != ypred)\n",
    "neg_cor = (y <= 0) & (y == ypred)\n",
    "ax.scatter(X[neg_err, 0], X[neg_err, 1], marker='o', c=col.rgb2hex(colors[0]), edgecolors='k', s=80)\n",
    "ax.scatter(X[pos_err, 0], X[pos_err, 1], marker='s', c=col.rgb2hex(colors[1]), edgecolors='k', s=80)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.show();plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1c45d",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Next iterations: fit the following weak learners (stumps)\n",
    "\n",
    "Now we proceed to fit the next stumps, iterating through the following steps:\n",
    "\n",
    " 1. Train a weak learner $h_k(x)$ using the weights assigned to the samples in previous steps.\n",
    " 2. Compute the training error $e$ of this weak learner and use it to compute the weight $a$ of the weak learner.\n",
    " 3. Update the weights $D$ of the training examples, with the rule we have seen above (increase weight of misclassified, decrease the weight of wellclassified).\n",
    "\n",
    "The following code (adapted from @Kunapuli2023) shows the next two steps of the AdaBoost algorithm. We train two more stumps (recall we set `n_estimators = 3` above) and plot the decision boundaries and the weights of the samples after each iteration. Notice that the classification error of the stumps can be quite high. Still,  the ensemble model will be able to classify the samples correctly thanks to the weights assigned to the stumps.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, n_estimators):\n",
    "    \n",
    "    # -- Plot the training examples in different sizes proportional to their weights\n",
    "    s = D / np.max(D)\n",
    "    s[(0.00 <= s) & (s < 0.25)] = 16\n",
    "    s[(0.25 <= s) & (s < 0.50)] = 32\n",
    "    s[(0.50 <= s) & (s < 0.75)] = 64\n",
    "    s[(0.75 <= s) & (s <= 1.00)] = 128\n",
    "\n",
    "    h = DecisionTreeClassifier(max_depth=1)  # Initialize a decision stump\n",
    "    h.fit(X, y, sample_weight=D)                # Train a weak learner using sample weights\n",
    "    ypred = h.predict(X)                        # Predict using the weak learner\n",
    "\n",
    "    e = 1 - accuracy_score(y, ypred, sample_weight=D)   # Weighted error of the weak learner\n",
    "    a = 0.5 * np.log((1 - e) / e)                       # Weak learner weight\n",
    "    m = (y == ypred) * 1 + (y != ypred) * -1    # Identify correctly classified and misclassified points\n",
    "    D *= np.exp(-a * m)                         # Update the sample weights\n",
    "\n",
    "    # -- Plot the (first and last, no more than 10) individual weak learner\n",
    "    if ((k < 5) or (n_estimators - k < 5)):\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        err = (1 - accuracy_score(y, ypred)) * 100\n",
    "        title = f'Iteration {k + 1}: Weak Learner (error = {err:3.1f}%, weight of this stump = {a:3.2f})'\n",
    "        plot_2d_classifier(ax, X, y, predict_function=h.predict, \n",
    "                        alpha=0.25, xlabel=None, ylabel=None, \n",
    "                        title=title, colormap='Blues')\n",
    "\n",
    "        pos_err = (y > 0) & (y != ypred)\n",
    "        pos_cor = (y > 0) & (y == ypred)\n",
    "        neg_err = (y <= 0) & (y != ypred)\n",
    "        neg_cor = (y <= 0) & (y == ypred)\n",
    "        ax.scatter(X[neg_err, 0], X[neg_err, 1], marker='o', c=col.rgb2hex(colors[0]), edgecolors='k', s=80)\n",
    "        ax.scatter(X[pos_err, 0], X[pos_err, 1], marker='s', c=col.rgb2hex(colors[1]), edgecolors='k', s=80)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.show();plt.close()\n",
    "        # --\n",
    "\n",
    "\n",
    "    ensemble.append((a, h))                     # Save the weighted weak hypothesis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7aeed",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Combining the stumps to get the ensemble model predictions\n",
    "\n",
    "Finally, when we stop training weak learners, their predictions are combined, taking into account the quality of each of the models: the opinion of good classifiers gets a high positive score, the opinion of a random classifier counts zero and a worse than random one gets negative score (so that it is still actually helpful!). The precise formulation of this combination is given by the formula \n",
    "\n",
    "$f(x) = \\text{sign} \\left( F(x) \\right) = \\text{sign}\\left(\\sum_{k=1}^{K} a_k h_k(x)\\right),$\n",
    "\n",
    "where $f(x)$ is the prediction of the ensemble model (either -1 or 1), $a_k$ is the weight of the $k$-th weak learner and $h_k(x)$ is the prediction of the $k$-th weak learner. Here $F(x)$ represents the **confidence score** of the ensemble model. The larger $|F(x)|$ is, the higher the confidence in the classification.\n",
    "+ When $F(x) \\gg 0 $ the model is very confident that $x$ belongs to class +1 and conversely, when $F(x) \\ll 0$ the model is very confident that $x$ belongs to class -1.\n",
    "+ If $F(x) \\approx 0$, the model is uncertain about the classification.\n",
    "\n",
    "The code below does precisely this: it computes the predictions of the ensemble model on the test set and plots the decision boundaries of the ensemble model.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce84f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_boosting(X, estimators):\n",
    "    pred = np.zeros((X.shape[0], ))\n",
    "\n",
    "    for a, h in estimators:\n",
    "        pred += a * h.predict(X)\n",
    "    y = np.sign(pred)\n",
    "\n",
    "    return y\n",
    "\n",
    "ypred = predict_boosting(X, ensemble)\n",
    "err = (1 - accuracy_score(y, ypred)) * 100\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4))\n",
    "plot_2d_classifier(ax, X, y, predict_function=predict_boosting, predict_args=(ensemble), \n",
    "                   boundary_level=[0.0], alpha=0.25, xlabel='$x_1$', ylabel='$x_2$', s=80,\n",
    "                   title=title, colormap='Blues')\n",
    "\n",
    "fig.tight_layout()\n",
    "ax.set_title(f'Overall ensemble (error = {err:3.1f}%)'.format(fontsize=12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ffa077",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "### Exercise 011\n",
    "\n",
    "+ Go back to the top of the Adaboost code and set the number of stumps to 10. What happens now?\n",
    "+ Set it to 100 and look at the decision boundary end classification error of the ensemble model. Any thoughts? What is going on? \n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054fee84",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### AdaBoost and outliers. LogitBoost.\n",
    "\n",
    "Adaboost is well known to be highly sensitive to noisy data and outliers. The reason why outliers are a problem is that they are often misclassified by the weak learners, and Adaboost will try to correct these mistakes by giving them more weight. This can also be a cause for overfitting in AdaBoost models. \n",
    "\n",
    "This discussion is connected with the weak learner weight update formula that we saw before\n",
    "$$a = \\dfrac{1}{2} \\log \\left( \\dfrac{1 - \\text{weighted error of the samples}}{\\text{weighted error of the samples}} \\right).$$\n",
    "This formula is in turn based in the **exponential loss** function, one of the possible [loss functions for classification](https://en.wikipedia.org/wiki/Loss_functions_for_classification) used to fit Machine Learning models. There are other adaptive boosting ensemble methods that use different loss functions, like the **logistic loss** function, which is used in the LogitBoost method. We refer to [Section 4.5 of @Kunapuli2023] for a detailed discussion of LogitBoost and related methods.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda9c04",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### AdaBoost and overfitting. Learning rate and early stopping.\n",
    "\n",
    "Using Adaboost allows the combined force of weak learners to result in a strong model. Even though the stumps can only produce very simple linear boundariess (horizontal or vertical) the ensemble model is able to learn complex nonlinear boundaries, and it is less prone to overfitting than other models. That is not to say that AdaBoost is immune to overfitting, that can eventually appear as we increase the number of stumps.\n",
    "\n",
    "One way to keep overfitting under control is to add a new hyperparameter to the model, the **learning rate** $\\eta$. This parameter controls the contribution of each weak learner to the ensemble model. The predictions of the weak learners are multiplied by $\\eta$ before being added to the ensemble model. This way, the learning rate can be used to slow down the learning process, and to prevent the model from overfitting. The precise way in which this is done is given by the formula \n",
    "\n",
    "$$F_{k + 1}(x) = F_k(x) + \\eta\\,a_k\\,h_k(x)$$\n",
    "\n",
    "where $h_k(x)$ is the (-1, 1) prediction of the $k$-th stump, $a_k$ is the weight of that stump and $F_k(x)$ is the confidence score of the ensemble model after the $k$-th iteration. This iterative expression replaces the basic computation of $F(x)$ we saw before. The learning rate, as usual with hyperparameters, needs to be tuned. Note that the effect of $\\eta$ is multiplicative: later stumps in the algorithm are affected by higher powers of $\\eta$. That means that smaller values of $\\eta$ will slow down the learning process, trying to prevent the the model from overfitting the training data.\n",
    "\n",
    "Another startegy to prevent overfitting is to use **early stopping**. This means that we stop the training process as soon as we detect that the classification rate has not improved in the last $q$ iterations. \n",
    "\n",
    "Both the use of learning rate and early stopping belong to the **regularization** set of tools that are used with many Machine Learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e99d4c",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### AdaBoost in Scikit. \n",
    "\n",
    "The example above showed a manual implementation of AdaBoost to illustrate the inner working of the algorithm. In real problems we would use the `AdaBoostClassifier` class in `sklearn`. In this section we will see how to use it to train AdaBoost based ensemble models. Let us see how this is done with the same dataset that we used for bagging and random forests.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2269636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "\n",
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "N = 1000\n",
    "\n",
    "\n",
    "X, Y = make_classification(n_classes=2, n_samples=N, n_informative=5, random_state=1)\n",
    "\n",
    "inputs = [\"X\" + str(k) for k in range(X.shape[1])]\n",
    "output = \"Y\"\n",
    "\n",
    "df = pd.DataFrame(X, columns = inputs)\n",
    "df[output] = Y\n",
    "df.iloc[:,:12].head()\n",
    "\n",
    "\n",
    "XTR, XTS, YTR, YTS = train_test_split(df[inputs], df[output],\n",
    "\t\t\t\t\t\t\t\t\t  test_size=0.2,  # percentage preserved as test data\n",
    "\t\t\t\t\t\t\t\t\t  random_state=1, # seed for replication\n",
    "\t\t\t\t\t\t\t\t\t  stratify = df[output])   # Preserves distribution of y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9d769",
   "metadata": {},
   "source": [
    "Now we create and fit the grid search for this model with learning rate as hyperparameter. Note the way that the stumps are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b87689",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_grid = {'AdB__learning_rate': np.linspace(0.01, 1, 11)} \n",
    "hyp_grid\n",
    "\n",
    "n_trees = 100\n",
    "\n",
    "stump = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "AdB = AdaBoostClassifier(estimator= stump,                        \n",
    "                         n_estimators=n_trees,\n",
    "                         algorithm='SAMME', # to prevent a deprecation warning\n",
    "                         random_state=1)\n",
    "\n",
    "AdB_pipe = Pipeline(steps=[('AdB', AdB)]) \n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "AdB_gridCV = GridSearchCV(estimator=AdB_pipe, \n",
    "                        param_grid=hyp_grid, \n",
    "                        cv=num_folds,\n",
    "                        return_train_score=True,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "model_name = \"AdB\"\n",
    "model = AdB_gridCV\n",
    "model.fit(XTR, YTR)\n",
    "\n",
    "# Dataset for MOdel Predictions\n",
    "dfTR_eval = XTR.copy()\n",
    "dfTR_eval['Y'] = YTR \n",
    "\n",
    "dfTS_eval = XTS.copy()\n",
    "dfTS_eval['Y'] = YTS \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af13c81d",
   "metadata": {},
   "source": [
    "We can now add the model to our dictionary as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1043839",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDict = {model_name : {\"model\" : model, \"inputs\" : inputs}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb47fc",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### First Checks for the AdaBoost Results\n",
    "\n",
    "Let us see the scores achieved by this model.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(XTR, YTR), model.score(XTS, YTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951861a",
   "metadata": {},
   "source": [
    "As you can see, it is a decent performance without much tuning. And no overfitting seems to be happening. Let us visualize the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5fd4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_name = list(hyp_grid.keys())[0]\n",
    "param_values = hyp_grid[param_name]\n",
    "\n",
    "mean_train_scores = model.cv_results_['mean_train_score']\n",
    "plt.plot(param_values, 1 - mean_train_scores, marker='o', label='Mean Train Score')\n",
    "\n",
    "mean_test_scores = model.cv_results_['mean_test_score']\n",
    "plt.plot(param_values, 1 - mean_test_scores, marker='*', label='Mean Validation Score')\n",
    "\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Error (1 - accuracy)')\n",
    "plt.title('Hyperparameter Tuning Results')\n",
    "plt.legend()  # Add a legend to the plot\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f950e9f8",
   "metadata": {},
   "source": [
    "The selected learning rate is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e24d7a",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "### Exercise 012\n",
    "\n",
    "In AdaBoost the learning rate and the number of stumps are related, as there is an interplay between the two. Add `n_trees` as a new hyperparameter  to the grid search (you only need to modify the `hyp_grid` for this) and try to find the best combination of these two hyperparameters.  \n",
    "**Note:** after doing that the grid search plot code will not work as it is. You will need to modify it if you want to plot the results of the new grid search.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10159554",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Visualizing the stumps\n",
    "\n",
    "Remember that this is an ensemble of stumps. Run the code below to visualize the first few of them.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(4, 3), dpi = 300)\n",
    "\n",
    "for i in range(6):    \n",
    "    DT = AdB_gridCV.best_estimator_.named_steps[\"AdB\"].estimators_[i]\n",
    "    plot_tree(DT, max_depth=2, ax=axes[i//3, i%3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb594c6",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "### Exercise 013\n",
    "\n",
    "Do a performance analysis for this new model. We have already seen many examples so running the code for this should be easy!\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980792ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../exclude/MLMIINprv/exercises/2_5_Exercise013.py\"\n",
    "# %run -i \"../exclude/MLMIINprv/exercises/2_5_Exercise013.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf308e2d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c6690",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e2b6f",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Introduction to Gradient Boosting\n",
    "\n",
    "**Gradient boosting** methods are similar to AdaBoost in that they try to **sequentially improve the basic models** they train. But they do this in a different way, which is inspired by the optimization theory [**gradient descent method**](http://www.benfrederickson.com/numerical-optimization/) (a 3d attempt at visualization [here](https://www.geogebra.org/m/jgmhjfpw)). \n",
    "In gradient boosting methods the models are trained to sequentially minimize the loss function, mimicking the idea of gradient descent. It turns out that in most cases the gradient of the loss function depends on the **residuals** of the model, i.e., the difference between the labels and the current predictions $F_k(x)$ of the model, which in boosting is chosen to be a soft classification model.\n",
    "$$\\text{true value} - F_k(x)$$\n",
    "This implies that missclasified samples will have a large residual, and well classified samples will have a small residual. This reminds of the way that AdaBoost assigns weights to the samples, but in this case we are *not using weight but residuals*. This is illustrated in the following [Fig. 5-8 from [@Kunapuli2023].\n",
    "\n",
    "\n",
    "![](./fig/2_6_Kunapali_fig5_8.jpg){width=75% fig-align=\"center\" fig-alt=\"Kunapali Fig 5.8\"}\n",
    "\n",
    "So the basic idea is to fit a model to the residuals of the preceding model, much like every model in AdaBoost updated the weights it received from the previous model. But the criterion in Adaboost was the misclassification of the samples, while in gradient boosting what we have the residuals of the model. The key insight is this: **gradient boosting models fit a weak learner to these residuals in order to approximate the gradient of the loss function.** Each sample residual can be considered an approximate loss gradient and the weak model combines them to take the next step in gradient descent. So instead of a weight update we have an approximate-gradient update. The model update equation now is analogous to what we have seen for AdaBoost:\n",
    "\n",
    "$$F_{k + 1}(x) = F_k(x) + \\eta\\,a_k\\,h_k(x)$$\n",
    "\n",
    "only now $h_k(x)$ is the prediction of the $k$-th weak learner on the residuals of the model and the \"weight\" $a_k$ is not really a weight but a measure of the step length in the gradient direction that we take in order to minimize the loss as much as possible (see [@Kunapuli2023], section 5.2.1 for a detailed discussion). The learning rate $\\eta$ is again used to control the step size in the approximate gradient descent.\n",
    "\n",
    "From a technical point of view, stumps are used as weak learners to approximate the loss gradient. But since gradients are real numbers, these are regression stumps instead of classification stumps. Regression trees return values are obtained as averages of the output variable value for all the data points in a leaf, instead of using majority vote as in classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab439c22",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Gradient Boosting in Scikit Learn: HistGradientBoostingClassifier\n",
    "\n",
    "Scikit provides two classes to train gradient boosting classification ensembles: [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) and [`HistGradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier). The first one is a general implementation of gradient boosting, while the second one is a more efficient implementation that uses histograms to speed up the training process that we will discuss below. But first let us see how to apply the basic `GradientBoostingClassifier` model to the dataset we have been using.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bb808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "GradBoost = GradientBoostingClassifier(max_depth=1, \n",
    "                                      n_estimators=20, \n",
    "                                      learning_rate=0.75)\n",
    "\n",
    "GradBoost_pipe = Pipeline(steps=[('GradBoost', GradBoost)]) \n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "hyp_grid = {'GradBoost__learning_rate': 10.**np.arange(-6, 1, 1), \n",
    "            'GradBoost__n_estimators': [20, 50, 100, 200, 500]}           \n",
    "\n",
    "GradBoost_gridCV = GridSearchCV(estimator=GradBoost_pipe, \n",
    "                        param_grid=hyp_grid, \n",
    "                        cv=num_folds,\n",
    "                        return_train_score=True,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "model_name = \"GradBoost\"\n",
    "model = GradBoost_gridCV\n",
    "model.fit(XTR, YTR)\n",
    "\n",
    "modelDict[model_name] = {\"model\" : model, \"inputs\" : inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f16bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(XTR, YTR), model.score(XTS, YTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264607d8",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### Histogram Based Gradient Boosting in Scikit Learn: HistGradientBoostingClassifier\n",
    "\n",
    "The problem with a naive approach to gradient boosting is that for large datasets the process of fitting many regression stumps to the model residuals in order to estimate the loss gradient can be computationally expensive (he model residuals need to be computed *for each sample* in the training set). The `HistGradientBoostingClassifier` in `sklearn` is a more efficient implementation of gradient boosting that uses histograms to speed up the training process. The idea is to bin the values of each feature into histograms and fit the weak learners to the histograms instead of the individual samples. This reduces the number of computations needed to fit the weak learners and makes the training process faster. The `HistGradientBoostingClassifier` is a good choice for large datasets with many samples and features. See Figure 5.17 of [@Kunapuli2023] for a nice illustration of the use of histograms in gradient boosting.\n",
    "\n",
    "The implementation is very similar to the `GradientBoostingClassifier` class, but it an additional hyperparameter that control the number of bins used in the histograms.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hyp_grid = {'HGB__learning_rate': 10.**np.arange(-6, 1, 1), \n",
    "            'HGB__max_iter':np.arange(25, 150, 25),\n",
    "            'HGB__max_bins':[10, 25, 50, 100]} \n",
    "\n",
    "HGB = HistGradientBoostingClassifier(random_state=1)\n",
    "\n",
    "HGB_pipe = Pipeline(steps=[('HGB', HGB)]) \n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "HGB_gridCV = GridSearchCV(estimator=HGB_pipe, \n",
    "                        param_grid=hyp_grid, \n",
    "                        cv=num_folds,\n",
    "                        return_train_score=True,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "HGB_gridCV.fit(XTR, YTR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed5b39",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "### Exercise 014\n",
    "\n",
    "Finish the job with this model:\n",
    "\n",
    "+ Add the model to the dictionary.\n",
    "+ Find the scores for training and test sets.\n",
    "+ What are the best parameters selected by the model?\n",
    "+ Visualize the hyperparameter grid search. Hint: run the same script we used before.  \n",
    "+ Do a performance analysis of the model.\n",
    "\n",
    "And remember, for this and all subsequent exercises: this is not about copy-pasting and running code and getting plots. Your interpretation of the results is the really important part of the task.  \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b20906",
   "metadata": {},
   "source": [
    "::: {.callout-tip  icon=false}\n",
    "\n",
    "### Exercise 015\n",
    "\n",
    "In this and the preceding session we have now trained several different classification models for this dataset. Compare the models! Which one would you choose and why? What are the pros and cons of each model?\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea538b",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "### LightGBM and XGBoost\n",
    "\n",
    "Outside of scikit there are some very powerful gradient boosting libraries, like LightGBM and XGBoost. These libraries are optimized for speed and performance and are widely used in practice. They are based on the same principles as the gradient boosting models we have seen here, but they have additional features and optimizations that make them more efficient and powerful. We will return to these libraries when we deal with regression problems. Meanwhile, you can check the [LightGBM](https://lightgbm.readthedocs.io/en/latest/) and [XGBoost](https://xgboost.readthedocs.io/en/latest/) documentation for more information, below we include a basic example of how to use LightGBM with the same dataset.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79cde9",
   "metadata": {},
   "source": [
    "::: {.callout-warning  icon=false}\n",
    "\n",
    "## LightGBM setup\n",
    "\n",
    "In order to run lightgbm you need to install the library. Remeber to backup ypur environment first!\n",
    "\n",
    "For Windows machines it should be enough to activate the MLMIC 25 environment in an Anaconda prompt and then run:\n",
    "```bash\n",
    "conda install -c conda-forge lightgbm\n",
    "```\n",
    "\n",
    "\n",
    "For Mac mahines you need to make sure that you have gcc installed with brew. You can do this by running the following command in your terminal. Ask for help if you need it. \n",
    "\n",
    "```bash\n",
    "brew install cmake libomp gcc\n",
    "```\n",
    "\n",
    "Then you can activate the MLMIC 25 environment in a terminal and install lightgbm with pip:\n",
    "\n",
    "```bash\n",
    "pip install lightgbm\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cda1f",
   "metadata": {},
   "source": [
    "::: {.callout-warning  icon=false}\n",
    "\n",
    "## The next code cell can take > 10 minutes to run\n",
    "\n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "\n",
    "# lgb_pipeline = Pipeline([\n",
    "#     ('lgb', lgb.LGBMClassifier())  # First and only step\n",
    "# ])\n",
    "\n",
    "# lgb_hyp_grid = {\n",
    "#     'lgb__num_leaves': [20, 31, 40],         # Number of leaves in one tree\n",
    "#     'lgb__learning_rate': [0.01, 0.05, 0.1], # Step size shrinkage\n",
    "#     'lgb__n_estimators': [100, 500, 1000],   # Number of boosting iterations\n",
    "#     'lgb__max_depth': [-1, 10, 20],          # Maximum tree depth\n",
    "#     'lgb__min_child_samples': [10, 20, 30]   # Minimum data points in a leaf\n",
    "# }\n",
    "\n",
    "# lgb_gridCV = GridSearchCV(estimator=lgb_pipeline, \n",
    "#                            param_grid=lgb_hyp_grid,\n",
    "#                            cv=5, scoring='accuracy', verbose=3, n_jobs=-1)\n",
    "\n",
    "# lgb_gridCV.fit(XTR, YTR)\n",
    "\n",
    "# # Step 7: Get the best hyperparameters\n",
    "# best_params = lgb_gridCV.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "# from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "\n",
    "# lgb_pipeline = Pipeline([\n",
    "#     ('lgb', lgb.LGBMClassifier())  # First and only step\n",
    "# ])\n",
    "\n",
    "# lgb_hyp_grid = {\n",
    "#     'lgb__num_leaves': [20, 31, 40],         # Number of leaves in one tree\n",
    "#     'lgb__learning_rate': [0.01, 0.05, 0.1], # Step size shrinkage\n",
    "#     'lgb__n_estimators': [100, 500, 1000],   # Number of boosting iterations\n",
    "#     'lgb__max_depth': [-1, 10, 20],          # Maximum tree depth\n",
    "#     'lgb__min_child_samples': [10, 20, 30]   # Minimum data points in a leaf\n",
    "# }\n",
    "\n",
    "# # This is significantly faster and gives updates on each \"round\"\n",
    "# lgb_halvingCV = HalvingGridSearchCV(estimator=lgb_pipeline, \n",
    "#                                      param_grid=lgb_hyp_grid,\n",
    "#                                      scoring='accuracy',\n",
    "#                                      factor=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "# lgb_halvingCV.fit(XTR, YTR)\n",
    "\n",
    "# # Step 7: Get the best hyperparameters\n",
    "# best_params = lgb_halvingCV.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184e3cf",
   "metadata": {},
   "source": [
    "::: {.callout-note  icon=false}\n",
    "\n",
    "## In the Next Session\n",
    "\n",
    "We will meet another top performing model for many classification problems, the Support Vector Machine.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9affb6ba",
   "metadata": {},
   "source": [
    "# References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
